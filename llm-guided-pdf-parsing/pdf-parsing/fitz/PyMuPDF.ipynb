{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|██▍       | 129/519 [00:05<00:16, 23.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 519/519 [00:31<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 519 PDFs in 32.63 seconds\n",
      "Average time per PDF: 0.06 seconds\n",
      "Results saved to fitz_extraction_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tempfile\n",
    "import pymupdf\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from tempfile import NamedTemporaryFile\n",
    "from dotenv import load_dotenv\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Minio client using boto3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=os.getenv('MINIO_API_ENDPOINT'),\n",
    "    aws_access_key_id=os.getenv('MINIO_ACCESS_KEY'),\n",
    "    aws_secret_access_key=os.getenv('MINIO_SECRET_KEY')\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(file_path, s3_path):\n",
    "    \"\"\"Extract text from a PDF file using pymupdf\"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(file_path)\n",
    "        pdf_text = \"\"\n",
    "        for page in doc:\n",
    "            text = page.get_text().encode(\"utf8\").decode(\"utf8\", errors='ignore')\n",
    "            pdf_text += text + \"\\n\"\n",
    "        return {\"s3_path\": s3_path, \"text\": pdf_text, \"status\": \"success\"}\n",
    "    except pymupdf.EmptyFileError:\n",
    "        print(f\"Empty PDF file: {s3_path}\")\n",
    "        return {\"s3_path\": s3_path, \"text\": \"\", \"status\": \"empty_file\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {s3_path}: {e}\")\n",
    "        return {\"s3_path\": s3_path, \"text\": \"\", \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "def process_pdf(key, bucket, temp_dir):\n",
    "    \"\"\"Process a single PDF file - for parallel execution\"\"\"\n",
    "    # Create a new S3 client for each process to avoid sharing connections\n",
    "    s3_process_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=os.getenv('MINIO_API_ENDPOINT'),\n",
    "        aws_access_key_id=os.getenv('MINIO_ACCESS_KEY'),\n",
    "        aws_secret_access_key=os.getenv('MINIO_SECRET_KEY')\n",
    "    )\n",
    "    \n",
    "    temp_file_path = os.path.join(temp_dir, os.path.basename(key))\n",
    "    \n",
    "    try:\n",
    "        # Download the PDF\n",
    "        s3_process_client.download_file(bucket, key, temp_file_path)\n",
    "        \n",
    "        # Extract text\n",
    "        result = extract_text_from_pdf(temp_file_path, key)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {key}: {e}\")\n",
    "        result = {\"s3_path\": key, \"text\": \"\", \"status\": \"download_error\", \"error\": str(e)}\n",
    "    \n",
    "    # Clean up\n",
    "    if os.path.exists(temp_file_path):\n",
    "        os.remove(temp_file_path)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_cache_filename(bucket, prefix):\n",
    "    \"\"\"Generate a consistent cache filename based on bucket and prefix\"\"\"\n",
    "    # Create a clean filename by replacing invalid chars with underscores\n",
    "    safe_prefix = prefix.replace('/', '_').replace('\\\\', '_').rstrip('_')\n",
    "    return f\"s3_keys_cache_{bucket}_{safe_prefix}.json\"\n",
    "\n",
    "def save_keys_to_cache(bucket, prefix, pdf_keys):\n",
    "    \"\"\"Save the list of PDF keys to a local cache file\"\"\"\n",
    "    cache_file = get_cache_filename(bucket, prefix)\n",
    "    cache_data = {\n",
    "        \"bucket\": bucket,\n",
    "        \"prefix\": prefix,\n",
    "        \"timestamp\": time.time(),\n",
    "        \"pdf_keys\": pdf_keys\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(cache_data, f)\n",
    "        print(f\"Saved {len(pdf_keys)} keys to cache file: {cache_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving keys to cache: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_keys_from_cache(bucket, prefix):\n",
    "    \"\"\"Load the list of PDF keys from a local cache file if it exists\"\"\"\n",
    "    cache_file = get_cache_filename(bucket, prefix)\n",
    "    \n",
    "    if not os.path.exists(cache_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cache_data = json.load(f)\n",
    "            \n",
    "        # Verify cache matches current request\n",
    "        if cache_data.get(\"bucket\") != bucket or cache_data.get(\"prefix\") != prefix:\n",
    "            print(\"Cache mismatch: bucket or prefix has changed\")\n",
    "            return None\n",
    "            \n",
    "        pdf_keys = cache_data.get(\"pdf_keys\", [])\n",
    "        timestamp = cache_data.get(\"timestamp\", 0)\n",
    "        \n",
    "        # Calculate age of cache in hours\n",
    "        cache_age_hours = (time.time() - timestamp) / 3600\n",
    "        \n",
    "        print(f\"Loaded {len(pdf_keys)} keys from cache (age: {cache_age_hours:.2f} hours)\")\n",
    "        return pdf_keys\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading keys from cache: {e}\")\n",
    "        return None\n",
    "\n",
    "def speed_test_fitz_extraction(bucket, prefix):\n",
    "    \"\"\"Speed test pymupdf text extraction on PDFs in a bucket with parallel processing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Try to load keys from cache first\n",
    "    pdf_keys = load_keys_from_cache(bucket, prefix)\n",
    "    \n",
    "    # If no cache exists, perform pagination\n",
    "    if pdf_keys is None:\n",
    "        print(f\"No valid cache found. Paginating S3 bucket {bucket} with prefix {prefix}...\")\n",
    "        pdf_keys = []\n",
    "        \n",
    "        # List objects in the bucket with the specified prefix\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "        \n",
    "        # Collect all PDF keys\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' not in page:\n",
    "                continue\n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.pdf'):\n",
    "                    pdf_keys.append(key)\n",
    "        \n",
    "        # Save keys to cache for future use\n",
    "        save_keys_to_cache(bucket, prefix, pdf_keys)\n",
    "    \n",
    "    print(f\"Processing {len(pdf_keys)} PDF files...\")\n",
    "    \n",
    "    # Create a temporary directory for downloading PDFs\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Determine the number of processes to use (leave one core free)\n",
    "        num_processes = max(1, multiprocessing.cpu_count() - 1)\n",
    "        \n",
    "        # Create a partial function with fixed arguments\n",
    "        process_pdf_with_args = partial(process_pdf, bucket=bucket, temp_dir=temp_dir)\n",
    "        \n",
    "        # Process PDFs in parallel\n",
    "        results = []\n",
    "        with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "            # Submit all tasks and get futures\n",
    "            futures = [executor.submit(process_pdf_with_args, key) for key in pdf_keys]\n",
    "            \n",
    "            # Process results as they complete using tqdm for progress tracking\n",
    "            for future in tqdm(futures, desc=\"Processing PDFs\", total=len(pdf_keys)):\n",
    "                results.append(future.result())\n",
    "    \n",
    "    # Write results to JSONL file\n",
    "    with open(\"fitz_extraction_results.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nProcessed {len(results)} PDFs in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Average time per PDF: {elapsed_time/len(results):.2f} seconds\")\n",
    "    print(f\"Results saved to fitz_extraction_results.jsonl\")\n",
    "\n",
    "# Example usage\n",
    "# Replace with the same bucket and prefix from file_context_0\n",
    "dest_bucket = os.getenv('MINIO_BUCKET_NAME', 'neurips-2024')\n",
    "dest_prefix = 'neurips-500-speedtest/' \n",
    "\n",
    "# Run the speed test\n",
    "if __name__ == \"__main__\":\n",
    "    speed_test_fitz_extraction(dest_bucket, dest_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Metadata ---\n",
      "Text length: 67935 characters\n",
      "s3_path: neurips-500-speedtest/3D Structure Prediction of Atomic Systems with Flow-based Direct Preference Optimization.pdf\n",
      "Line 1 of 519:\n",
      "\n",
      "--- Document Text ---\n",
      "\n",
      "3D Structure Prediction of Atomic Systems with\n",
      "Flow-Based Direct Preference Optimization\n",
      "Rui Jiao1,2 Xiangzhe Kong1,2 Wenbing Huang3,4∗Yang Liu1,2∗\n",
      "1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University\n",
      "2Institute for AIR, Tsinghua University\n",
      "3Gaoling School of Artificial Intelligence, Renmin University of China\n",
      "4 Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\n",
      "Abstract\n",
      "Predicting high-fidelity 3D structures of atomic systems is a fundamental yet\n",
      "challenging problem in scientific domains. While recent work demonstrates the\n",
      "advantage of generative models in this realm, the exploration of different probability\n",
      "paths are still insufficient, and hallucinations during sampling are persistently\n",
      "occurring. To address these pitfalls, we introduce FlowDPO, a novel framework\n",
      "that explores various probability paths with flow matching models and further\n",
      "suppresses hallucinations using Direct Preference Optimization (DPO) for structure\n",
      "generation. Our approach begins with a pre-trained flow matching model to\n",
      "generate multiple candidate structures for each training sample. These structures are\n",
      "then evaluated and ranked based on their distance to the ground truth, resulting in\n",
      "an automatic preference dataset. Using this dataset, we apply DPO to optimize the\n",
      "original model, improving its performance in generating structures closely aligned\n",
      "with the desired reference distribution. As confirmed by our theoretical analysis,\n",
      "such paradigm and objective function are compatible with arbitrary Gaussian paths,\n",
      "exhibiting favorable universality. Extensive experimental results on antibodies and\n",
      "crystals demonstrate substantial benefits of our FlowDPO, highlighting its potential\n",
      "to advance the field of 3D structure prediction with generative models.\n",
      "1\n",
      "Introduction\n",
      "Predicting 3D structures of atomic systems is indispensable in various scientific domains, ranging\n",
      "from pharmaceutical drug design [1, 17] to materials science [7]. Accurate 3D modeling is not\n",
      "only crucial for understanding the physical and chemical properties of substances at the atomic\n",
      "level [2, 18] but also for simulating and predicting their behavior in various environments [3, 26].\n",
      "Nevertheless, it remains challenging due to the intricate nature of atomic interactions, the vastness of\n",
      "the conformational space, as well as limited resources of structure data.\n",
      "Conventional methods typically employ physics-based algorithms to derive structures at local energy\n",
      "optimum [24, 31, 32]. Recent advancements leverage deep generative model to learn the distribution\n",
      "of stable structures from available data, showcasing remarkable success across various domains. For\n",
      "example, DiffAb [20] designs a diffusion-based method for antigen-specific antibody design, which\n",
      "is further available for antibody structure prediction, and DiffCSP [14] proposes a joint diffusion\n",
      "framework for crystal structure prediction. Despite these advancements, generative models for\n",
      "structure prediction are confronted with two primary challenges.\n",
      "First, existing structure prediction methods predominantly utilize diffusion-based generative models.\n",
      "While effective, this focus narrows the scope of exploration into other probability paths that could\n",
      "∗Wenbing Huang and Yang Liu are corresponding authors.\n",
      "38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n",
      "\n",
      "𝑐!\n",
      "⋯\n",
      "≻\n",
      "≻\n",
      "𝑥!\n",
      "%𝑥!\"!\n",
      "%𝑥!\"\"\n",
      "𝑑(𝑥!, %𝑥!\")\n",
      "𝛿\n",
      "(b) Preference Dataset Construction\n",
      "𝜃#$%\n",
      "𝜃#$%\n",
      "𝜃&'(\n",
      "≻\n",
      "≻\n",
      "⋯\n",
      "𝒟#$%\n",
      "𝒟')*#\n",
      "ℒ+,-\n",
      "(c) Direct Preference Optimization\n",
      "Init.\n",
      "ℒ./\n",
      "ℒ.,\n",
      "ℒ-0\n",
      "𝜃#$%\n",
      "𝜃#$%\n",
      "𝜃#$%\n",
      "(a) Training Flow Models\n",
      "Figure 1: Overview of the proposed FlowDPO pipeline. As described in Section 3.1, the process\n",
      "begins by training a flow matching model, denoted as θref, using an arbitrary pre-defined Gaussian\n",
      "path. Next, as outlined in Section 3.2, we construct a preference dataset, Dpair, by evaluating the\n",
      "distances between generated samples ˆxij and the ground structure xi under a given context condition\n",
      "ci—such as an antibody sequence or crystal composition. These samples are derived from the\n",
      "reference training set Dref. This dataset is then used to fine-tune the model θopt through the DPO\n",
      "training objective LDPO, detailed in Section 3.3.\n",
      "potentially offer substantial benefits. A notable example is the Optimal Transport (OT) path, which\n",
      "has recently been demonstrated to be particularly effective in the field of molecular generation [28].\n",
      "Second, current training paradigm frequently leads to hallucinated distribution peaks [1]. Most\n",
      "generative models are trained through maximizing the likelihood or its lower bound on the ground-\n",
      "truth structures, which are easily haunted by hallucinations due to the lack of negative samples\n",
      "during training. In the field of natural language processing or computer vision, Direct Preference\n",
      "Optimization (DPO) [25, 30] is proposed to align the model with human preferences, which effectively\n",
      "reduces hallucinations. For 3D structure prediction, such preferences can be naturally extended to\n",
      "similarity with the reference structure (e.g. RMSD). However, it remains unclear whether the DPO\n",
      "method is compatible with arbitrary probability paths.\n",
      "To address the above pitfalls, we introduce FlowDPO, a novel framework that explores flexible\n",
      "selection of Gaussian paths and enhances the quality of generated structures by alignment with the\n",
      "reference distribution. Specifically, we approach the structure prediction task via flow matching\n",
      "models regarding various paths. Given a pre-trained flow matching model, we sample multiple\n",
      "structures for each entry in the training set, evaluate these candidates against known ground truths to\n",
      "compute similarity, and construct an automatic preference dataset. Notably, we theoretically derive\n",
      "the unified objective of DPO for arbitrary Gaussian paths, and leverage the preference to enhance\n",
      "the performance of the original generative model. Intuitively, such a paradigm not only augment\n",
      "data with self-distilled samples, but also endow the model with the ability to distinguish between\n",
      "high-fidelity and hallucinated samples.\n",
      "In summary, our contributions are threefold:\n",
      "• We explore multiple accessible probability paths for the 3D structure prediction task, and to\n",
      "the best of our knowledge, we are the first to theoretically prove the compatibility of DPO\n",
      "with arbitrary Gaussian paths by deriving a unified objective.\n",
      "• Based on the theoretical results, we develop a novel framework to encourage better alignment\n",
      "of flow matching models with desired reference distribution in 3D structure prediction, which\n",
      "effectively suppresses the probability of hallucinations.\n",
      "• Our approach yields promising results on antibody and crystal structure prediction tasks,\n",
      "showcasing the versatility and efficacy of our FlowDPO.\n",
      "2\n",
      "\n",
      "2\n",
      "Related Work\n",
      "Structure Prediction for Atomic Systems.\n",
      "3D Structure prediction, including predicting con-\n",
      "formations from molecular topological graphs [34], determining unit cell structures from crystal\n",
      "compositions [14], or inferring structures based on protein sequences [1], is crucial in computational\n",
      "chemistry and material science. Traditionally, these predictions have relied on physics-inspired\n",
      "scoring functions [21, 11] or density functional theory (DFT)-based energy calculations [10] to define\n",
      "the search space, with subsequent application of search algorithms to identify optimal structures.\n",
      "Recently, deep generative methods, particularly diffusion models [12, 27], have proven to be highly\n",
      "effective in this field. These models have been successfully applied across multiple specific do-\n",
      "mains, including small molecules [34], crystals [14], antibodies [20], complexes [6], and general\n",
      "biomolecules [1]. The emergence of flow matching models [19], which generalize diffusion paths\n",
      "to more flexible probability flows, has further enhanced the generative capabilities for geometric\n",
      "graphs [28]. The goal of our work is to explore structure prediction from the perspective of flow\n",
      "matching, and align these models towards more accurate predictions.\n",
      "Aligning Generative Models.\n",
      "In the domain of generative model alignment, recent work has\n",
      "focused on refining models to better meet human preferences. Direct Preference Optimization (DPO),\n",
      "introduced by [25], offers a significant advancement over traditional Reinforcement Learning from\n",
      "Human Feedback (RLHF, [23]) methods by directly optimizing a policy based on human preference\n",
      "data. This approach has proven effective in aligning large language models (LLMs) with user\n",
      "expectations. Extending this concept, [30] propose Diffusion-DPO, a novel method that adapts DPO\n",
      "for text-to-image diffusion models. By reformulating the preference optimization for diffusion model\n",
      "likelihoods, Diffusion-DPO achieves state-of-the-art performance in generating images that are not\n",
      "only visually appealing but also closely aligned with textual prompts. Recently, [36] introduces\n",
      "ABDPO, a DPO-based method tailored for antibody design. Unlike ABDPO, which concentrates on\n",
      "guiding diffusion models to generate antibody candidates with lower energy, our approach emphasizes\n",
      "aligning flow models for precise structure predictions.\n",
      "3\n",
      "FlowDPO\n",
      "3.1\n",
      "Flow Matching for Geometric Graphs\n",
      "Flow Matching (FM, [19]) is a general paradigm for generative tasks by learning a vector field to\n",
      "connect the pre-defined prior distribution with the targeted data distribution. Let q denote the data\n",
      "distribution, x0 is a data point acquired from p0 = q, and x1 is a random sample from the prior\n",
      "distribution p1. A time-dependent flow ψt is then defined to shift samples from the prior distribution\n",
      "to the time-dependent distribution pt via the vector field vt, that is\n",
      "ψ1(x) = x1, d(ψt(x))\n",
      "dt\n",
      "= vt(ψt(x)).\n",
      "(1)\n",
      "The vector field can be further parameterized by a time-dependent model vθ(xt, t), leading to the\n",
      "continuous normalizing flows (CNFs, [5]). To avoid numerical ODE simulations to train vθ, FM\n",
      "simplifies the training target by aligning the model with a pre-defined vector field ut to yield pt, i.e.,\n",
      "LFM = Et,xt∼pt(xt)[∥vθ(xt, t) −ut(xt)∥2\n",
      "2].\n",
      "(2)\n",
      "However, as pt is still unknown, we are still unable to sample xt and apply the above objective.\n",
      "To address this gap, [19] leverages the more accessible conditional vector field ut(xt|x0) and its\n",
      "corresponding probability path pt(xt|x0), resulting in the following Conditional Flow Matching\n",
      "(CFM) objective, which is equivalent to LFM in terms of gradients and accessible for sampling:\n",
      "LCFM = Et,xt∼pt(xt)[∥vθ(xt, t) −ut(xt|x0)∥2\n",
      "2].\n",
      "(3)\n",
      "Different vector fields lead to different probability paths. For the commonly-used Gaussian distribu-\n",
      "tion defined as\n",
      "pt(xt|x0) = N(xt; µt(x0), σ2\n",
      "t (x0)),\n",
      "(4)\n",
      "3\n",
      "\n",
      "the corresponding vector field [19] is calculated as\n",
      "ut(xt|x0) = µ′\n",
      "t(x0) + σ′\n",
      "t(x0)\n",
      "σt(x0)(x −µt(x0)),\n",
      "(5)\n",
      "where µ′\n",
      "t, σ′\n",
      "t are derivatives of µt, σt w.r.t. t. We consider three lines of Gaussian paths in this paper,\n",
      "namely the Variance Exploding (VE), Variance Preserving (VP) and Optimal Transport (OT) paths,\n",
      "which are listed in Table 1.\n",
      "Table 1: Parameters of different Gaussian paths. VE, VP and OT represent Variable Exploding,\n",
      "Variable Preserving and Optimal Transport, respectively.\n",
      "Probability Path\n",
      "Mean\n",
      "Standard Deviation\n",
      "Conditional Vector Field\n",
      "VE path\n",
      "µt(x0) = x0\n",
      "σt(x0) = σt\n",
      "ut(xt|x0) = σ′\n",
      "t\n",
      "σt (xt −x0)\n",
      "VP path\n",
      "µt(x0) = αtx0\n",
      "σt(x0) =\n",
      "p\n",
      "1 −α2\n",
      "t\n",
      "ut(xt|x0) =\n",
      "α′\n",
      "t\n",
      "1−α2\n",
      "t (αtxt −x0)\n",
      "OT path\n",
      "µt(x0) = (1 −t)x0\n",
      "σt(x0) = t\n",
      "ut(xt|x0) = 1\n",
      "t (xt −x0)\n",
      "Based on these paths, we are capable of designing proper flow models to maintain symmetries for\n",
      "specific structure prediction tasks. In this paper, we mainly focus on the two typical tasks on atomic\n",
      "systems: antibody structure prediction and crystal structure prediction. Note that symmtries are\n",
      "crucial in 3D atomic systems, and we provide more discussions in Appendix B.\n",
      "Example 1: Antibody Structure Prediction. Antibodies are Y-shaped proteins generated by the\n",
      "immune system to identify and bind to specific antigens, with the structure depicted in Figure 2.\n",
      "Researchers mainly center on the variable domains of antibodies, which comprise a heavy chain and\n",
      "a light chain. Each chain includes three Complementarity-Determining Regions (CDRs) and four\n",
      "framework regions in an alternating sequence. The six CDRs are volatile and crucial in defining\n",
      "the binding specificity and affinity, while the framework regions remain conserved. Among them,\n",
      "CDR-H3, which is the third CDR on the heavy chain, is the most diverse region and the primary\n",
      "focus of antibody design. Therefore, it is a fundamental yet challenging problem to accurately predict\n",
      "the structure of the CDRs upon binding.\n",
      "FR1\n",
      "H1\n",
      "FR2\n",
      "H2\n",
      "FR3\n",
      "H3\n",
      "FR4\n",
      "Heavy Chain\n",
      "FR1\n",
      "L1\n",
      "FR2\n",
      "L2\n",
      "FR3\n",
      "L3\n",
      "FR4\n",
      "Light Chain\n",
      "Antigen\n",
      "Figure 2: Graphical depiction of antibody variable domains, which consist of a heavy chain and a light\n",
      "chain. Each chain is equipped with 4 Framework Regions (FRs) and 3 Complementarity-Determining\n",
      "Regions (CDRs). The CDRs, especially CDR-H3, are volatile and thus are the key focus.\n",
      "Task Definition: Let A = {a1, a2, · · · , aN} denote the sequence of the targeted CDR region with the\n",
      "length of N, where ai ∈{0, 1}20 is the one-hot type of the amino acid, and ⃗X = {⃗x1, ⃗x2, · · · , ⃗xN}\n",
      "is the corresponding 3D structures with xi ∈R3×4 as the backbone coordinates including N, Cα, C,\n",
      "and O. Similarly, the sequence and structure of the context (i.e. framework regions and the antigen)\n",
      "are defined as AC, ⃗XC. The goal is to predict the structure of the CDR region given the context:\n",
      "⃗X ∼p0( ⃗X|A, ⃗XC, AC).\n",
      "(6)\n",
      "Probability Paths and Training Objectives: DiffAb [20] has designed the VP path for the coordinates\n",
      "of the CDR region as\n",
      "⃗ut,VP( ⃗Xt| ⃗X0, A, ⃗XC, AC) =\n",
      "α′\n",
      "t\n",
      "1 −α2\n",
      "t\n",
      "(αt ⃗Xt −⃗X0),\n",
      "(7)\n",
      "where αt is scheduled as αt = e−1\n",
      "2\n",
      "R t\n",
      "0 β(s)ds. After sampling ⃗X0 = ⃗ϵ ∼N(0, I), we have\n",
      "⃗Xt = αt ⃗X0 +\n",
      "p\n",
      "1 −α2\n",
      "t⃗ϵ. With proper reparameterization, the training objective is defined as\n",
      "LVP = Et,⃗ϵ\n",
      "\u0002\n",
      "∥⃗ϵθ( ⃗Xt, A, ⃗XC, AC) −⃗ϵ∥2\n",
      "2\n",
      "\u0003\n",
      ",\n",
      "(8)\n",
      "4\n",
      "\n",
      "which only requires a model θ to predict the denoising term given the current state.\n",
      "Moreover, it is also practicable to linearly connect the data point ⃗X0 and the noisy prior ⃗ϵ via the OT\n",
      "path as ⃗Xt = (1 −t) ⃗X0 + t⃗ϵ. The vector field is then defined as\n",
      "⃗ut,OT( ⃗Xt| ⃗X0, A, ⃗XC, AC) = 1\n",
      "t ( ⃗Xt −⃗X0) = ⃗ϵ −⃗X0.\n",
      "(9)\n",
      "The training objective directly align the model with the simple vector field:\n",
      "LOT = Et,⃗ϵ\n",
      "\u0002\n",
      "∥⃗vθ( ⃗Xt, A, ⃗XC, AC) −(⃗ϵ −⃗X0)∥2\n",
      "2\n",
      "\u0003\n",
      ".\n",
      "(10)\n",
      "Example 2: Crystal Structure Prediction. Crystal Structure Prediction (CSP), a fundamental\n",
      "aspect of material science, requires to predict the stable 3D structure of a compound solely from its\n",
      "composition. Unlike molecules or proteins, which have a finite number of atoms, the uniqueness of\n",
      "crystals lies in their periodic repetition in infinite 3D space. The infinite crystal structure is typically\n",
      "simplified by its repeating unit, which is called a unit cell. The key point of CSP is the representation\n",
      "and generation of the unit cell.\n",
      "Task Definition: A unit cell is usually characterized by a triplet M = (A, L, F ), where A =\n",
      "[a1, a2, ..., aN] ∈Rh×N represents the one-hot encoded atom types, L = [l1, l2, l3] ∈R3×3\n",
      "denotes the lattice matrix with three basis vectors describing the crystal’s periodicity, and F =\n",
      "[x1, x2, ..., xN] ∈R3×N\n",
      "[0,1) contains the fractional coordinates of the atoms, specifying their positions\n",
      "relative to the lattice matrix. The goal of CSP is to predict the lattice matrix and the atomic coordinates\n",
      "based on the given crystal composition as\n",
      "(L, F ) ∼p0(L, F |A).\n",
      "(11)\n",
      "𝒇!\n",
      "𝒇\"\n",
      "𝒇#\n",
      "Figure 3: A crystal is the infinite peri-\n",
      "odic arrangement of atoms, and the re-\n",
      "peating unit is named as a unit cell.\n",
      "Probability Paths and Training Objectives: As the lattice\n",
      "matrix L also lies in the Euclidean space, we can design\n",
      "similar VP and OT paths as Eq. (7-10). Given ϵL ∼\n",
      "N(0, I), with Lt = αtL0+\n",
      "p\n",
      "1 −α2\n",
      "tϵL, the loss function\n",
      "of the VP path is defined as\n",
      "LL,VP = Et,ϵL\n",
      "\u0002\n",
      "∥ϵL,θ(Lt, Ft, A) −ϵL∥2\n",
      "2\n",
      "\u0003\n",
      ".\n",
      "(12)\n",
      "Besides, with Lt = (1−t)L0+tϵL, the training objective\n",
      "of the OT path is similarly defined as\n",
      "LL,OT = Et,ϵL\n",
      "\u0002\n",
      "∥vL,θ(Lt, Ft, A) −(ϵL −L0)∥2\n",
      "2\n",
      "\u0003\n",
      ".\n",
      "(13)\n",
      "The fractional coordinates lie in the torus space of R3×N\n",
      "[0,1) to inherently reflect the periodicity of the\n",
      "crystal. Previous works [15, 14] project the VE path to this manifold, and the Gaussian distribution is\n",
      "changed into the Wrapped Normal (WN) distribution as pt(Ft|F0) = Nw(Ft; F0, σ2\n",
      "t I),\n",
      "pt(Ft|F0) = Nw(Ft; F0, σ2\n",
      "t I),\n",
      "(14)\n",
      "where Nw(x; ·, ·) = P∞\n",
      "i=−∞Nw(x + i; ·, ·). An accessible way to learn this path is to match the\n",
      "score, i.e. the negative logarithmic gradient, of pt, and the loss function is defined as\n",
      "LF ,VE = Et,Ft\n",
      "\u0002\n",
      "λt∥ϵF ,θ(Lt, Ft, A) −∇Ft log pt(Ft|F0)∥2\n",
      "2\n",
      "\u0003\n",
      ",\n",
      "(15)\n",
      "where λt = E−1\u0002\n",
      "∥∇log Nw(0, σ2\n",
      "t )∥2\n",
      "2\n",
      "\u0003\n",
      "is the pre-computed weight. If σ1 in Eq. (14) is sufficiently\n",
      "large, p1 would finally approach the uniform distribution, which can be selected as the prior dis-\n",
      "tribution. Apart from the VE path, it is also applicable to directly connect the data point and the\n",
      "prior sample via the shortest path on the manifold. Specfically, given F0 ∼p0, F1 ∼p1, where\n",
      "p1 is defined as the uniform distribution, the shortest path s(F0, F1) can be determined by the\n",
      "logarithmic map from F0 to F1 as s(F0, F1) = logF0 F1 = w(F1 −F0 + 0.5) −0.5. Alternatively,\n",
      "F1 can also be considered as the destination of s(F0, F1) via the exponential map from F0 as\n",
      "expF0 s(F0, F1) = w(F0 + s(F0, F1)). To eliminate the effect of the overall translation introduced\n",
      "by the prior, we further normalize F1 as ˆF1 = expF0 ˆs(F0, F1) = expF0\n",
      "\u0000s(F0, F1) −¯s(F0, F1)\n",
      "\u0001\n",
      ",\n",
      "5\n",
      "\n",
      "where ¯s averages the paths of all atoms. With the path of Ft defined as Ft = expF0\n",
      "\u0000tˆs(F0, F1)\n",
      "\u0001\n",
      ",\n",
      "the training objective for the OT path is\n",
      "LF ,OT = Et,F1\n",
      "h\n",
      "∥vF ,θ(Lt, Ft, A) −ˆs(F0, F1)∥2\n",
      "2\n",
      "i\n",
      ".\n",
      "(16)\n",
      "Generalized Notations. Overall, the structure prediction tasks aims at generating the targeted\n",
      "structure x given some condition c, i.e. to learn p0(x|c). And the flow matching objective min-\n",
      "imizes the Mean Square Error (MSE) of the predicted and pre-defined vector fields with proper\n",
      "reparameterization or simplification, which can be generalized as\n",
      "L = Et,x0∼p0,x1∼p1[MSEt(x0, x1; θ)].\n",
      "(17)\n",
      "Hereinafter, we use these generalized notations for simplicity.\n",
      "3.2\n",
      "Preference Dataset Construction\n",
      "Building on the flow paths introduced in § 3.1, we now delve into the details of constructing a\n",
      "preference dataset, which is pivotal for the application of DPO, as detailed in § 3.3.\n",
      "Algorithm 1 Candidate Generation\n",
      "1: Input: N, M, Dref = {(xi, ci)}N\n",
      "i=1, θref,\n",
      "d(·, ·), δ\n",
      "2: Output: Dgen, Dpos, check\n",
      "3: Initialize: Dgen, Dpos, check ←[], [], []\n",
      "4: for i = 1 to N do\n",
      "5:\n",
      "Dgen[i], Dpos[i] ←[], []\n",
      "6:\n",
      "match ←False, jpos ←1\n",
      "7:\n",
      "for j = 1 to M do\n",
      "8:\n",
      "Generate ˆxij ∼p(x|ci; θref)\n",
      "9:\n",
      "Dgen[i, j] ←ˆxij\n",
      "10:\n",
      "if d(xi, ˆxij) ≤δ then\n",
      "11:\n",
      "Dpos[i, jpos] ←ˆxij\n",
      "12:\n",
      "match ←True, jpos ←jpos + 1\n",
      "13:\n",
      "end if\n",
      "14:\n",
      "end for\n",
      "15:\n",
      "check[i] ←match\n",
      "16: end for\n",
      "Algorithm 2 Preference Pairs Construction\n",
      "1: Input: Dref, Dgen, Dpos, check, N, K,\n",
      "d(·, ·), r\n",
      "2: Output: Dpair\n",
      "3: Initialize: Dpair ←[]\n",
      "4: for i = 1 to N do\n",
      "5:\n",
      "Dpair[i] ←[]\n",
      "6:\n",
      "for k = 1 to K do\n",
      "7:\n",
      "if k ≤rK or check[i] = False then\n",
      "8:\n",
      "xw\n",
      "ik ←xi\n",
      "9:\n",
      "xl\n",
      "ik ∼Dgen,i\n",
      "10:\n",
      "else\n",
      "11:\n",
      "xw\n",
      "ik ∼Dpos,i, xl\n",
      "ik ∼Dgen,i\n",
      "12:\n",
      "Swap if d(xi, xw\n",
      "ik) > d(xi, xl\n",
      "ik)\n",
      "13:\n",
      "end if\n",
      "14:\n",
      "Dpair[i, j] ←(xw\n",
      "ik, xl\n",
      "ik, ci)\n",
      "15:\n",
      "end for\n",
      "16: end for\n",
      "Candidate Generation As shown in Algorithm 1, the construction of the preference dataset\n",
      "begins with the generation of multiple candidate structures for each sample in our reference dataset,\n",
      "Dref. Leveraging the pre-trained flow-based generative model θref, we generate M candidate\n",
      "structures {ˆxij}M\n",
      "j=1 for each sample (xi, ci) via p(x|ci; θref), ensuring that each generated structure\n",
      "is contextually relevant and adheres to the geometric constraints discussed previously.\n",
      "As each candidate is generated, we compute the distance between ˆxij and the original structure xi\n",
      "using a predefined metric d(·, ·). If this distance is less than or equal to a threshold δ, the candidate is\n",
      "considered a close match and is added to Dpos, a subset of promising candidates. This step is crucial\n",
      "for efficiently filtering the generated data to retain only the most relevant candidates for DPO.\n",
      "Preference Pairs Construction Subsequently, we construct K preference pairs (xw\n",
      "ik, xl\n",
      "ik) for each\n",
      "sample i by Algorithm 2, where xw\n",
      "ik is preferred over xl\n",
      "ik. This preference is determined based on\n",
      "their proximity to the original structure xi. Apart from sampling pairs from generated structures,\n",
      "we also use a ratio r to select the ground truth as the preferred sample. Moreover, if all generated\n",
      "structures for a sample are far from the original, the original structure xi is always preferred. The\n",
      "other pairs are formed by selecting xw\n",
      "ik from the promising subset Dpos and xl\n",
      "ik from the broader\n",
      "set Dgen. This process ensures that the pairs reflect a clear preference based on the closeness to the\n",
      "original structure, facilitating effective training through DPO, which is explored in the next section.\n",
      "6\n",
      "\n",
      "3.3\n",
      "Direct Preference Optimization\n",
      "To align large language models with human preference, DPO [25] is proposed to replace the\n",
      "RLHF [23] training objective with directly maximizing the likelihood of the preference. [30] extends\n",
      "DPO to text-to-image generation task, adapting the DPO target to diffusion models. Given the\n",
      "preference pair (xw, xl), DPO [25] designs the training objective as\n",
      "LDPO = −Exw,xl\n",
      "h\n",
      "log σ\n",
      "\u0000β log popt(xw)\n",
      "pref(xw) −β log popt(xl)\n",
      "pref(xl)\n",
      "\u0001i\n",
      ",\n",
      "(18)\n",
      "where popt, pref are probabilities yielded by the fine-tuned model θopt and the pre-trained flow model,\n",
      "and β is a hyperparameter to control the KL divergence of these two distributions.\n",
      "It is nontrivial to efficiently acquire p(x) via iterative generative models. Inspired by [30], we\n",
      "uniformly discretize the time interval into T steps, where step i is located at t = i/T. By formulating\n",
      "the probability from the path x0:T , Eq. (18) can be rewritten as\n",
      "LDPO = −Exw,xl log σ\n",
      "\u0010\n",
      "βExw\n",
      "1:T ,xl\n",
      "1:T\n",
      "\u0002\n",
      "log popt(xw\n",
      "0:T )\n",
      "pref(xw\n",
      "0:T ) −log popt(xl\n",
      "0:T )\n",
      "pref(xl\n",
      "0:T )\n",
      "\u0003\u0011\n",
      ".\n",
      "(19)\n",
      "To avoid costly sampling through the entire path, Jensen’s inequality [30] is applied to bound Eq. (19)\n",
      "as\n",
      "LDPO ≤−Exw,xl,i log σ\n",
      "\u0010\n",
      "B\n",
      "\u0002\n",
      "log popt(xw\n",
      "i−1|xw\n",
      "i )\n",
      "pref(xw\n",
      "i−1|xw\n",
      "i ) −log popt(xl\n",
      "i−1|xl\n",
      "i)\n",
      "pref(xl\n",
      "i−1|xl\n",
      "i)\n",
      "\u0003\u0011\n",
      ",\n",
      "(20)\n",
      "where B = βT servers as a hyperparameter. As directly sampling xi−1, xi from an arbitrary\n",
      "intermediate step i is still unfeasible, we can estimate them via the accessible Gaussian paths p in\n",
      "Table 1 as\n",
      "LDPO = −Exw,xl,i log σ\n",
      "\u0010\n",
      "BEp(xw\n",
      "i−1|xw\n",
      "i,0),p(xw\n",
      "i−1|xl\n",
      "i,0)\n",
      "\u0002\n",
      "log popt(xw\n",
      "i−1|xw\n",
      "i )\n",
      "pref(xw\n",
      "i−1|xw\n",
      "i ) −log popt(xl\n",
      "i−1|xl\n",
      "i)\n",
      "pref(xl\n",
      "i−1|xl\n",
      "i)\n",
      "\u0003\u0011\n",
      "(21)\n",
      "= −Exw,xl,i log σ\n",
      "\u0010\n",
      "B\n",
      "\u0002\n",
      "J (xw\n",
      "i ; p, pref) −J (xw\n",
      "i ; p, popt) −J (xl\n",
      "i; p, pref) + J (xl\n",
      "i; p, popt)\n",
      "\u0003\u0011\n",
      ",\n",
      "(22)\n",
      "where J (xw\n",
      "i ; p, pθ) denotes DKL\n",
      "\u0000p(xw\n",
      "i−1|xw\n",
      "i,0)∥pθ(xw\n",
      "i−1|xw\n",
      "i )\n",
      "\u0001\n",
      "and the same for J (xl\n",
      "i; p, pθ). As p\n",
      "and pθ are Gaussian distributions with the same noise scheduler, the KL divergence can be formulated\n",
      "as\n",
      "J (xi; p, pθ) =\n",
      "1\n",
      "2σ2\n",
      "i−1|i\n",
      "µ(xi−1|xi,0) −µθ(xi−1|xi)\n",
      "\n",
      "2\n",
      "2.\n",
      "(23)\n",
      "According to DDIM [27], if a time-dependent Gaussian path follows the form xi ∼N(xi; kix0, σiI),\n",
      "we can further design p(xi−1|xi,0) = N\n",
      "\u0000x; µ(xi−1|xi,0), σ2\n",
      "i−1|i\n",
      "\u0001\n",
      ". Given σ2\n",
      "i−1|i, the mean can be\n",
      "formulated as\n",
      "µ(xi−1|xi,0) = 1\n",
      "σi\n",
      "q\n",
      "σ2\n",
      "i−1 −σ2\n",
      "i−1|ixi +\n",
      "\u0010\n",
      "ki−1 −ki\n",
      "σi\n",
      "q\n",
      "σ2\n",
      "i−1 −σ2\n",
      "i−1|i\n",
      "\u0011\n",
      "x0.\n",
      "(24)\n",
      "Fortunately, all paths defined in Table 1 follows this form. And µθ(xi−1|xi) can be parameterized\n",
      "similarly as Eq. (24), with estimating x0 via predicted vector field or denoising terms. Hence, we can\n",
      "approximate J (xi; p, pθ) by MSEi(x0, x1; θ). With sufficiently large T, Eq. (??) can be changed\n",
      "into an applicable form as follows, which is our final training objective.\n",
      "LDPO = −Exw\n",
      "0,1,xl\n",
      "0,1,t log σ\n",
      "\u0010\n",
      "B\n",
      "\u0002\n",
      "MSEt(xw\n",
      "0 , xw\n",
      "1 ; θref) −MSEt(xw\n",
      "0 , xw\n",
      "1 ; θopt)\n",
      "−MSEt(xl\n",
      "0, xl\n",
      "1; θref) + MSEt(xl\n",
      "0, xl\n",
      "1; θopt)\n",
      "\u0003\u0011\n",
      ",\n",
      "(25)\n",
      "7\n",
      "\n",
      "Table 2: Cα and bb indicates RMSD calculated on Cα atoms and backbone atoms, repectively. Cα-w\n",
      "and bb-w averages the RMSDs of the worst generated conformations of each complex.\n",
      "Model\n",
      "L1\n",
      "L2\n",
      "L3\n",
      "Cα-w\n",
      "Cα\n",
      "bb-w\n",
      "bb\n",
      "Cα-w\n",
      "Cα\n",
      "bb-w\n",
      "bb\n",
      "Cα-w\n",
      "Cα\n",
      "bb-w\n",
      "bb\n",
      "VP Path [20]\n",
      "2.71\n",
      "2.00\n",
      "2.56\n",
      "2.06\n",
      "1.11\n",
      "0.95\n",
      "1.08\n",
      "0.96\n",
      "1.32\n",
      "0.99\n",
      "1.39\n",
      "1.08\n",
      "OT Path\n",
      "2.25\n",
      "1.77\n",
      "2.24\n",
      "1.83\n",
      "1.13\n",
      "0.96\n",
      "1.10\n",
      "0.96\n",
      "1.49\n",
      "1.05\n",
      "1.45\n",
      "1.13\n",
      "VP Path + DPO\n",
      "2.47\n",
      "1.91\n",
      "2.31\n",
      "1.95\n",
      "1.09\n",
      "0.94\n",
      "1.07\n",
      "0.94\n",
      "1.22\n",
      "0.94\n",
      "1.30\n",
      "1.01\n",
      "OT Path + DPO\n",
      "2.22\n",
      "1.74\n",
      "2.19\n",
      "1.78\n",
      "1.09\n",
      "0.93\n",
      "1.05\n",
      "0.93\n",
      "1.28\n",
      "0.95\n",
      "1.34\n",
      "1.05\n",
      "Model\n",
      "H1\n",
      "H2\n",
      "H3\n",
      "Cα-w\n",
      "Cα\n",
      "bb-w\n",
      "bb\n",
      "Cα-w\n",
      "Cα\n",
      "bb-w\n",
      "bb\n",
      "Cα-w\n",
      "Cα\n",
      "bb-w\n",
      "bb\n",
      "VP Path [20]\n",
      "1.18\n",
      "0.83\n",
      "1.14\n",
      "0.89\n",
      "1.41\n",
      "0.92\n",
      "1.45\n",
      "1.00\n",
      "5.01\n",
      "3.77\n",
      "4.95\n",
      "3.78\n",
      "OT Path\n",
      "1.31\n",
      "0.89\n",
      "1.26\n",
      "0.94\n",
      "1.69\n",
      "1.06\n",
      "1.60\n",
      "1.13\n",
      "4.81\n",
      "3.66\n",
      "4.83\n",
      "3.70\n",
      "VP Path + DPO\n",
      "1.13\n",
      "0.80\n",
      "1.13\n",
      "0.86\n",
      "1.35\n",
      "0.87\n",
      "1.37\n",
      "0.95\n",
      "4.42\n",
      "3.44\n",
      "4.38\n",
      "3.45\n",
      "OT Path + DPO\n",
      "1.23\n",
      "0.83\n",
      "1.19\n",
      "0.89\n",
      "1.46\n",
      "0.95\n",
      "1.41\n",
      "1.02\n",
      "4.28\n",
      "3.32\n",
      "4.23\n",
      "3.32\n",
      "4\n",
      "Experiments\n",
      "We validate our method on two distinct domains: antibody structure prediction (§ 4.1) and crystal\n",
      "structure prediction (§ 4.2).\n",
      "4.1\n",
      "Antibody Structure Prediction\n",
      "Dataset\n",
      "Following previous literature [20], we extract antibody structures from the SAbDab\n",
      "database [8] for training and utilize the manually curated test set from DiffAb [20], which con-\n",
      "tains 19 antibody-antigen complexes. We first derive all structures deposited before April 11th, 2024,\n",
      "and remove those with resolution above 4.0Å or non-protein targets, resulting in 12,428 antibodies.\n",
      "Subsequently, we use mmseqs2 [29] to cluster the antibodies based on 50% sequence identity for\n",
      "each CDR, and exclude those in the same clusters as the test set antibodies. The dataset is then split\n",
      "into training and validation sets at a 9:1 ratio based on the clusters.\n",
      "Metrics We employ the following metrics for evaluation. RMSDCα measures the Root Mean Square\n",
      "Deviation of the generated alpha carbon coordinates with respect to the reference. RMSDbb is the\n",
      "RMSD calculated on the four backbone atoms including C, Cα, N, O. To better profile the generated\n",
      "distribution, for each antibody, we generate 20 structures and use two strategies to aggregate the\n",
      "results across different antibodies. Strategy worst select the worst generated structure per antibody\n",
      "according to RMSD and then averagse across different antibodies, while strategy mean averages\n",
      "the RMSD of 20 candidates first, and then across antibodies. Strategy worst measures the furthest\n",
      "deviation of the generated distribution compared to the reference, while strategy mean is commonly\n",
      "adopted in previous works [20, 16]. Results aggregated with worst are denoted as Cα-w and bb-w,\n",
      "while those with mean are denoted as Cα and bb.\n",
      "Results We evaluate VP path (DiffAb) [20] and OT path [19] with the proposed FlowDPO on\n",
      "CDR structure prediction. Results in Table 2 illustrate that either using VP path or OT path, further\n",
      "training with DPO consistently enhances performance across different CDRs. Notably, on the most\n",
      "challenging part (i.e. CDR-H3), the DPO phase yields the most significant improvement. Metrics\n",
      "aggregated with strategy worst demonstrate noticeable gains, indicating effective supperssion of\n",
      "low-quality samples by the DPO phase, which we attribute to the objective of DPO in distinguishing\n",
      "the prefered samples. Such characteristics are favorable in practical applications where it requires\n",
      "blind selection of generated structures without prior knowledge of which structures might be more\n",
      "correct. We also depict the distributions of RMSD and examples of generated CDR-H3 structures\n",
      "in Figure 4. It shows that the blue curves, yielded by the original flow models, often exhibit a\n",
      "bimodal distribution. While the first peak at a lower RMSD indicating higher quality generations, the\n",
      "second peak at a higher RMSD suggests the models experience hallucination, confidently generating\n",
      "conformations that significantly deviate from the ground truth. DPO effectively suppresses this\n",
      "erroneous second peak, leading to an overall improvement in the quality of generated samples. On\n",
      "closer inspection, this correction also addresses physical invalidities, such as the twisted backbone\n",
      "seen in Figure 4.\n",
      "8\n",
      "\n",
      "Reference (PDB: 7CHE)\n",
      "VP path\n",
      "VP path + DPO\n",
      "RMSDC⍺ = 2.09Å\n",
      "RMSDbb = 2.33Å\n",
      "RMSDC⍺ = 1.69Å\n",
      "RMSDbb = 1.67Å\n",
      "Reference (PDB: 7D6I)\n",
      "RMSDC⍺ = 1.30Å\n",
      "RMSDbb = 1.83Å\n",
      "OT path\n",
      "OT path + DPO\n",
      "RMSDC⍺ = 1.14Å\n",
      "RMSDbb = 1.37Å\n",
      "Figure 4: Examples of generated CDR-H3 structures and the distribution of RMSDCα for different\n",
      "antigen-antibody complexes and different probability paths. The visualized samples are the ones with\n",
      "the lowest RMSD of all the generated counterparts for the corresponding complexes. In addition to\n",
      "driving the distribution towards lower RMSD, it is also observed that the DPO phase tends to rectify\n",
      "the physical invalidity (e.g. twisted backbone in the above examples) in the generated samples.\n",
      "4.2\n",
      "Crystal Structure Prediction\n",
      "Dataset We conduct the crystal structure prediction task on three datasets in line with previous\n",
      "works [33, 14]. Perov-5 [4] includes 18,928 perovskite crystals, each characterized by similar\n",
      "structures but varying compositions, and exactly 5 atoms per unit cell. MP-20 [13] comprises\n",
      "45,231 materials from the Materials Project, featuring a wide range of compositions and structures,\n",
      "with each material containing no more than 20 atoms per unit cell. These materials predominantly\n",
      "represent crystals that have been synthesized experimentally. MPTS-52 is an advanced version of\n",
      "MP-20, containing 40,476 structures with unit cells that include up to 52 atoms, presenting a more\n",
      "complex challenge. For Perov-5 and MP-20, we maintain the conventional 60-20-20 split for training,\n",
      "validation, and testing. For the MPTS-52 dataset, we use a chronological split, assigning 27,380\n",
      "crystals for training, 5,000 for validation, and 8,096 for testing.\n",
      "Metrics For inference, we generate one structure given each composition. The predicted sample is\n",
      "then matched with the ground truth via the StructureMatcher class in pymatgen [22] with thresholds\n",
      "stol=0.5, angle_tol=10, ltol=0.3 as applied in previous works [33, 14]. We use Match Rate (MR)\n",
      "as the proportion of matched structures among the testing set, and the RMSD is averaged over the\n",
      "matched pairs, and normalized by\n",
      "3p\n",
      "V/N where V is the volume of the unit cell.\n",
      "Results We compare the results with two generative baselines P-cG-SchNet [9] and CDVAE [33].\n",
      "The results are shown in Table 3, where we explore three combinations of paths for jointly generating\n",
      "the lattice and the fractional coordinates: VP+VE, OT+OT, and OT+VE. Notably, the VP+VE path\n",
      "is previously developed by DiffCSP [14]. We find that the OT path is more effective for lattice\n",
      "generation, while the VE path provides more accurate predictions of atomic coordinates within the\n",
      "cell. Overall, the OT+VE combination generally delivers the best performance. Furthermore, DPO\n",
      "consistently enhances the performance of the model trained on each combination, demonstrating\n",
      "its capability to refine the predictions to a more precise alignment with experimental structures.\n",
      "We additionally visualize the RMSD distribution of predicted structures from different Gaussian\n",
      "paths. Results in Figure 5 reveal a similar pattern to Figure 4, demonstrating that DPO reduces the\n",
      "probability of low-quality generations.\n",
      "5\n",
      "Conclusion\n",
      "In this work, we propose FlowDPO, a novel framework for 3D structure prediction that integrates flow-\n",
      "based generative models with Direct Preference Optimization. We achieve 3D structure prediction via\n",
      "flow matching models with various probability paths, and generalize the DPO training objective to\n",
      "arbitrary Gaussian paths. To refine the model via DPO, we generate multiple candidate structures and\n",
      "construct the preference dataset by aligning with ground truth. The results demonstrate substantial\n",
      "9\n",
      "\n",
      "Table 3: Results on crystal structure prediction task. MR stands for Match Rate.\n",
      "Perov-5\n",
      "MP-20\n",
      "MPTS-52\n",
      "MR (%)\n",
      "RMSE\n",
      "MR (%)\n",
      "RMSE\n",
      "MR (%)\n",
      "RMSE\n",
      "P-cG-SchNet [9]\n",
      "48.22\n",
      "0.4179\n",
      "15.39\n",
      "0.3762\n",
      "3.67\n",
      "0.4115\n",
      "CDVAE [33]\n",
      "45.31\n",
      "0.1138\n",
      "33.90\n",
      "0.1045\n",
      "5.34\n",
      "0.2106\n",
      "VP + VE Path [14]\n",
      "52.02\n",
      "0.0760\n",
      "51.49\n",
      "0.0631\n",
      "12.19\n",
      "0.1786\n",
      "OT + OT Path\n",
      "53.95\n",
      "0.1508\n",
      "57.40\n",
      "0.1185\n",
      "17.40\n",
      "0.2405\n",
      "OT + VE Path\n",
      "52.29\n",
      "0.0782\n",
      "58.94\n",
      "0.0621\n",
      "18.91\n",
      "0.1435\n",
      "VP + VE + DPO\n",
      "53.47\n",
      "0.0762\n",
      "59.98\n",
      "0.0622\n",
      "14.75\n",
      "0.1780\n",
      "OT + OT + DPO\n",
      "55.56\n",
      "0.1376\n",
      "59.62\n",
      "0.0898\n",
      "22.36\n",
      "0.1678\n",
      "OT + VE + DPO\n",
      "53.94\n",
      "0.0765\n",
      "62.47\n",
      "0.0606\n",
      "20.27\n",
      "0.1419\n",
      "Figure 5: Visualizations on crystal structure prediction results. The left column depicts the RMSD\n",
      "distribution of the models before (blue) and after (red) DPO. The middle column shows the ground\n",
      "truth structures, and the right column shows typical high RMSD generations to be suppressed.\n",
      "improvements in prediction accuracy for both antibody and crystal structures, highlighting the\n",
      "effectiveness and versatility of FlowDPO in the field of 3D structure prediction.\n",
      "Acknowledgments\n",
      "This work is jointly supported by the National Science and Technology Major Project under Grant\n",
      "2020AAA0107300, the National Natural Science Foundation of China (No. 61925601, No. 62376276,\n",
      "No. 62236011), and Beijing Nova Program (20230484278).\n",
      "References\n",
      "[1] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore,\n",
      "A. J. Ballard, J. Bambrick, et al. Accurate structure prediction of biomolecular interactions with\n",
      "alphafold 3. Nature, pages 1–3, 2024.\n",
      "[2] I. Batatia, D. P. Kovacs, G. Simm, C. Ortner, and G. Csányi. Mace: Higher order equivariant\n",
      "message passing neural networks for fast and accurate force fields.\n",
      "Advances in Neural\n",
      "10\n",
      "\n",
      "Information Processing Systems, 35:11423–11436, 2022.\n",
      "[3] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E.\n",
      "Smidt, and B. Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate\n",
      "interatomic potentials. Nature communications, 13(1):2453, 2022.\n",
      "[4] I. E. Castelli, D. D. Landis, K. S. Thygesen, S. Dahl, I. Chorkendorff, T. F. Jaramillo, and\n",
      "K. W. Jacobsen. New cubic perovskites for one-and two-photon water splitting using the\n",
      "computational materials repository. Energy & Environmental Science, 5(10):9034–9043, 2012.\n",
      "[5] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential\n",
      "equations. Advances in neural information processing systems, 31, 2018.\n",
      "[6] G. Corso, B. Jing, R. Barzilay, T. Jaakkola, et al. Diffdock: Diffusion steps, twists, and turns\n",
      "for molecular docking. In International Conference on Learning Representations (ICLR 2023),\n",
      "2023.\n",
      "[7] G. R. Desiraju. Cryptic crystallography. Nature materials, 1(2):77–79, 2002.\n",
      "[8] J. Dunbar, K. Krawczyk, J. Leem, T. Baker, A. Fuchs, G. Georges, J. Shi, and C. M. Deane.\n",
      "Sabdab: the structural antibody database. Nucleic acids research, 42(D1):D1140–D1146, 2014.\n",
      "[9] N. W. Gebauer, M. Gastegger, S. S. Hessmann, K.-R. Müller, and K. T. Schütt. Inverse design\n",
      "of 3d molecular structures with conditional generative neural networks. Nature communications,\n",
      "13(1):1–11, 2022.\n",
      "[10] J. Hafner. Ab-initio simulations of materials using vasp: Density-functional theory and beyond.\n",
      "Journal of computational chemistry, 29(13):2044–2078, 2008.\n",
      "[11] T. A. Halgren, R. B. Murphy, R. A. Friesner, H. S. Beard, L. L. Frye, W. T. Pollard, and J. L.\n",
      "Banks. Glide: a new approach for rapid, accurate docking and scoring. 2. enrichment factors in\n",
      "database screening. Journal of medicinal chemistry, 47(7):1750–1759, 2004.\n",
      "[12] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural\n",
      "information processing systems, 33:6840–6851, 2020.\n",
      "[13] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter,\n",
      "D. Skinner, G. Ceder, et al. Commentary: The materials project: A materials genome approach\n",
      "to accelerating materials innovation. APL materials, 1(1):011002, 2013.\n",
      "[14] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and Y. Liu. Crystal structure prediction by\n",
      "joint equivariant diffusion. Advances in Neural Information Processing Systems, 36, 2024.\n",
      "[15] B. Jing, G. Corso, J. Chang, R. Barzilay, and T. Jaakkola. Torsional diffusion for molecular\n",
      "conformer generation. Advances in Neural Information Processing Systems, 35:24240–24253,\n",
      "2022.\n",
      "[16] X. Kong, W. Huang, and Y. Liu. Conditional antibody design as 3d equivariant graph translation.\n",
      "In The Eleventh International Conference on Learning Representations, 2022.\n",
      "[17] J. Kreitz, M. J. Friedrich, A. Guru, B. Lash, M. Saito, R. K. Macrae, and F. Zhang. Programmable\n",
      "protein delivery with a bacterial contractile injection system. Nature, 616(7956):357–364, 2023.\n",
      "[18] Y.-L. Liao and T. Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic\n",
      "graphs. In The Eleventh International Conference on Learning Representations, 2022.\n",
      "[19] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative\n",
      "modeling. In The Eleventh International Conference on Learning Representations, 2022.\n",
      "[20] S. Luo, Y. Su, X. Peng, S. Wang, J. Peng, and J. Ma. Antigen-specific antibody design and\n",
      "optimization with diffusion-based generative models for protein structures. Advances in Neural\n",
      "Information Processing Systems, 35:9754–9767, 2022.\n",
      "11\n",
      "\n",
      "[21] G. M. Morris, R. Huey, W. Lindstrom, M. F. Sanner, R. K. Belew, D. S. Goodsell, and A. J.\n",
      "Olson. Autodock4 and autodocktools4: Automated docking with selective receptor flexibility.\n",
      "Journal of computational chemistry, 30(16):2785–2791, 2009.\n",
      "[22] S. P. Ong, W. D. Richards, A. Jain, G. Hautier, M. Kocher, S. Cholia, D. Gunter, V. L. Chevrier,\n",
      "K. A. Persson, and G. Ceder. Python materials genomics (pymatgen): A robust, open-source\n",
      "python library for materials analysis. Computational Materials Science, 68:314–319, 2013.\n",
      "[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\n",
      "K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\n",
      "Advances in neural information processing systems, 35:27730–27744, 2022.\n",
      "[24] C. J. Pickard and R. Needs. Ab initio random structure searching. Journal of Physics: Condensed\n",
      "Matter, 23(5):053201, 2011.\n",
      "[25] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference\n",
      "optimization: Your language model is secretly a reward model. Advances in Neural Information\n",
      "Processing Systems, 36, 2024.\n",
      "[26] M. Schreiner, O. Winther, and S. Olsson. Implicit transfer operator learning: Multiple time-\n",
      "resolution surrogates for molecular dynamics. arXiv preprint arXiv:2305.18046, 2023.\n",
      "[27] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International\n",
      "Conference on Learning Representations, 2020.\n",
      "[28] Y. Song, J. Gong, M. Xu, Z. Cao, Y. Lan, S. Ermon, H. Zhou, and W.-Y. Ma. Equivariant flow\n",
      "matching with hybrid probability transport for 3d molecule generation. Advances in Neural\n",
      "Information Processing Systems, 36, 2024.\n",
      "[29] M. Steinegger and J. Söding. Mmseqs2 enables sensitive protein sequence searching for the\n",
      "analysis of massive data sets. Nature biotechnology, 35(11):1026–1028, 2017.\n",
      "[30] B. Wallace, M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong,\n",
      "S. Joty, and N. Naik. Diffusion model alignment using direct preference optimization. arXiv\n",
      "preprint arXiv:2311.12908, 2023.\n",
      "[31] Y. Wang, J. Lv, L. Zhu, and Y. Ma. Crystal structure prediction via particle-swarm optimization.\n",
      "Physical Review B, 82(9):094116, 2010.\n",
      "[32] B. D. Weitzner, J. R. Jeliazkov, S. Lyskov, N. Marze, D. Kuroda, R. Frick, J. Adolf-Bryfogle,\n",
      "N. Biswas, R. L. Dunbrack, and J. J. Gray. Modeling and docking of antibody structures with\n",
      "rosetta. Nature protocols, 12(2):401–416, 2017.\n",
      "[33] T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola. Crystal diffusion variational\n",
      "autoencoder for periodic material generation. In International Conference on Learning Repre-\n",
      "sentations, 2021.\n",
      "[34] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang. Geodiff: A geometric diffusion model for\n",
      "molecular conformation generation. In International Conference on Learning Representations,\n",
      "2021.\n",
      "[35] C. Zeni, R. Pinsler, D. Zügner, A. Fowler, M. Horton, X. Fu, S. Shysheya, J. Crabbé, L. Sun,\n",
      "J. Smith, et al. Mattergen: a generative model for inorganic materials design. arXiv preprint\n",
      "arXiv:2312.03687, 2023.\n",
      "[36] X. Zhou, D. Xue, R. Chen, Z. Zheng, L. Wang, and Q. Gu. Antigen-specific antibody design\n",
      "via direct energy-based preference optimization. arXiv preprint arXiv:2403.16576, 2024.\n",
      "12\n",
      "\n",
      "A\n",
      "From RLHF to DPO\n",
      "Given the preference pair (xw, xl) with condition c, the Bradley-Terry (BT) model considers a latent\n",
      "reward model r(x|c) behind them and formulates the preference as\n",
      "p(xw ≻xl|c) =\n",
      "exp(r(xw|c))\n",
      "exp(r(xw|c)) + exp(r(xl|c)).\n",
      "(26)\n",
      "RLHF [23] optimizes the generative model by explicitly training a reward model rϕ, and maximizing\n",
      "the reward with a KL regularization term to control the model by the original reference pref as\n",
      "max\n",
      "popt Ex∼popt(x)[rϕ(x)] −βDKL[popt(x)∥pref(x)].\n",
      "(27)\n",
      "We omit the condition c for simplicity.\n",
      "As Eq. (27) exists a close-form solution p∗\n",
      "θ(x) =\n",
      "pref(x)er∗(x)/β/Z, where Z is the normalization term, we can rewrite the optimal reward model as\n",
      "r∗(x) = β log popt(x)\n",
      "pref(x) + βZ.\n",
      "(28)\n",
      "After introducing Eq. (28) into Eq. (26) and directly maximizing the log likelihood, DPO [25]\n",
      "simplifies the training objective as\n",
      "LDPO = −Exw,xl\n",
      "h\n",
      "log σ\n",
      "\u0000β log popt(xw)\n",
      "pref(xw) −β log popt(xl)\n",
      "pref(xl)\n",
      "\u0001i\n",
      ".\n",
      "(29)\n",
      "B\n",
      "Required Symmetries of Atomic Systems\n",
      "The design of flow paths is constrained by the symmetry requirements of specific atomic systems,\n",
      "which are detailed as follows.\n",
      "Antibody Structure Prediction\n",
      "The designed vector field should maintain equivariance to any\n",
      "rotation Q ∈SO(3) and be invariant to any translation ⃗t ∈R3:\n",
      "⃗ut(Q ⃗Xt + ⃗t|Q ⃗X0 + ⃗t, A, Q ⃗XC + ⃗t, AC) = Q⃗ut( ⃗Xt| ⃗X0, A, ⃗XC, AC).\n",
      "(30)\n",
      "Crystal Structure Prediction\n",
      "Previous works [14, 35] consider the task defined in Eq. (11) as a\n",
      "joint generation task on L and F . For the generative process, the vector field of the lattice should be\n",
      "equivariant to an arbitrary rotation Q ∈SO(3), and that of the coordinates is required to ensure the\n",
      "periodic translation invariance for any translation vector t. Specfically, we have\n",
      "uL,t(QLt|QL0, F0, A) = Qu(Lt|L0, F0, A),\n",
      "(31)\n",
      "uF ,t(w(Ft + t)|L0, w(F0 + t), A) = u(Lt|L0, F0, A),\n",
      "(32)\n",
      "where the operation w(F ) = F −⌊F ⌋∈[0, 1)3×N returns the coordinates back to the unit cell.\n",
      "After maintaining the required symmetries, the proposed flow model is capable of generating equiva-\n",
      "lent structures under different E(3) transformations. An example of the OT-OT path for the crystal is\n",
      "shown in Figure 6.\n",
      "C\n",
      "Implementation Details\n",
      "C.1\n",
      "Antibody Structure Prediction\n",
      "We use the framework of DiffAb [20] to train the flow models. The original denoising network in\n",
      "DiffAb requires orientation matrices as input, yet the OT path of the SO(3) matrices is not naive\n",
      "to derive, which is out of our main scope. Therefore, we replace the denoising network with the\n",
      "multi-channel EGNN proposed in MEAN [16] to avoid this problem. All experiments can be run on\n",
      "one GeForce RTX 3090 GPU. Detailed hyperparameters for our FlowDPO are presented in Table 4.\n",
      "13\n",
      "\n",
      "Ground Truth\n",
      "Generated Samples\n",
      "Figure 6: Visualizations of multiple generated crystals via OT-OT path on MP-20. As the designed\n",
      "path maintain the symmetries, the model is able to generate structures equivalent to the ground truth\n",
      "after proper rotations and (periodic) translations.\n",
      "Table 4: Hyperparameters for the antibody structure prediction task.\n",
      "CDR\n",
      "Flow\n",
      "Preference Dataset\n",
      "DPO\n",
      "d\n",
      "L\n",
      "Lr\n",
      "Epoch\n",
      "M\n",
      "K2\n",
      "δ\n",
      "r\n",
      "Lr\n",
      "Epoch\n",
      "B\n",
      "VP\n",
      "L1\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "50\n",
      "200\n",
      "L2\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "400\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "1e-6\n",
      "20\n",
      "200\n",
      "L3\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "5\n",
      "200\n",
      "H1\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "3e-6\n",
      "10\n",
      "200\n",
      "H2\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "10\n",
      "200\n",
      "H3\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "5\n",
      "200\n",
      "OT\n",
      "L1\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "50\n",
      "200\n",
      "L2\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.05\n",
      "5e-6\n",
      "20\n",
      "200\n",
      "L3\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "900\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "5\n",
      "200\n",
      "H1\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "900\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.0\n",
      "5e-6\n",
      "25\n",
      "200\n",
      "H2\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "700\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "50\n",
      "200\n",
      "H3\n",
      "128\n",
      "6\n",
      "1e-4\n",
      "500\n",
      "5\n",
      "1\n",
      "1.0\n",
      "0.1\n",
      "5e-6\n",
      "5\n",
      "200\n",
      "C.2\n",
      "Crystal Structure Prediction\n",
      "We use the denoising network designed in DiffCSP [14] as the backbone model to train the flow\n",
      "models. To predict the vector field via the denoising output, for the OT path designed on lattice, we\n",
      "apply the reparameterization as\n",
      "vL,θ(Lt, Ft, A) =\n",
      "\n",
      "\n",
      "\n",
      "0, t = 1,\n",
      "ϵL,θ(Lt, Ft, A) −Lt\n",
      "1 −t\n",
      ", 0 ≤t < 1.\n",
      "(33)\n",
      "And for the OT path on the fractional coordinates, we directly use vF ,θ(Lt, Ft, A)\n",
      "=\n",
      "ϵF ,θ(Lt, Ft, A). We select RMSD defined by StructureMatcher class in pymatgen [22] with thresh-\n",
      "olds stol=0.5, angle_tol=10, ltol=0.3 as the distance metric to construct the preference dataset.\n",
      "Specially, the RMSD of the unmatched structure is set as +∞, and such candidates will never be\n",
      "selected as the preferred sample.\n",
      "The detailed hyperparameters for the FlowDPO pipeline on each crystal dataset are provided in\n",
      "Table 5. Each experiment is run on one GeForce RTX 3090 GPU.\n",
      "D\n",
      "Comparison with Regressive Methods\n",
      "To further investigate the advantages of the generative paradigm, we employ the same backbone\n",
      "model (MEAN) for a direct regression task as an additional baseline. The results are presented in\n",
      "Table 6. Our findings indicate that generative models outperform the regression model in 4 of the 6\n",
      "CDRs, particularly in the highly variable and functionally critical regions, CDR-H3 and CDR-L3.\n",
      "2Each time the pair is randomly sampled from the 5 candidates plus the ground truth.\n",
      "14\n",
      "\n",
      "Table 5: Hyperparameters for the crystal structure prediction task.\n",
      "Flow\n",
      "Preference Dataset\n",
      "DPO\n",
      "d\n",
      "L\n",
      "Lr\n",
      "Epoch\n",
      "M\n",
      "K\n",
      "δ\n",
      "r\n",
      "Lr\n",
      "Epoch\n",
      "B\n",
      "VP+VE\n",
      "Perov-5\n",
      "256\n",
      "4\n",
      "1e-3\n",
      "3000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-3\n",
      "100\n",
      "2000\n",
      "MP-20\n",
      "512\n",
      "6\n",
      "1e-3\n",
      "1000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-3\n",
      "300\n",
      "2000\n",
      "MPTS-52\n",
      "512\n",
      "6\n",
      "1e-3\n",
      "1000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-3\n",
      "200\n",
      "2000\n",
      "OT+OT\n",
      "Perov-5\n",
      "256\n",
      "4\n",
      "1e-3\n",
      "3000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-4\n",
      "70\n",
      "2000\n",
      "MP-20\n",
      "512\n",
      "6\n",
      "1e-3\n",
      "3000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-4\n",
      "30\n",
      "2000\n",
      "MPTS-52\n",
      "512\n",
      "6\n",
      "1e-3\n",
      "2000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-4\n",
      "100\n",
      "2000\n",
      "OT+VE\n",
      "Perov-5\n",
      "256\n",
      "4\n",
      "1e-3\n",
      "3000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "5e-5\n",
      "5\n",
      "2000\n",
      "MP-20\n",
      "512\n",
      "6\n",
      "1e-3\n",
      "3000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-3\n",
      "300\n",
      "2000\n",
      "MPTS-52\n",
      "512\n",
      "6\n",
      "1e-3\n",
      "2000\n",
      "5\n",
      "12\n",
      "0.3\n",
      "1/6\n",
      "1e-3\n",
      "200\n",
      "2000\n",
      "Additionally, we report both the mean and minimum RMSD values across 20 generations for each\n",
      "generative model. The significantly lower minimum RMSD values demonstrate that generative\n",
      "models not only yield predictions closer to the observed reference structures but also possess the\n",
      "capability to generate multiple feasible conformations around the stable state.\n",
      "Table 6: Results compare to regressive baselines on antibody structure prediction tasks.\n",
      "CDR\n",
      "Regression\n",
      "FlowDPO (VP, mean)\n",
      "FlowDPO (OT, mean)\n",
      "FlowDPO (VP, min)\n",
      "FlowDPO (OT, min)\n",
      "RMSDCA\n",
      "RMSDbb\n",
      "RMSDCA\n",
      "RMSDbb\n",
      "RMSDCA\n",
      "RMSDbb\n",
      "RMSDCA\n",
      "RMSDbb\n",
      "RMSDCA\n",
      "RMSDbb\n",
      "L1\n",
      "1.03\n",
      "1.05\n",
      "1.91\n",
      "1.95\n",
      "1.74\n",
      "1.78\n",
      "1.44\n",
      "1.60\n",
      "1.22\n",
      "1.33\n",
      "L2\n",
      "0.96\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.80\n",
      "0.85\n",
      "0.83\n",
      "0.87\n",
      "L3\n",
      "1.17\n",
      "1.18\n",
      "0.94\n",
      "1.01\n",
      "0.95\n",
      "1.05\n",
      "0.69\n",
      "0.82\n",
      "0.69\n",
      "0.84\n",
      "H1\n",
      "1.68\n",
      "1.67\n",
      "0.80\n",
      "0.86\n",
      "0.83\n",
      "0.89\n",
      "0.53\n",
      "0.67\n",
      "0.52\n",
      "0.67\n",
      "H2\n",
      "0.72\n",
      "0.78\n",
      "0.87\n",
      "0.95\n",
      "0.95\n",
      "1.02\n",
      "0.49\n",
      "0.65\n",
      "0.57\n",
      "0.71\n",
      "H3\n",
      "3.46\n",
      "3.48\n",
      "3.44\n",
      "3.45\n",
      "3.32\n",
      "3.32\n",
      "2.60\n",
      "2.64\n",
      "2.55\n",
      "2.61\n",
      "E\n",
      "Limitations\n",
      "As detailed in § 3.3, the derivation of the rationality of DPO for flow models primarily focuses\n",
      "on Gaussian paths. However, flow models have the potential to learn mappings from an arbitrary\n",
      "prior to the data distribution if the probability path is appropriately defined. Therefore, a more\n",
      "general derivation that does not rely on Gaussian assumptions could be explored in future research.\n",
      "Additionally, our current evaluation is based predominantly on computational metrics. Conducting\n",
      "wet-lab experiments would provide a more robust validation of the model’s effectiveness in practical\n",
      "applications.\n",
      "F\n",
      "Broader Impacts\n",
      "The introduction of FlowDPO marks a pivotal advancement in scientific domains such as drug devel-\n",
      "opment, materials research, and molecular informatics. Recent developments, such as AlphaFold3,\n",
      "have demonstrated remarkable accuracy in predicting structures across various domains [1]. However,\n",
      "issues such as hallucinations, like erroneous structural order in inherently disordered regions, remain\n",
      "a challenge. It is intriguing to explore whether alignment strategies based on DPO can mitigate these\n",
      "hallucinations and enhance overall prediction accuracy.\n",
      "G\n",
      "Codes\n",
      "Our codes are available at https://github.com/jiaor17/FlowDPO.\n",
      "15\n",
      "\n",
      "NeurIPS Paper Checklist\n",
      "1. Claims\n",
      "Question: Do the main claims made in the abstract and introduction accurately reflect the\n",
      "paper’s contributions and scope?\n",
      "Answer: [Yes]\n",
      "Justification: In abstract and introduction, we summarize our contribution as enabling DPO\n",
      "for flow-based structure prediction models, constructing preference dataset to align model\n",
      "predictions with reference, and achieving promising results for antibody and crystal predic-\n",
      "tion tasks. These claims are detailed and verified by the method (§3) and experiment (§4)\n",
      "sections.\n",
      "Guidelines:\n",
      "• The answer NA means that the abstract and introduction do not include the claims\n",
      "made in the paper.\n",
      "• The abstract and/or introduction should clearly state the claims made, including the\n",
      "contributions made in the paper and important assumptions and limitations. A No or\n",
      "NA answer to this question will not be perceived well by the reviewers.\n",
      "• The claims made should match theoretical and experimental results, and reflect how\n",
      "much the results can be expected to generalize to other settings.\n",
      "• It is fine to include aspirational goals as motivation as long as it is clear that these goals\n",
      "are not attained by the paper.\n",
      "2. Limitations\n",
      "Question: Does the paper discuss the limitations of the work performed by the authors?\n",
      "Answer: [Yes]\n",
      "Justification: The limitations of this paper are discussed in Appendix E.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper has no limitation while the answer No means that\n",
      "the paper has limitations, but those are not discussed in the paper.\n",
      "• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n",
      "• The paper should point out any strong assumptions and how robust the results are to\n",
      "violations of these assumptions (e.g., independence assumptions, noiseless settings,\n",
      "model well-specification, asymptotic approximations only holding locally). The authors\n",
      "should reflect on how these assumptions might be violated in practice and what the\n",
      "implications would be.\n",
      "• The authors should reflect on the scope of the claims made, e.g., if the approach was\n",
      "only tested on a few datasets or with a few runs. In general, empirical results often\n",
      "depend on implicit assumptions, which should be articulated.\n",
      "• The authors should reflect on the factors that influence the performance of the approach.\n",
      "For example, a facial recognition algorithm may perform poorly when image resolution\n",
      "is low or images are taken in low lighting. Or a speech-to-text system might not be\n",
      "used reliably to provide closed captions for online lectures because it fails to handle\n",
      "technical jargon.\n",
      "• The authors should discuss the computational efficiency of the proposed algorithms\n",
      "and how they scale with dataset size.\n",
      "• If applicable, the authors should discuss possible limitations of their approach to\n",
      "address problems of privacy and fairness.\n",
      "• While the authors might fear that complete honesty about limitations might be used by\n",
      "reviewers as grounds for rejection, a worse outcome might be that reviewers discover\n",
      "limitations that aren’t acknowledged in the paper. The authors should use their best\n",
      "judgment and recognize that individual actions in favor of transparency play an impor-\n",
      "tant role in developing norms that preserve the integrity of the community. Reviewers\n",
      "will be specifically instructed to not penalize honesty concerning limitations.\n",
      "3. Theory Assumptions and Proofs\n",
      "16\n",
      "\n",
      "Question: For each theoretical result, does the paper provide the full set of assumptions and\n",
      "a complete (and correct) proof?\n",
      "Answer: [Yes]\n",
      "Justification: The derivation are provided in §3.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not include theoretical results.\n",
      "• All the theorems, formulas, and proofs in the paper should be numbered and cross-\n",
      "referenced.\n",
      "• All assumptions should be clearly stated or referenced in the statement of any theorems.\n",
      "• The proofs can either appear in the main paper or the supplemental material, but if\n",
      "they appear in the supplemental material, the authors are encouraged to provide a short\n",
      "proof sketch to provide intuition.\n",
      "• Inversely, any informal proof provided in the core of the paper should be complemented\n",
      "by formal proofs provided in appendix or supplemental material.\n",
      "• Theorems and Lemmas that the proof relies upon should be properly referenced.\n",
      "4. Experimental Result Reproducibility\n",
      "Question: Does the paper fully disclose all the information needed to reproduce the main ex-\n",
      "perimental results of the paper to the extent that it affects the main claims and/or conclusions\n",
      "of the paper (regardless of whether the code and data are provided or not)?\n",
      "Answer: [Yes]\n",
      "Justification: The used datasets and evaluation setups are provided in §4, and we provide\n",
      "more details in Appendix C.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not include experiments.\n",
      "• If the paper includes experiments, a No answer to this question will not be perceived\n",
      "well by the reviewers: Making the paper reproducible is important, regardless of\n",
      "whether the code and data are provided or not.\n",
      "• If the contribution is a dataset and/or model, the authors should describe the steps taken\n",
      "to make their results reproducible or verifiable.\n",
      "• Depending on the contribution, reproducibility can be accomplished in various ways.\n",
      "For example, if the contribution is a novel architecture, describing the architecture fully\n",
      "might suffice, or if the contribution is a specific model and empirical evaluation, it may\n",
      "be necessary to either make it possible for others to replicate the model with the same\n",
      "dataset, or provide access to the model. In general. releasing code and data is often\n",
      "one good way to accomplish this, but reproducibility can also be provided via detailed\n",
      "instructions for how to replicate the results, access to a hosted model (e.g., in the case\n",
      "of a large language model), releasing of a model checkpoint, or other means that are\n",
      "appropriate to the research performed.\n",
      "• While NeurIPS does not require releasing code, the conference does require all submis-\n",
      "sions to provide some reasonable avenue for reproducibility, which may depend on the\n",
      "nature of the contribution. For example\n",
      "(a) If the contribution is primarily a new algorithm, the paper should make it clear how\n",
      "to reproduce that algorithm.\n",
      "(b) If the contribution is primarily a new model architecture, the paper should describe\n",
      "the architecture clearly and fully.\n",
      "(c) If the contribution is a new model (e.g., a large language model), then there should\n",
      "either be a way to access this model for reproducing the results or a way to reproduce\n",
      "the model (e.g., with an open-source dataset or instructions for how to construct\n",
      "the dataset).\n",
      "(d) We recognize that reproducibility may be tricky in some cases, in which case\n",
      "authors are welcome to describe the particular way they provide for reproducibility.\n",
      "In the case of closed-source models, it may be that access to the model is limited in\n",
      "some way (e.g., to registered users), but it should be possible for other researchers\n",
      "to have some path to reproducing or verifying the results.\n",
      "17\n",
      "\n",
      "5. Open access to data and code\n",
      "Question: Does the paper provide open access to the data and code, with sufficient instruc-\n",
      "tions to faithfully reproduce the main experimental results, as described in supplemental\n",
      "material?\n",
      "Answer: [Yes]\n",
      "Justification: Our codes are provided in Appendix G.\n",
      "Guidelines:\n",
      "• The answer NA means that paper does not include experiments requiring code.\n",
      "• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\n",
      "public/guides/CodeSubmissionPolicy) for more details.\n",
      "• While we encourage the release of code and data, we understand that this might not be\n",
      "possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\n",
      "including code, unless this is central to the contribution (e.g., for a new open-source\n",
      "benchmark).\n",
      "• The instructions should contain the exact command and environment needed to run to\n",
      "reproduce the results. See the NeurIPS code and data submission guidelines (https:\n",
      "//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n",
      "• The authors should provide instructions on data access and preparation, including how\n",
      "to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n",
      "• The authors should provide scripts to reproduce all experimental results for the new\n",
      "proposed method and baselines. If only a subset of experiments are reproducible, they\n",
      "should state which ones are omitted from the script and why.\n",
      "• At submission time, to preserve anonymity, the authors should release anonymized\n",
      "versions (if applicable).\n",
      "• Providing as much information as possible in supplemental material (appended to the\n",
      "paper) is recommended, but including URLs to data and code is permitted.\n",
      "6. Experimental Setting/Details\n",
      "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n",
      "parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n",
      "results?\n",
      "Answer: [Yes]\n",
      "Justification: The hyperparameters are provided in Appendix C.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not include experiments.\n",
      "• The experimental setting should be presented in the core of the paper to a level of detail\n",
      "that is necessary to appreciate the results and make sense of them.\n",
      "• The full details can be provided either with the code, in appendix, or as supplemental\n",
      "material.\n",
      "7. Experiment Statistical Significance\n",
      "Question: Does the paper report error bars suitably and correctly defined or other appropriate\n",
      "information about the statistical significance of the experiments?\n",
      "Answer: [No]\n",
      "Justification: The dataset used in the experiments are large and the results are relatively\n",
      "stable. Rerunning the pipeline for multiple times is costly. Instead, we further compare the\n",
      "prediction results for different models from the perspective of distribution in Figure 4.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not include experiments.\n",
      "• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n",
      "dence intervals, or statistical significance tests, at least for the experiments that support\n",
      "the main claims of the paper.\n",
      "18\n",
      "\n",
      "• The factors of variability that the error bars are capturing should be clearly stated (for\n",
      "example, train/test split, initialization, random drawing of some parameter, or overall\n",
      "run with given experimental conditions).\n",
      "• The method for calculating the error bars should be explained (closed form formula,\n",
      "call to a library function, bootstrap, etc.)\n",
      "• The assumptions made should be given (e.g., Normally distributed errors).\n",
      "• It should be clear whether the error bar is the standard deviation or the standard error\n",
      "of the mean.\n",
      "• It is OK to report 1-sigma error bars, but one should state it. The authors should\n",
      "preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n",
      "of Normality of errors is not verified.\n",
      "• For asymmetric distributions, the authors should be careful not to show in tables or\n",
      "figures symmetric error bars that would yield results that are out of range (e.g. negative\n",
      "error rates).\n",
      "• If error bars are reported in tables or plots, The authors should explain in the text how\n",
      "they were calculated and reference the corresponding figures or tables in the text.\n",
      "8. Experiments Compute Resources\n",
      "Question: For each experiment, does the paper provide sufficient information on the com-\n",
      "puter resources (type of compute workers, memory, time of execution) needed to reproduce\n",
      "the experiments?\n",
      "Answer: [Yes]\n",
      "Justification: The compute resources are provided in Appendix C.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not include experiments.\n",
      "• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\n",
      "or cloud provider, including relevant memory and storage.\n",
      "• The paper should provide the amount of compute required for each of the individual\n",
      "experimental runs as well as estimate the total compute.\n",
      "• The paper should disclose whether the full research project required more compute\n",
      "than the experiments reported in the paper (e.g., preliminary or failed experiments that\n",
      "didn’t make it into the paper).\n",
      "9. Code Of Ethics\n",
      "Question: Does the research conducted in the paper conform, in every respect, with the\n",
      "NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\n",
      "Answer: [Yes]\n",
      "Justification: This paper definitely follows the NeurIPS Code of Ethics.\n",
      "Guidelines:\n",
      "• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n",
      "• If the authors answer No, they should explain the special circumstances that require a\n",
      "deviation from the Code of Ethics.\n",
      "• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\n",
      "eration due to laws or regulations in their jurisdiction).\n",
      "10. Broader Impacts\n",
      "Question: Does the paper discuss both potential positive societal impacts and negative\n",
      "societal impacts of the work performed?\n",
      "Answer: [Yes]\n",
      "Justification: The broader impacts of this paper are discussed in Appendix ??.\n",
      "Guidelines:\n",
      "• The answer NA means that there is no societal impact of the work performed.\n",
      "• If the authors answer NA or No, they should explain why their work has no societal\n",
      "impact or why the paper does not address societal impact.\n",
      "19\n",
      "\n",
      "• Examples of negative societal impacts include potential malicious or unintended uses\n",
      "(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n",
      "(e.g., deployment of technologies that could make decisions that unfairly impact specific\n",
      "groups), privacy considerations, and security considerations.\n",
      "• The conference expects that many papers will be foundational research and not tied\n",
      "to particular applications, let alone deployments. However, if there is a direct path to\n",
      "any negative applications, the authors should point it out. For example, it is legitimate\n",
      "to point out that an improvement in the quality of generative models could be used to\n",
      "generate deepfakes for disinformation. On the other hand, it is not needed to point out\n",
      "that a generic algorithm for optimizing neural networks could enable people to train\n",
      "models that generate Deepfakes faster.\n",
      "• The authors should consider possible harms that could arise when the technology is\n",
      "being used as intended and functioning correctly, harms that could arise when the\n",
      "technology is being used as intended but gives incorrect results, and harms following\n",
      "from (intentional or unintentional) misuse of the technology.\n",
      "• If there are negative societal impacts, the authors could also discuss possible mitigation\n",
      "strategies (e.g., gated release of models, providing defenses in addition to attacks,\n",
      "mechanisms for monitoring misuse, mechanisms to monitor how a system learns from\n",
      "feedback over time, improving the efficiency and accessibility of ML).\n",
      "11. Safeguards\n",
      "Question: Does the paper describe safeguards that have been put in place for responsible\n",
      "release of data or models that have a high risk for misuse (e.g., pretrained language models,\n",
      "image generators, or scraped datasets)?\n",
      "Answer: [NA]\n",
      "Justification: The paper poses no such risks.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper poses no such risks.\n",
      "• Released models that have a high risk for misuse or dual-use should be released with\n",
      "necessary safeguards to allow for controlled use of the model, for example by requiring\n",
      "that users adhere to usage guidelines or restrictions to access the model or implementing\n",
      "safety filters.\n",
      "• Datasets that have been scraped from the Internet could pose safety risks. The authors\n",
      "should describe how they avoided releasing unsafe images.\n",
      "• We recognize that providing effective safeguards is challenging, and many papers do\n",
      "not require this, but we encourage authors to take this into account and make a best\n",
      "faith effort.\n",
      "12. Licenses for existing assets\n",
      "Question: Are the creators or original owners of assets (e.g., code, data, models), used in\n",
      "the paper, properly credited and are the license and terms of use explicitly mentioned and\n",
      "properly respected?\n",
      "Answer: [Yes]\n",
      "Justification: All datasets used in this paper have been properly cited.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not use existing assets.\n",
      "• The authors should cite the original paper that produced the code package or dataset.\n",
      "• The authors should state which version of the asset is used and, if possible, include a\n",
      "URL.\n",
      "• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n",
      "• For scraped data from a particular source (e.g., website), the copyright and terms of\n",
      "service of that source should be provided.\n",
      "• If assets are released, the license, copyright information, and terms of use in the\n",
      "package should be provided. For popular datasets, paperswithcode.com/datasets\n",
      "has curated licenses for some datasets. Their licensing guide can help determine the\n",
      "license of a dataset.\n",
      "20\n",
      "\n",
      "• For existing datasets that are re-packaged, both the original license and the license of\n",
      "the derived asset (if it has changed) should be provided.\n",
      "• If this information is not available online, the authors are encouraged to reach out to\n",
      "the asset’s creators.\n",
      "13. New Assets\n",
      "Question: Are new assets introduced in the paper well documented and is the documentation\n",
      "provided alongside the assets?\n",
      "Answer: [NA]\n",
      "Justification: This paper does not release new assets.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not release new assets.\n",
      "• Researchers should communicate the details of the dataset/code/model as part of their\n",
      "submissions via structured templates. This includes details about training, license,\n",
      "limitations, etc.\n",
      "• The paper should discuss whether and how consent was obtained from people whose\n",
      "asset is used.\n",
      "• At submission time, remember to anonymize your assets (if applicable). You can either\n",
      "create an anonymized URL or include an anonymized zip file.\n",
      "14. Crowdsourcing and Research with Human Subjects\n",
      "Question: For crowdsourcing experiments and research with human subjects, does the paper\n",
      "include the full text of instructions given to participants and screenshots, if applicable, as\n",
      "well as details about compensation (if any)?\n",
      "Answer: [NA]\n",
      "Justification: This paper does not involve crowdsourcing nor research with human subjects.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not involve crowdsourcing nor research with\n",
      "human subjects.\n",
      "• Including this information in the supplemental material is fine, but if the main contribu-\n",
      "tion of the paper involves human subjects, then as much detail as possible should be\n",
      "included in the main paper.\n",
      "• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\n",
      "or other labor should be paid at least the minimum wage in the country of the data\n",
      "collector.\n",
      "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\n",
      "Subjects\n",
      "Question: Does the paper describe potential risks incurred by study participants, whether\n",
      "such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\n",
      "approvals (or an equivalent approval/review based on the requirements of your country or\n",
      "institution) were obtained?\n",
      "Answer: [NA]\n",
      "Justification: This paper does not involve crowdsourcing nor research with human subjects.\n",
      "Guidelines:\n",
      "• The answer NA means that the paper does not involve crowdsourcing nor research with\n",
      "human subjects.\n",
      "• Depending on the country in which research is conducted, IRB approval (or equivalent)\n",
      "may be required for any human subjects research. If you obtained IRB approval, you\n",
      "should clearly state this in the paper.\n",
      "• We recognize that the procedures for this may vary significantly between institutions\n",
      "and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\n",
      "guidelines for their institution.\n",
      "• For initial submissions, do not include any information that would break anonymity (if\n",
      "applicable), such as the institution conducting the review.\n",
      "21\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_result(line_number):\n",
    "    \"\"\"\n",
    "    Opens the fitz_extraction_results.jsonl file and pretty prints the specified line.\n",
    "    \n",
    "    Args:\n",
    "        line_number (int): The line number to print (1-based indexing)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"fitz_extraction_results.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        if line_number < 1 or line_number > len(lines):\n",
    "            print(f\"Error: Line number {line_number} is out of range. File has {len(lines)} lines.\")\n",
    "            return\n",
    "        \n",
    "        # Get the specified line (adjusting for 0-based indexing)\n",
    "        json_line = lines[line_number - 1].strip()\n",
    "        \n",
    "        # Parse the JSON\n",
    "        data = json.loads(json_line)\n",
    "        \n",
    "        print(\"\\n--- Document Metadata ---\")\n",
    "        # Print some basic stats about the extraction\n",
    "        if \"text\" in data and \"text\" in data[\"text\"]:\n",
    "            print(f\"Text length: {len(data['text'])} characters\")\n",
    "            print(f\"s3_path: {data['s3_path']}\")\n",
    "        \n",
    "\n",
    "        print(f\"Line {line_number} of {len(lines)}:\")\n",
    "        print(\"\\n--- Document Text ---\\n\")\n",
    "        \n",
    "        # Print the actual text with proper newlines\n",
    "        if \"text\" in data:\n",
    "            # Print the text directly to preserve formatting\n",
    "            print(data[\"text\"])\n",
    "        \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: fitz_extraction_results.jsonl file not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Line {line_number} contains invalid JSON.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing expected key in JSON structure: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "pretty_print_result(1)  # Print the first result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "paper_id": "tmpstpea0z62303",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-06-26T22:19:30.952494Z"
    },
    "title": "TRAK: Attributing Model Behavior at Scale",
    "authors": [
        {
            "first": "Sung",
            "middle": [
                "Min"
            ],
            "last": "Park",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Kristian",
            "middle": [],
            "last": "Georgiev",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Andrew",
            "middle": [],
            "last": "Ilyas",
            "suffix": "",
            "affiliation": {},
            "email": "ailyas@mit.edu"
        },
        {
            "first": "Guillaume",
            "middle": [],
            "last": "Leclerc",
            "suffix": "",
            "affiliation": {},
            "email": "leclerc@mit.edu"
        },
        {
            "first": "Aleksander",
            "middle": [
                "M"
            ],
            "last": "\u0104dry Mit",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.\nIn this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scales: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). We provide code for using TRAK (and reproducing our work) at https://github.com/MadryLab/trak.",
    "pdf_parse": {
        "paper_id": "tmpstpea0z62303",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scales: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). We provide code for using TRAK (and reproducing our work) at https://github.com/MadryLab/trak.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Training data is a key driver of model behavior in modern machine learning systems. Indeed, model errors, biases, and capabilities can all stem from the training data [IST+19; GDG17; GRM+19]. Furthermore, improving the quality of training data generally improves the performance of the resulting models [HAE16; LIN+22] . The importance of training data to model behavior has motivated extensive work on data attribution, i.e., the task of tracing model predictions back to the training examples that informed these predictions. Recent work demonstrates, in particular, the utility of data attribution methods in applications such as explaining predictions [KL17; IPE+22], debugging model behavior [KSH22; SPI+22] , assigning data valuations [GZ19; JDW+19], detecting poisoned or mislabeled data [LZL+22; HL22a] , and curating data [KKG+19; LDZ+21; JWS+21].",
                "cite_spans": [
                    {
                        "start": 303,
                        "end": 310,
                        "text": "[HAE16;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 311,
                        "end": 318,
                        "text": "LIN+22]",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 697,
                        "end": 704,
                        "text": "[KSH22;",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 705,
                        "end": 712,
                        "text": "SPI+22]",
                        "ref_id": "BIBREF84"
                    },
                    {
                        "start": 795,
                        "end": 803,
                        "text": "[LZL+22;",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 804,
                        "end": 810,
                        "text": "HL22a]",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, a recurring tradeoff in the space of data attribution methods is that of computational demand versus efficacy. On the one hand, methods such as influence approximation [KL17; SZV+22] or gradient agreement scoring [PLS+20] are computationally attractive but can be unreliable in non-convex settings [BPF21; IPE+22; ABL+22]. On the other hand, sampling-based methods such as empirical influence functions [FZ20] , Shapley value estimators [GZ19; JDW+19] or datamodels [IPE+22] are more successful at accurately attributing predictions to training data but require training thousands (or tens of thousands) of models to be effective. We thus ask:",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 230,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 412,
                        "end": 418,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 475,
                        "end": 483,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Are there data attribution methods that are both scalable and effective in large-scale non-convex settings? ResNet-9 on CIFAR-10 TRAK Datamodel [IPE+22] Emp. Influence [FZ20] IF-Arnoldi [SZV+22] IF [KL17] Representation Sim.",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 152,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 168,
                        "end": 174,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 198,
                        "end": 204,
                        "text": "[KL17]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "TracIn [PLS+20] 10 1 10 2 10 3 10 4 10 5 0 0.2 0.4 0.6",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 15,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "Computation time (mins) on 1xA100 (\u2190 more efficient)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "BERT-base on QNLI Figure 1 : Our data attribution method TRAK achieves state-of-the-art tradeoffs between speed and efficacy. Here, we benchmark its performance relative to prior methods on CIFAR-10-trained ResNet-9 models and QNLI-trained BERT-BASE models. The x-axis indicates the time (in minutes) it takes to run each method on a single A100 GPU (see Appendix A.4 for details). The y-axis indicates the method's efficacy as measured by its ability to make accurate counterfactual predictions (see Definition 2.3 for the precise metric); error bars indicate 95% bootstrap confidence intervals.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "To properly answer this question, we first need a unifying metric for evaluating data attribution methods. To this end, we adopt the view that a data attribution method is useful insofar as it can make accurate counterfactual predictions, i.e., answer questions of the form \"what would happen if I trained the model on a given subset S of my training set?\" This perspective motivates a benchmark-inspired by the datamodeling framework [IPE+22] -that measures the correlation between true model outputs and attribution-derived predictions for those outputs.",
                "cite_spans": [
                    {
                        "start": 435,
                        "end": 443,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "With this benchmark in hand, in Section 3 we consider our motivating question and introduce TRAK (Tracing with the Randomly-projected After Kernel), a new data attribution method for parametric, differentiable models. The key idea behind TRAK is to first approximate models with a kernel machine (e.g., through the empirical neural tangent kernel [JGH18] ) and then to leverage our understanding of the resulting kernel domain to derive data attribution scores.",
                "cite_spans": [
                    {
                        "start": 347,
                        "end": 354,
                        "text": "[JGH18]",
                        "ref_id": "BIBREF45"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "We demonstrate that TRAK retains the efficacy of sampling-based attribution methods while being several orders of magnitude cheaper computationally. For example (Figure 1 ), on CIFAR-10 (image classification) and QNLI (natural language inference), TRAK can be as effective as datamodels [IPE+22] while being 100-1000x faster to compute. Furthermore, TRAK is as fast as existing gradient-based methods such as TracIn [PLS+20] or variations of influence functions [KL17; SZV+22], while being significantly more predictive of model behavior.",
                "cite_spans": [
                    {
                        "start": 287,
                        "end": 295,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 416,
                        "end": 424,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 169,
                        "end": 170,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "As a result, TRAK enables us to study the connection between model predictions and training data in large-scale settings. For example, we use TRAK to study predictions of ImageNet classifiers (Section 4); to understand the shared image-text embedding space of CLIP models [RKH+21] trained on MS COCO [LMB+14] (Section 5.1); and to fact-trace language models (a 300M-parameter mT5-small model [RSR+20; XCR+21]) finetuned on FTRACE-TREX (Section 5.2).",
                "cite_spans": [
                    {
                        "start": 272,
                        "end": 280,
                        "text": "[RKH+21]",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 300,
                        "end": 308,
                        "text": "[LMB+14]",
                        "ref_id": "BIBREF58"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GAS [HL22]",
                "sec_num": null
            },
            {
                "text": "We begin with a focus on the supervised learning regime. We will denote by S = {z 1 , . . . , z n } an ordered training set of examples, where each z i = (x i , y i ) \u2208 Z is an input-label pair. We represent machine learning models (implicitly) using a model output function f (z; \u03b8), which maps an example of interest z and model parameters \u03b8 to a real number. There are a variety of model output functions that one can employ-for example, the loss L(z; \u03b8) of the model on the example z is a natural choice. Ultimately, though, the appropriate model output function to use will depend on the setting that we are studying.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": "Throughout this work, we also assume that models are trained to minimize the empirical training loss, i.e., that the parameters of these models are given by \u03b8 (S) := arg min",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 \u2211 z i \u2208S L(z i ; \u03b8),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": "where, again, L(z i ; \u03b8) is the model training loss on example z i . We write \u03b8 as a function of S as we will later consider varying S-but when S is clear from the context, we omit it and just write \u03b8 . In this paper, our overarching goal is to trace model predictions back to the composition of training data. This goal-which we refer to as data attribution-is not new. Prior work has approached it using methods such as influence functions and their many variants [HRR+11; KL17; FZ20; HL22a]; sampling-based estimators such as Shapley values [LL17] , empirical influences [FZ20] , and datamodels [IPE+22] ; as well as various other approaches [YKY+18; PLS+20; HL22b]. Each of these methods implements a similar interface: given a model and an output of interest (e.g., loss for a given prediction), a data attribution method computes a score for each training input indicating its importance to the output of interest. Definition 2.1 below makes this interface precise: Definition 2.1 (Data attribution). Consider an ordered training set of examples S = {z 1 , . . . , z n } and a model output function f (z; \u03b8). A data attribution method \u03c4(z, S) is a function \u03c4 : Z \u00d7 Z n \u2192 R n that, for any example z \u2208 Z and a training set S, assigns a (real-valued) score to each training input z i \u2208 S indicating its importance1 to the model output f (z; \u03b8 (S)). When the second argument S is clear from the context, we will omit the second argument and simply write \u03c4(z).",
                "cite_spans": [
                    {
                        "start": 544,
                        "end": 550,
                        "text": "[LL17]",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 574,
                        "end": 580,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 598,
                        "end": 606,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": "Example 2.2 (Influence functions as an attribution method). An example of a data attribution method is the influence function approach, a concept from robust statistics. For a specific model output function f (z; \u03b8) on an example of interest z, an influence function assigns a score to each training example z i that approximates the effect on the output f (z; \u03b8) of infinitesimally up-weighting that training example. A classic result from [CW82] shows that this score can be computed as",
                "cite_spans": [
                    {
                        "start": 441,
                        "end": 447,
                        "text": "[CW82]",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": "\u03c4 IF (z) i = \u2207 \u03b8 f (z; \u03b8 ) \u2022 H -1 \u03b8 \u2022 \u2207 \u03b8 L(z i ; \u03b8 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": ", where, again, \u03b8 are the parameters that minimize the empirical risk, L(z i ; \u03b8 ) is the training loss of example z i , and H \u03b8 is the Hessian \u2207 2 \u03b8 1 n \u2211 z i \u2208S L(z i ; \u03b8 ) of the total training loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motivation and Setup",
                "sec_num": "2"
            },
            {
                "text": "Given the variety of existing data attribution methods, we need a method to evaluate them in a consistent way. One popular approach is to simply manually inspect the training examples that the method identifies as most important for a given prediction or set of predictions. Such manual inspection can be a useful sanity check, but is also often subjective and unreliable. For example, in computer vision, visual similarity between two images does not fully capture the influence of one on the other in terms of model behavior [IPE+22] .",
                "cite_spans": [
                    {
                        "start": 527,
                        "end": 535,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating attribution methods.",
                "sec_num": null
            },
            {
                "text": "A more objective alternative is to treat the scores from a data attribution method as estimates of some ground-truth parameters-such as leave-one-out influences [KL17; BPF21; KAT+19] or Shapley values [LL17] -and then measure the accuracy of these estimates. This approach to evaluation is not only more quantitative than visual inspection but also inherits all favorable properties of the ground-truth parameter being considered (e.g., additivity of Shapley values [Sha51] ). However, getting access to these ground-truth parameters can be prohibitively expensive in large-scale settings.",
                "cite_spans": [
                    {
                        "start": 201,
                        "end": 207,
                        "text": "[LL17]",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 466,
                        "end": 473,
                        "text": "[Sha51]",
                        "ref_id": "BIBREF82"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating attribution methods.",
                "sec_num": null
            },
            {
                "text": "Finally, yet another possibility is to measure the utility of data attribution scores for an auxiliary task such as identifying mislabeled data [KL17; HL22a] or active learning [JWS+21] . This approach can indeed be a useful proxy for evaluating data attribution methods, but the resulting metrics may be too sensitive to the particulars of the auxiliary task and thus make comparisons across different problems and settings difficult.",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 150,
                        "text": "[KL17;",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 151,
                        "end": 157,
                        "text": "HL22a]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 177,
                        "end": 185,
                        "text": "[JWS+21]",
                        "ref_id": "BIBREF47"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating attribution methods.",
                "sec_num": null
            },
            {
                "text": "Motivated by the above shortcomings of existing methodologies, we propose a new metric for evaluating data attribution methods. At the heart of our metric is the perspective that an effective data attribution method should be able to make accurate counterfactual predictions about model outputs. In other words, if a method can accurately quantify the importance of individual training examples to model outputs, it should also be able to predict how model outputs change when the training set is modified in a particular way.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "Inspired by Ilyas et al. [IPE+22] , we cast this counterfactual estimation task as that of predicting the model output function f (z; \u03b8 (S )) given different subsets of the training set S . More precisely, consider-for a fixed example of interest z \u2208 Z-the model output f (z; \u03b8 (S )) arising from training on a subset S \u2282 S of the training set S (see (1)).2 Since z is fixed and the learning algorithm \u03b8 (\u2022) is fixed, we can view this model output as a function of S alone. A good data attribution method should help us predict the former from the latter.",
                "cite_spans": [
                    {
                        "start": 25,
                        "end": 33,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "To operationalize this idea, we first need a way of converting a given data attribution method \u03c4(\u2022) into a counterfactual predictor. We observe that the vast majority of data attribution methods are additive-that is, they define the importance of a group of training examples to be the sum of the importances of the examples in the group. 3 Motivated by this observation, we define an attribution method's prediction of the model output for a subset S \u2282 S as the sum of the corresponding scores: Definition 2.3 (Attribution-based output predictions). Consider a training set S, a model output function f (z; \u03b8), and a corresponding data attribution method \u03c4 (see Definition 2.1). The attributionbased output prediction of the model output f (z; \u03b8 (S )) is defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "g \u03c4 (z, S ; S) := \u2211 i : z i \u2208S \u03c4(z, S) i = \u03c4(z, S) \u2022 1 S ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "where 1 S is the indicator vector of the subset S of S (i.e., (1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "S ) i = 1{z i \u2208 S }).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "Intuitively, Definition 2.3 turns any data attribution method into a counterfactual predictor. Specifically, for a given counterfactual training set S \u2282 S, the attribution method's prediction is simply the sum of the scores of the examples contained in S . Now that we have defined how to derive predictions from an attribution method, we can evaluate these predictions using the linear datamodeling score, defined as follows: Definition 2.4 (Linear datamodeling score). Consider a training set S, a model output function f (z; \u03b8), and a corresponding data attribution method \u03c4 (see Definition 2.1). Let {S 1 , . . . , S m : S i \u2282 S} be m randomly sampled subsets of the training set S, each of size \u03b1 \u2022 n for some \u03b1 \u2208 (0, 1). The linear datamodeling score (LDS) of a data attribution \u03c4 for a specific example z \u2208 Z is given by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "LDS(\u03c4, z) := \u03c1({ f (z; \u03b8 (S j )) : j \u2208 [m]}, {g \u03c4 (z, S j ; S) : j \u2208 [m]}),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "where \u03c1 denotes Spearman rank correlation [Spe04] . The attribution method's LDS for an entire test set is then simply the average per-example score.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 49,
                        "text": "[Spe04]",
                        "ref_id": "BIBREF83"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "Note that the linear datamodeling score defined above is quantitative, simple to compute, 4 and not tied to a specific task or modality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The linear datamodeling score (LDS)",
                "sec_num": "2.1"
            },
            {
                "text": "Definition 2.4 immediately suggests an \"optimal\" approach to data attribution (at least, in terms of optimizing LDS). This approach simply samples random subsets {S 1 , . . . S m } of the training set; trains a model on each subset (yielding {\u03b8 (S 1 ), . . . , \u03b8 (S m )}); evaluates each corresponding model output function f (z; \u03b8 (S j )); and then fits scores \u03c4(z) that predict f (z; \u03b8 (S i )) from the indicator vector 1 S i using (regularized) empirical risk minimization. Indeed, Ilyas et al. [IPE+22] take exactly this approach-the resulting datamodel-based attribution for an example z is then given by \u03c4 DM (z) := min",
                "cite_spans": [
                    {
                        "start": 498,
                        "end": 506,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An oracle for data attribution",
                "sec_num": "2.2"
            },
            {
                "text": "\u03b2\u2208R n 1 m m \u2211 i=1 \u03b2 1 S i -f (z; \u03b8 (S i )) 2 + \u03bb \u03b2 1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An oracle for data attribution",
                "sec_num": "2.2"
            },
            {
                "text": "(3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An oracle for data attribution",
                "sec_num": "2.2"
            },
            {
                "text": "The attributions \u03c4 DM (z) turn out to indeed perform well according to Definition 2.4-that is, they yield counterfactual predictions that are highly correlated with true model outputs (see Figure 1 ). Unfortunately, however, estimating accurate linear predictors (3) may require tens (or even hundreds) of thousands of samples (S j , f (z; \u03b8 (S j ))). In light of the above, we can view the approach of Ilyas et al. [IPE+22] as an \"oracle\" of sorts-it makes accurate counterfactual predictions (and as a result has found downstream utility [IPE+22; SPI+22; CJ22]), but is (often prohibitively) costly to compute.",
                "cite_spans": [
                    {
                        "start": 416,
                        "end": 424,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 196,
                        "end": 197,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "An oracle for data attribution",
                "sec_num": "2.2"
            },
            {
                "text": "How might we be able to circumvent the estimation cost of sampling-based attributions? Let us start by examining the existing data attribution methods-specifically, the ones that use only one (or a few) trained models-and evaluate them on our LDS benchmark.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data attribution methods beyond sampling",
                "sec_num": "2.3"
            },
            {
                "text": "Simulating re-training with influence functions. The bottleneck of the \"oracle\" datamodels attribution method (3) [IPE+22] is that obtaining each sample (S j , f (z; S j )) requires re-training our model of interest from scratch on each subset S j . An alternative approach could be to simulate the effect of this re-training by making some structural assumptions about the model being studiede.g., that its loss is locally well-approximated by a quadratic. This idea has inspired a long line of work around influence function estimation [KL17; PLS+20; SZV+22]. The resulting influence function attributions (Example 2.2) accurately approximate linear models and other simple models, but can perform poorly in non-convex settings (e.g., in the context of deep neural networks) [BPF21; IPE+22; BNL+22]. Indeed, as we can see in Figure 1 (and as we later study in Section 4), estimators based on influence functions [KL17; SZV+22; HL22a] significantly underperform on our LDS benchmark (Definition 2.4) when evaluated on neural networks on standard vision and natural language tasks.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 122,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 834,
                        "end": 835,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Data attribution methods beyond sampling",
                "sec_num": "2.3"
            },
            {
                "text": "There are also approaches that use more heuristic measures of training example importance for data attribution. These include methods based on, e.g., representation space similarity [ZIE+18; HYH+21] or gradient agreement [HL22a] . While such methods often yield qualitatively compelling results, our experiments (again, see Figure 1 ) indicate that, similarly to influence-based estimators, they are unable to make meaningful counterfactual predictions about model outputs in the large-scale, non-convex settings we evaluate them on.",
                "cite_spans": [
                    {
                        "start": 221,
                        "end": 228,
                        "text": "[HL22a]",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 331,
                        "end": 332,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Heuristic measures of example importance.",
                "sec_num": null
            },
            {
                "text": "We now present TRAK, a new data attribution method which is designed to be both effective and scalable in large-scale differentiable settings. (Recall from Definition 2.1 that a data attribution function is a function mapping examples z to a vector of per-training example scores in R n .)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK: Tracing with the Randomly-Projected After Kernel",
                "sec_num": "3"
            },
            {
                "text": "As a warm-up, and to illustrate the core primitive behind TRAK, we first study the simple case of logistic regression (Section 3.1). In this setting, data attribution is well-understood-in particular, there is a canonical attribution method [Pre81] that is both easy-to-compute and highly effective [WCZ+16; KAT+19]. In Section 3.2, using this canonical attribution method as a primitive, we derive our data attribution method \u03c4 TRAK (\u2022) (Equation (17), also summarized in Algorithm 1 in Section 3.4) which operates by reducing complex models back to the logistic regression case.5 ",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 248,
                        "text": "[Pre81]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK: Tracing with the Randomly-Projected After Kernel",
                "sec_num": "3"
            },
            {
                "text": "Consider the case where the model being studied is (a generalized form of) binary logistic regression. In particular, adapting our notation from Section 2, we consider a training set of n examples",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "S = {z 1 , . . . , z n : z i = (x i \u2208 R d , b i \u2208 R, y i \u2208 {-1, 1})},",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "where each example comprises an input x i \u2208 R d , a bias b i \u2208 R, and a label y i \u2208 {-1, 1}. The final model parameters \u03b8 (S) then minimize the log-loss over the training set, i.e., \u03b8 (S)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ":= arg min \u03b8 \u2211 (x i ,y i )\u2208S log 1 + exp(-y i \u2022 (\u03b8 x i + b i )) . (",
                        "eq_num": "4"
                    }
                ],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "(Note that when the bias terms b i are identically zero, we recover ordinary logistic regression.) The natural choice of model output function in this case is then the \"raw logit\" function:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f (z; \u03b8) := \u03b8 x + b, where z = (x, b, y).",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "Data attribution in this simple setting is a well-studied problem. In particular, the one-step Newton approximation [Pre81; WCZ+16; RM18; KAT+19], which we present as a data attribution method \u03c4 NS below, is a standard tool for analyzing and understanding logistic regression models in terms of their training data. (We present the theoretical basis for this method in Appendix C.1.) Definition 3.1 (One-step Newton approximation [Pre81] ). For logistic regression, we define the Newton step data attribution method \u03c4 NS as the approximate leave-one-out influence [Pre81] of training examples z i = (x i , b i , y i ) on the model output function (5). That is,",
                "cite_spans": [
                    {
                        "start": 430,
                        "end": 437,
                        "text": "[Pre81]",
                        "ref_id": "BIBREF70"
                    },
                    {
                        "start": 564,
                        "end": 571,
                        "text": "[Pre81]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c4 NS (z) i := x (X RX) -1 x i 1 -x i (X RX) -1 x i \u2022 p i (1 -p i ) (1 -p i ) \u2248 f (z; \u03b8 (S)) -f (z; \u03b8 (S \\ z i ))",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "where X \u2208 R n\u00d7k is the matrix of stacked inputs x i ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "p i := (1 + exp(-y i \u2022 f (z i ; \u03b8 ))) -1 is the predicted correct-class probability at \u03b8 and R is a diagonal n \u00d7 n matrix with R ii = p i (1 -p i ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "If our model class of interest was binary logistic regression, we could simply apply Definition 3.1 to perform data attribution. As we discuss, however, our goal is precisely to scale data attribution beyond such convex settings. To this end, we next derive our data attribution method TRAK (Tracing with the Randomly-projected After Kernel) which leverages \u03c4 NS (Definition 3.1) as a primitive.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Warmup: Data attribution for logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "We now present our method (TRAK) for scaling data attribution to non-convex differentiable settings. More precisely, following Definition 2.1, we describe how to compute a function \u03c4 TRAK : Z \u2192 R n that maps examples of interest z to vectors of per-training example importance scores in R n . The key primitive here will be Definition 3.1 from above-in particular, we will show how to adapt our problem into one to which we can apply the approximation (6).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "For ease of exposition, we will first show how to compute \u03c4 TRAK in the context of binary classifiers trained with the negative log-likelihood loss. We later generalize TRAK to other types of models (e.g., to multi-class classifiers in Section 3.3, to contrastive models in Section 5.1, and to language models in Section 5.2). In this setting, let the model output function f (z; \u03b8) be the raw output (i.e., the logit) of a binary classifier with parameters \u03b8. 6 The final parameters of the model can thus be written as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 (S) = arg min \u03b8 \u2211 (x i ,y i )\u2208S log [1 + exp (-y i \u2022 f (z i ; \u03b8))] .",
                        "eq_num": "(7)"
                    }
                ],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "Note that unlike in Section 3.1, we do not assume that the model itself is linear-e.g., the model might be a deep neural network parameterized by weights \u03b8.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "We implement TRAK as a sequence of five steps:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "1. Linearizing the model output function (via Taylor approximation), which reduces the model of interest to a linear function in parameter space. Prior work (around, e.g., the empirical neural tangent kernel) suggests that this approximation can be relatively accurate, especially for overparameterized neural networks [JGH18; WHS22; Lon21; MWY+22].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "2. Reducing the dimensionality of the linearized model using random projections. Specifically, we take advantage of the Johnson-Lindenstrauss lemma [JL84] , which guarantees that this projection preserves the model-relevant information.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 154,
                        "text": "[JL84]",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "3. Estimating attribution scores by leveraging the attribution method described in Definition 3.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "4. Ensembling results over several models, each trained on a random subset of the original training set S.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "5. Sparsifying the attribution scores using soft-thresholding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "We discuss these steps in more depth below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "(Step 1) Linearizing the model. Recall that our goal here is to apply the data attribution method \u03c4 NS from Definition 3.1. The main roadblock to applying Definition 3.1 in our setting is that we are studying a non-linear model-that is, our model output function may not be a linear function of \u03b8.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "We address this issue by approximating f (z; \u03b8) with its Taylor expansion centered around the final model parameters \u03b8 . In particular, for any \u03b8, we replace f (z; \u03b8) with f (z; \u03b8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ":= f (z; \u03b8 ) + \u2207 \u03b8 f (z; \u03b8 ) (\u03b8 -\u03b8 ).",
                        "eq_num": "(8)"
                    }
                ],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "This approximation suggests a change in perspective-rather than viewing f (z; \u03b8) as a non-linear model acting on inputs x, we can view it as a linear model acting on inputs \u2207 \u03b8 f (z; \u03b8 ). In particular, rewriting the loss minimization (7) while replacing f (z; \u03b8) with f (z; \u03b8) yields",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "\u03b8 (S) = arg min \u03b8 \u2211 (x i ,y i )\u2208S log 1 + exp -y i \u2022 \u03b8 \u2207 \u03b8 f (z i ; \u03b8 ) + f (z i ; \u03b8 ) -\u2207 \u03b8 f (z i ; \u03b8 ) \u03b8 . (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "Now, Equation (9) should look familiar-specifically, if we define the variables",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "g i := \u2207 \u03b8 f (z i ; \u03b8 ) and b i := f (z i ; \u03b8 ) -\u2207 \u03b8 f (z i ; \u03b8 ) \u03b8 , then (9) becomes \u03b8 (S) = arg min \u03b8 \u2211 (g i ,b i ,y i ) log 1 + exp -y i \u2022 \u03b8 g i + b i .",
                        "eq_num": "(10)"
                    }
                ],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "Comparing (10) to (4) (from Section 3.1) makes it clear that we can view \u03b8 as the solution to a (generalized) logistic regression, in which the inputs x i are gradients g i := \u2207 \u03b8 f (z i ; \u03b8 ) of the model, the bias terms are b i := f (z i ; \u03b8 ) -\u2207 \u03b8 f (z i ; \u03b8 ) \u03b8 and the labels y i remain the same.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRAK for binary (non-linear) classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "In the context of neural networks, we can view Step 1 as replacing the binary classifier with its empirical neural tangent kernel (eNTK) approximation [JGH18; ABP22; WHS22]. We discuss how TRAK connects to the eNTK in more detail in Section 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Note:",
                "sec_num": null
            },
            {
                "text": "Step 1 dramatically simplifies our model class of interest from a highly non-linear classifier to simple logistic regression. Still, the resulting logistic regression can be extremely high dimensional.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "In particular, the input dimension of the linear model ( 8) is the number of parameters of the original model (which can be on the order of millions or billions), not the dimensionality of the inputs x i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "To reduce the dimensionality of this problem, we leverage a classic result of Johnson and Lindenstrauss [JL84] . This result guarantees that multiplying each gradient g i = \u2207 \u03b8 f (z i ; \u03b8 ) \u2208 R p by a random matrix P \u223c N (0, 1) p\u00d7k for k p preserves inner products g i g j with high probability7 (while significantly reducing the dimension). Thus, we define the \"feature map\" \u03c6 : Z \u2192 R k as",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 110,
                        "text": "[JL84]",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c6(z) := P \u2207 \u03b8 f (z; \u03b8 ),",
                        "eq_num": "(11)"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "i.e., a function taking an example z to its corresponding projected gradient, and from now on replace g i with",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c6 i := \u03c6(z i ) = P g i = P \u2207 \u03b8 f (z i ; \u03b8 ). (",
                        "eq_num": "12"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "(",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "Step 3) Estimating influences. Now that we have simplified our original model of interest to a logistic regression problem of tractable dimension, we can finally adapt Definition 3.1. To this end, recall that the training \"inputs\" are now the (projected) gradients \u03c6 i (see (12)). We thus replace the matrix X in (6) with the matrix \u03a6 := [\u03c6 1 ; . . . , \u03c6 n ] \u2208 R n\u00d7k of stacked projected gradients. We also find empirically that both the denominator in (6) and the diagonal matrix R have little effect on the resulting estimates, and so we omit them from our adapted estimator. Our estimator for attribution scores for an example of interest z thus becomes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c4(z, S) := \u03c6(z) (\u03a6 \u03a6) -1 \u03a6 Q, (",
                        "eq_num": "13"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "where we recall from (11) that \u03c6(z) = P \u2207 \u03b8 f (z; \u03b8 ), and where we define",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Q := diag({1 -p i }) = diag (1 + exp(y i \u2022 f (z i ; \u03b8 ))) -1",
                        "eq_num": "(14)"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "to be the n \u00d7 n diagonal matrix of \"one minus correct-class probability\" terms.8 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "Remark. An alternative way to motivate our single-model estimator (Equation ( 13)) is to compute the influence function [KL17] using the generalized Gauss-Newton approximation to the Hessian [SEG+17; Mar20 To \"smooth out\" the impact of such seed-based differences, we aggregate the estimator (13) across multiple trained models (for computational efficiency, one can also use different checkpoints from the same model-see Appendix E.3). In particular, we adopt the natural idea of just averaging \u03c4(z, S) from (13) directly, with two small modifications:",
                "cite_spans": [
                    {
                        "start": 120,
                        "end": 126,
                        "text": "[KL17]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "(a) Rather than computing M copies of (13) and averaging the results, we separately compute and average M copies of Q (i.e., ( 14)) and M copies of \u03c6(z) (\u03a6 \u03a6) -1 \u03a6 (i.e., the remaining terms in (13)). We then take the product of these averaged matrices.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "(b) Rather than training M copies of the same model \u03b8 (S), we sample M random subsets of S (S 1 , . . . , S M ), and use the resulting models \u03b8 (S 1 ), . . . , \u03b8 (S M ) to compute attribution scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "The first modification (a) is mainly for numerical stability, while the second modification (b) is meant to better handle duplicated training examples (and, more generally, features that are highly \"redundant\" in the training data). We study the effect of these modifications empirically in Appendix E. At this point, our estimator is of the form:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c4 M (z, S) := 1 M M \u2211 i=1 Q m \u2022 1 M M \u2211 i=1 \u03c6 m (z) (\u03a6 m \u03a6 m ) -1 \u03a6 m ,",
                        "eq_num": "(15)"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "where S 1 , . . . , S M are M randomly selected subsets of the training set S; \u03a6 m are the corresponding projected gradients from the model \u03b8 (S m ); \u03c6 m (z) is the featurized example z under model \u03b8 (S m ); and Q m is the corresponding matrix of probabilities as defined in Equation ( 14).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "(Step 5) Inducing sparsity via soft-thresholding. In the last step, we post-process the attribution scores from Step 4 via soft thresholding, a common denoising method in statistics [Don95] for when an underlying signal is known to be sparse. Within our particular context, Ilyas et al. [IPE+22] find that for neural networks attribution scores are often sparse-that is, each test example depends on only a few examples from the training set. Motivated by this observation, we apply the soft thresholding operator S(\u2022; \u03bb) defined for any \u03c4 \u2208 R n as:",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 189,
                        "text": "[Don95]",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 287,
                        "end": 295,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "S(\u03c4; \u03bb) = (\u03c4 i -\u03bb) \u2022 1{\u03c4 i > \u03bb} + (\u03c4 i + \u03bb) \u2022 1{\u03c4 i < -\u03bb}. (",
                        "eq_num": "16"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "We choose the soft threshold parameter \u03bb via cross-validation. That is, given a set of trained models, we first estimate attribution scores (15), then sample a range of values for \u03bb, compute corresponding attribution scores by applying (16), and finally select the value of \u03bb that yields that highest linear datamodeling score (Definition 2.4) on the set of trained models. After applying soft-thresholding, our final estimator becomes",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c4 TRAK (z, S) := S 1 M M \u2211 i=1 Q m \u2022 1 M M \u2211 i=1 \u03c6 m (z) (\u03a6 m \u03a6 m ) -1 \u03a6 m , \u03bb",
                        "eq_num": "(17)"
                    }
                ],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "where, again, \u03bb is selected via cross-validation (see Appendix A.2 for details).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "(Step 2) Reducing dimensionality with random projections. The linear approximation from",
                "sec_num": null
            },
            {
                "text": "In the previous section, we instantiated TRAK for binary classifiers; we now show how to extend TRAK to the multi-class setting. Recall that our key insight in the binary case was to linearize the model output function f (z; \u03b8) around the optimal parameters \u03b8 (S) (see (8)). Our choice of output function (i.e., the raw logit of the classifier) allowed us to then cast the original (non-convex) learning problem of interest as an instance of binary logistic regression with inputs \u2207 \u03b8 f (z; \u03b8 ). That is, we made the approximation",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 (S) \u2248 arg min \u03b8 \u2211 z i \u2208S log 1 + exp -y i \u2022 \u2207 \u03b8 f (z i ; \u03b8 ) \u03b8 + b i , (",
                        "eq_num": "18"
                    }
                ],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "and then leveraged Definition 3.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "To apply this same approach to the c-class setting (for c > 2), one possibility is to first transform the problem into c 2 binary classification problems, then apply the approach from Section 3.2 directly. (For example, Malladi et al. [MWY+22] use this transformation to apply the neural tangent kernel to c-way classification problems.) In large-scale settings, however, it is often expensive or infeasible to study of all c 2 subproblems, e.g., ImageNet has c = 1000 classes.",
                "cite_spans": [
                    {
                        "start": 235,
                        "end": 243,
                        "text": "[MWY+22]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "We thus take a different approach. In short, we leverage the fact that we always have labels available (even for test examples) to reduce the multi-class classification problem to a single logistic regression. More specifically, for an example z = (x, y), we define the model output function",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f (z; \u03b8) := log p(z; \u03b8) 1 -p(z; \u03b8) , (",
                        "eq_num": "19"
                    }
                ],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "where p(z; \u03b8) is the softmax probability assigned to the correct class.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "A crucial property of the model output function ( 19) is that it allows us to rewrite the loss function for c-way classification as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L(z; \u03b8) = -log(p(z; \u03b8)) (20) = log [1 + exp (-f (z; \u03b8))] ,",
                        "eq_num": "(21)"
                    }
                ],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "where the first line is the definition of cross-entropy loss, and the second line comes from (19). As a result, if we linearize f (z; \u03b8) as in Step 1 above (Section 3.2), we can make the approximation",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "\u03b8 (S) \u2248 arg min \u03b8 \u2211 z i \u2208S log 1 + exp -\u2207 \u03b8 f (z i ; \u03b8 ) \u03b8 + b i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "This approximation is identical to the one we made for the binary case (see ( 18)). We can thus treat the multi-class problem as a single binary logistic regression with inputs \u2207 \u03b8 f (z i ; \u03b8 )9 and then apply Steps 2-5 from Section 3.2 directly to this binary problem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extending to multi-class classification",
                "sec_num": "3.3"
            },
            {
                "text": "We summarize our final algorithm for computing the data attribution method \u03c4 TRAK in the general multi-class case (see also Equation ( 17)) in Algorithm 1. The output of the algorithm is an attribution matrix T, whose rows are given by \u03c4 TRAK (z, S). To make Algorithm 1 efficient even for very large models, we implemented a highly optimized random projector, which we discuss in Appendix B.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "Algorithm 1 TRAK for multi-class classifiers (as implemented)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "1: Input: Learning algorithm A, dataset S of size n, sampling fraction \u03b1 \u2208 (0, 1], correct-class likelihood function p(z; \u03b8), projection dimension k \u2208 N 2: Output: Matrix of attribution scores T \u2208 R n\u00d7n 3: f (z; \u03b8) := log( p(z;\u03b8) 1-p(z;\u03b8) ) Margin function f \u03b8 4: for m \u2208 {1, . . . , M} do 5: Sample random S \u2282 S of size \u03b1 \u2022 n 6: \u03b8 m \u2190 A(S )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "Train a model on S 7:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "P \u223c N (0, 1) p\u00d7k Sample projection matrix 8:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "Q (m) \u2190 0 n\u00d7n 9:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "for i \u2208 {1, . . . , n} do 10:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "\u03c6 i \u2190 P \u2207 \u03b8 f (z i ; \u03b8 m )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "Compute gradient at \u03b8 m and project to k dimensions 11:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "Q (m) ii \u2190 1 -p(z i ; \u03b8 ) Compute weighting term 12:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "end for 13:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "\u03a6 m \u2190 [\u03c6 1 ; \u2022 \u2022 \u2022 ; \u03c6 n ] 14: end for 15: T \u2190 1 m M \u2211 m=1 \u03a6 m (\u03a6 m \u03a6 m ) -1 \u03a6 m 1 m M \u2211 m=1 Q (m) 16: return SOFT-THRESHOLD(T)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing TRAK",
                "sec_num": "3.4"
            },
            {
                "text": "We now evaluate TRAK (see Equation ( 17) and Algorithm 1 in Section 3.4) in a variety of vision and natural language settings. To this end, we compare TRAK with existing data attribution methods and show that it achieves significantly better tradeoffs between efficacy and computational efficiency.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating TRAK",
                "sec_num": "4"
            },
            {
                "text": "We evaluate and study TRAK with the following experimental setup. Datasets, models, and baselines. We use ResNet-9 classifiers trained on the CIFAR dataset (CIFAR-10, and a two-class subset called CIFAR-2); ResNet-18 [HZR+15] classifiers trained on the 1000-class ImageNet [RDS+15] dataset, and pre-trained BERT [DCL+19] models finetuned on the QNLI (Question-answering Natural Language Inference) classification task from the GLUE benchmark [WSM+18] . We provide further details on these choices of dataset and task in Appendix A.1.",
                "cite_spans": [
                    {
                        "start": 217,
                        "end": 225,
                        "text": "[HZR+15]",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 273,
                        "end": 281,
                        "text": "[RDS+15]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 442,
                        "end": 450,
                        "text": "[WSM+18]",
                        "ref_id": "BIBREF94"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "4.1"
            },
            {
                "text": "To put TRAK's performance into context, we also evaluate a variety of existing attribution methods, including influence functions [KL17] ; a variant based on the Arnoldi iteration [SZV+22] ; TracIn [PLS+20] ; gradient aggregated similarity (GAS) [HL22b] ; representation similarity [HYH+21] ; empirical influences [FZ20] ; and datamodels [IPE+22] . (See Appendix A.3 for more details.)",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 136,
                        "text": "[KL17]",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 180,
                        "end": 188,
                        "text": "[SZV+22]",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 198,
                        "end": 206,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 246,
                        "end": 253,
                        "text": "[HL22b]",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 282,
                        "end": 290,
                        "text": "[HYH+21]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 314,
                        "end": 320,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 338,
                        "end": 346,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "4.1"
            },
            {
                "text": "Evaluation with linear datamodeling scores. For each method and each dataset we consider, we compute its linear datamodeling score (LDS) as described in Definition 2.4. Specifically, let \u03c4 be a given data attribution method (as framed in Definition 2.1), and let g \u03c4 (z, S ; S) be its corresponding attribution-derived prediction function (see Definition 2.3). Then, to evaluate \u03c4:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "4.1"
            },
            {
                "text": "1. We sample 100 different random subsets {S j \u2282 S : j \u2208 [100]} of the training set S, and train five models on each one of these subsets. Each subset S j is sampled to be 50% of the size of S, but we also consider other subsampling ratios in Appendix D.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "4.1"
            },
            {
                "text": "For each example of interest z (i.e., for each example in the test set of the dataset we are studying), we approximate the expectation of the model output E[ f (z; \u03b8 i (S j ))] for each training subset S j (where the expectation is taken over the learning algorithm's randomness) by averaging across the corresponding five models {\u03b8 i (S j )} 5 i=1 . 3. We then compute the linear datamodeling score for each example of interest z as the Spearman rank correlation [Spe04] between the averaged model outputs computed in the previous step and the attribution-derived predictions g \u03c4 (z, S j ; S) of model outputs. That is, we compute:",
                "cite_spans": [
                    {
                        "start": 464,
                        "end": 471,
                        "text": "[Spe04]",
                        "ref_id": "BIBREF83"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2.",
                "sec_num": null
            },
            {
                "text": "Spearman-\u03c1 1 5 5 \u2211 i=1 f (z; \u03b8 i (S j )) : j \u2208 [100] averaged model outputs , {g \u03c4 (z, S j ; S) : j \u2208 [100]}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2.",
                "sec_num": null
            },
            {
                "text": "attributed-derived predictions of model outputs 4. Finally, we average the LDS (Definition 2.4) across 2,000 examples of interest, sampled uniformly at random from the validation set, and report this score along with the 95% bootstrap confidence intervals corresponding to the random re-sampling from the subsets S j .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2.",
                "sec_num": null
            },
            {
                "text": "Computational cost. We quantify the computational cost of each attribution method using two metrics. The first one is the total wall-time of computing attribution scores on a single A100 GPU. This metric is intuitive and useful, but depends on implementation details and hardware. We thus also study a second metric, namely, the total number of trained models used. This metric is hardware and implementation-agnostic; it is motivated by an observation that for large models, the time it takes to compute attribution scores will be dominated by the time it takes to train the models needed for attribution. 10 We find that for both metrics, our results lead to similar conclusions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2.",
                "sec_num": null
            },
            {
                "text": "Across all models and datasets that we consider, TRAK attains a significantly better tradeoff between efficacy (as measured by the LDS) and computational efficiency than all the other attribution methods that we examine (see Figures 1 and 2 and Table D .2). Indeed, TRAK attains efficacy comparable to datamodels (which achieves the best performance among existing methods when unconstrained) with a computational footprint that is (on average) over 100x smaller. Understanding the roots of TRAK's performance. In Appendix E, we study the roots of TRAK's performance through an extensive ablation study. We vary, for example, how we linearize the model of interest (Step 1 in Section 3.2), the dimension k of the random projection we use (Step 2 in Section 3.2), how we apply the Newton step attribution from Definition 3.1 (Step 3 in Section 3.2), and how we aggregate information from independently trained models (Step 4 in Section 3.2). As a byproduct of this investigation, we find two ways of computing TRAK at even lower cost: (a) leveraging models that have not been trained to convergence, and (b) taking advantage of multiple checkpoints from the same model, rather than multiple models from independent training runs. We find (see Tables 5 and 6 , explained further and reproduced in Appendix E) that both of these optimizations can dramatically reduce TRAK's computational cost without significantly degrading its performance. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 233,
                        "end": 234,
                        "text": "1",
                        "ref_id": "FIGREF11"
                    },
                    {
                        "start": 239,
                        "end": 240,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 251,
                        "end": 252,
                        "text": "D",
                        "ref_id": null
                    },
                    {
                        "start": 1249,
                        "end": 1250,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 1255,
                        "end": 1256,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "In Section 4, we evaluated our data attribution method TRAK on standard image classification and NLP tasks and compared its performance to existing attribution methods. We now illustrate the usefulness of TRAK through three additional applications:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applications of TRAK",
                "sec_num": "5"
            },
            {
                "text": "Attributing CLIP models. In Section 5.1, we use TRAK to study image-text embeddings of models trained with the CLIP contrastive loss [RKH+21] . In particular, we show how leveraging TRAK allows us to identify small subsets of the training set that, when removed, cause the resulting CLIP embeddings to fail to capture a given image-caption pair association.",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 141,
                        "text": "[RKH+21]",
                        "ref_id": "BIBREF74"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applications of TRAK",
                "sec_num": "5"
            },
            {
                "text": "Fact tracing language models. Next, in Section 5.2, we use TRAK to provide data attribution for language models [VSP+17] . Accelerating datamodel applications. Finally, in Section 5.3, we use TRAK to accelerate two downstream applications that leverage datamodel scores. That is, first, we look at the problem of estimating prediction brittleness using datamodel scores [IPE+22] . Then, we revisit the MODELDIFF algorithm [SPI+22] , which leverages datamodel scores for learning algorithm comparison, i.e., the task of distinguishing two learning algorithms based on feature priors they instill. For both applications, using TRAK scores in place of datamodel scores reduces the total computational cost by at least a factor of 100 while retaining the same effectiveness.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 120,
                        "text": "[VSP+17]",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 370,
                        "end": 378,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 422,
                        "end": 430,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applications of TRAK",
                "sec_num": "5"
            },
            {
                "text": "Recent works have found that one can leverage natural language supervision to help models learn a rich joint image-text embedding space. In particular, CLIP (Contrastive Language-Image Pre-training) [RKH+21] representations have become a versatile primitive bridging visual and language domains and is used, for example, for zero-shot classification [RKH+21] and as text encoders for latent diffusion models [RBL+22] . While the quality of these representations-as measured by aggregate metrics such as downstream zero-shot accuracy-appears to be driven largely by the properties and scale of the training datasets [FIW+22; SDT+22; CBW+22], we lack a fine-grained understanding of how the composition of the training data contributes to learning well-aligned representations. To that end, we use TRAK to investigate how training data influences the resulting CLIP embeddings at a local level. That is, we want to be able to pin-point training examples that cause a model to learn a given specific image-caption pair association.",
                "cite_spans": [
                    {
                        "start": 199,
                        "end": 207,
                        "text": "[RKH+21]",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 350,
                        "end": 358,
                        "text": "[RKH+21]",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 408,
                        "end": 416,
                        "text": "[RBL+22]",
                        "ref_id": "BIBREF72"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attributing CLIP models",
                "sec_num": "5.1"
            },
            {
                "text": "Similarly to the classification setting we were considering so far, we need to first choose an appropriate model output function (see, e.g., Equation ( 19)) to compute attribution scores with TRAK. This choice will be motivated by the CLIP training loss (which we review below) and will reduce our setting back to the classification case.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": "The CLIP loss. A CLIP model with parameters \u03b8 takes in an image-caption pair (x, y) and outputs an image embedding \u03c6(x; \u03b8) and a text embedding \u03c8(y; \u03b8). Given a (random) batch of training examples B = {(x 1 , y 1 ), ..., (x n , y n )}, the CLIP training loss computes all n \u00d7 n pairwise cosine similarities between the image and text embeddings",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": "S ij := \u03c6(x i ; \u03b8) \u2022 \u03c8(y j ; \u03b8),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": "and aims to maximize the cosine similarities S ii of correct pairs while minimizing the cosine similarities S ij , for i = j, of incorrect pairs. More specifically, the training loss of example (x i , y i ) \u2208 B is defined as the following symmetric cross entropy over the similarity scores S ij :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L(x i , y i ; \u03b8) = -log exp(S ii ) \u2211 1\u2264j\u2264n exp(S ij ) -log exp(S ii ) \u2211 1\u2264j\u2264n exp(S ji ) , (",
                        "eq_num": "22"
                    }
                ],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": "where the first term corresponds to matching each image x i to its correct caption y i , and the second term corresponds to matching each caption to its correct image. In effect, we are solving two classification problems: one where the images are inputs and captions (from the same batch) are labels, and vice versa.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing TRAK for CLIP",
                "sec_num": "5.1.1"
            },
            {
                "text": "Recall that in the classification setting we trained the model with the cross entropy loss (i.e.,log p(z; \u03b8), where p(z; \u03b8) is the correct-class probability), and used the model output function f (z; \u03b8) = log p(z; \u03b8)/(1p(z; \u03b8)) (Equation ( 19)), i.e., the logit transform of the correct-class probability to compute TRAK scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reducing to classification.",
                "sec_num": null
            },
            {
                "text": "To take advantage of the same formula in the CLIP setting, note that our loss (22) can be viewed as having the form",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reducing to classification.",
                "sec_num": null
            },
            {
                "text": "L(x i , y i ; \u03b8) = -log p 1 (x i , y i ; \u03b8) -log p 2 (x i , y i ; \u03b8),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reducing to classification.",
                "sec_num": null
            },
            {
                "text": "where p 1 (x i , y i ; \u03b8) corresponds to the probability of matching an image to its corresponding caption based on the cosine similarity, and likewise for p 2 (x i , y i ; \u03b8). A natural choice of model output function in this case, then, is using the sum of the model output functions corresponding to the two classification problems:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reducing to classification.",
                "sec_num": null
            },
            {
                "text": "f (x i , y i ; \u03b8) := log p 1 (x i , y i ; \u03b8) 1 -p 1 (x i , y i ; \u03b8) + log p 2 (x i , y i ; \u03b8) 1 -p 2 (x i , y i ; \u03b8) = -log \u2211 1\u2264j\u2264n exp(S ij -S ii ) -log \u2211 1\u2264j\u2264n exp(S ji -S ii ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reducing to classification.",
                "sec_num": null
            },
            {
                "text": "Indeed, this choice allows us once again (see Section 3.3) to reduce our problem to an instance of logistic regression and apply the same formula for influence approximation (Definition 3.1) as before. We can then also compute TRAK scores following the same approach (i.e., using Algorithm 1 in Section 3.4). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reducing to classification.",
                "sec_num": null
            },
            {
                "text": "We train image-text models (with a ResNet-50 [HZR+15] as the image encoder and a Transformer [VSP+17] as the text encoder) using the CLIP objective on MS COCO [LMB+14] . To evaluate the effectiveness of TRAK applied to such CLIP models, we perform a qualitative (visual) analysis; and a quantitative (counterfactual) evaluation. In both cases, we compare TRAK with TracIn and CLIP similarity distance11 baselines.",
                "cite_spans": [
                    {
                        "start": 45,
                        "end": 53,
                        "text": "[HZR+15]",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 93,
                        "end": 101,
                        "text": "[VSP+17]",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 159,
                        "end": 167,
                        "text": "[LMB+14]",
                        "ref_id": "BIBREF58"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.1.2"
            },
            {
                "text": "training examples having the highest attribution scores (according to TRAK and CLIP similarity distance-see Appendix D.3 for the analysis corresponding to TracIn). For the first example, the nearest neighbor in the CLIP space (the polar bear) turns out to have a negative attribution score according to TRAK. For the second example, the most helpful TRAK examples are the ones for which the captions contain the phrase \"a couple of animals\" but where the images do not necessarily feature giraffes (possibly because the target caption does not mention \"giraffe\" either). On the other hand, the most helpful examples according to CLIP similarity distance all feature giraffes. These differences suggest that TRAK attribution scores may capture significantly different traits from CLIP similarity distance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual analysis. Figure 7 displays two target examples of interest along with the corresponding",
                "sec_num": null
            },
            {
                "text": "each attribution method affect the CLIP model's ability to learn a given image-caption association. Specifically, we say that a CLIP model has learned a given association between an image and a caption whenever their corresponding image and caption embeddings have high cosine similarity relative to other image-caption pairs. To evaluate each attribution method (i.e., TRAK, TracIn, and CLIP similarity distance), for a given target image-caption pair, we remove from the training set the k examples with the most positive attribution scores a given attribution method produces, and then re-train a model from scratch (averaging over ten training runs to reduce stochasticity). Finally, we examine the decrease in cosine similarity between the embeddings of target image and caption pair, and average this result over different target pairs. Our results (Figure 8 ) indicate that removing training inputs identified by TRAK can significantly degrade the model's ability to learn the target image-caption pair. Indeed, removing just k = 400 target-specific training puts (i.e., less than 0.5% of the train set) decreases the (average) CLIP similarity distance between the target image and caption embeddings by 0.36. In contrast, removing the same number of nearest neighbors in CLIP space results in a much smaller effect size (a 0. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 863,
                        "end": 864,
                        "text": "8",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Counterfactual evaluation. We next investigate to what extent training examples identified by",
                "sec_num": null
            },
            {
                "text": "As large language models are deployed in a variety of contexts, e.g., as conversation agents [TDH+22] or knowledge bases [PRR+19] , there is an emerging need to be able to attribute models' outputs back to specific data sources [BTV+22] . To that end, we study fact tracing [ABL+22], i.e., the task of identifying the training examples that cause a language model to generate a given \"fact.\"",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 101,
                        "text": "[TDH+22]",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 121,
                        "end": 129,
                        "text": "[PRR+19]",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 228,
                        "end": 236,
                        "text": "[BTV+22]",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact tracing for large language models (mT5)",
                "sec_num": "5.2"
            },
            {
                "text": "A benchmark for fact tracing. Akyurek et al. [ABL+22] develop a testbed for the fact tracing problem by way of a dataset (and corresponding evaluation methodology) called FTRACE-TREX. We provide a high-level overview of FTRACE-TREX here, and describe it in more depth in Appendix F.1. The FTRACE-TREX dataset consists of a set of \"abstracts\" and a set of \"queries,\" both of which pertain to the same database of \"facts.\" Akyurek et al. [ABL+22] annotate each abstract with a set of facts it expresses, and each query with the (single) fact that it asks about. As a part of the task setup, one finetunes a pre-trained language model on the set of abstracts using masked language modeling,12 and then evaluates this model's correctness on each query in the query set. This step defines a set of \"novel facts,\" i.e., queries that the model answers correctly only after finetuning.",
                "cite_spans": [
                    {
                        "start": 45,
                        "end": 53,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 436,
                        "end": 444,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact tracing for large language models (mT5)",
                "sec_num": "5.2"
            },
            {
                "text": "With the above setup in place, we can define the FTRACE-TREX fact tracing benchmark. Akyurek et al. [ABL+22] reason that each novel fact (as identified above) should have been learned (during finetuning) from the abstracts that express the same fact. The benchmark thus evaluates a given data attribution method's ability to retrieve, for each novel fact, the abstracts in the training set that express the same fact. (Such abstracts are called the ground-truth proponents of the query.)",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 108,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact tracing for large language models (mT5)",
                "sec_num": "5.2"
            },
            {
                "text": "In particular, observe that applying a data attribution method \u03c4(\u2022) to a particular query (treating the set of abstracts as the training set) yields scores that we can use as a ranking over the set of the abstracts. Akyurek et al. [ABL+22] compute the mean reciprocal rank (MRR) of the ground-truth proponents in this ranking (see Appendix F.1), a standard metric from information retrieval, to quantify the efficacy of \u03c4(\u2022) at fact tracing. We evaluate TRAK on this benchmark, along with two baselines from [ABL+22] , TracIn [PLS+20] and the information retrieval method BM25 [RWJ+95] .",
                "cite_spans": [
                    {
                        "start": 231,
                        "end": 239,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 508,
                        "end": 516,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 526,
                        "end": 534,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 577,
                        "end": 585,
                        "text": "[RWJ+95]",
                        "ref_id": "BIBREF78"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact tracing for large language models (mT5)",
                "sec_num": "5.2"
            },
            {
                "text": "Computing TRAK scores for language models. To apply TRAK to this setting, we need to choose an appropriate model output function, as we did before for the classification setting (see Section 3.3) and for CLIP (see Section 5.1). To this end, we observe that the masked language modeling objective has a natural interpretation as a sequence of v-way classification problems over the masked tokens, where v is the vocabulary size. Thus, inspired by our analysis of the multi-class classification setting from Section 3.3, we choose the model output function for this setting to be the sum of the \"canonical\" model output function (19) for each of the v-way classification problems (see Appendix F.3 for more details).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact tracing for large language models (mT5)",
                "sec_num": "5.2"
            },
            {
                "text": "We find that while TRAK significantly outperforms TracIn on the FTRACE-TREX benchmark (0.42 vs. 0.09 using the aforementioned MRR score), neither method matches the performance of the information retrieval baseline BM25 (0.77 MRR). 13To understand the possible roots of TRAK's underperformance relative to BM25 on FTRACE-TREX, we carry out a counterfactual analysis. 14 Specifically, for a subset S of the FTRACE-TREX query set, we create three corresponding counterfactual training sets. Each such training set corresponds to removing one of three collections of abstracts from the FTRACE-TREX abstract set:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussion",
                "sec_num": "5.2.1"
            },
            {
                "text": "(a) the most important abstracts for model performance on S , as estimated by TRAK;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussion",
                "sec_num": "5.2.1"
            },
            {
                "text": "(b) the abstracts that are most similar to the queries in S according to BM25;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussion",
                "sec_num": "5.2.1"
            },
            {
                "text": "(c) the corresponding \"ground-truth proponents\" for the queries in S as per FTRACE-TREX.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussion",
                "sec_num": "5.2.1"
            },
            {
                "text": "We then measure the average decrease in performance on S when a model is finetuned on these counterfactual datasets compared finetuning on the full training set. Intuition would suggest that performance would decrease the most when models are trained on the counterfactual training set (c); in particular, there is ostensibly no direct evidence for any of the facts corresponding to the queries in S anywhere in that set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussion",
                "sec_num": "5.2.1"
            },
            {
                "text": "We find (see Figure 9 ), however, that it is only the TRAK-based counterfactual training set that causes a large change in model behavior. That is, removing abstracts identified with TRAK leads to a 34% decrease in accuracy, significantly more than the decreases induced by removing abstracts according to BM25 (10%) or even removing ground-truth proponents (12%). Discussion. Our results demonstrate that while TRAK may not be effective at identifying abstracts that directly express the same fact as a given query (i.e., the ground-truth proponents as defined by FTRACE-TREX), it can successfully identify the abstracts that are most responsible for the finetuned model learning a given fact. In particular, TRAK's subpar performance on the attribution benchmark is an artifact of the FTRACE-TREX benchmark rather than a flaw of TRAK itself.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 20,
                        "end": 21,
                        "text": "9",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Results and discussion",
                "sec_num": "5.2.1"
            },
            {
                "text": "There are several potential explanations for this phenomenon, many of which Akyurek et al.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ground",
                "sec_num": null
            },
            {
                "text": "\u2022 There may be errors in the FTRACE-TREX benchmark. (Although, given the drastic difference between the TRAK scores and the ground-truth labels in their ability to identify counterfactually important abstracts, such data errors are unlikely to be the sole culprit.)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "[ABL+22] already discuss in their work:",
                "sec_num": null
            },
            {
                "text": "\u2022 Models may be answering queries by combining facts from the training set. For example, neither \"The largest pyramid is in Giza\" nor \"Giza is a city in Egypt\" would be ground-truth proponents for the query \"Which country is home to the largest pyramid?\" in FTRACE-TREX, but a model that learns both of these facts may still be able to correctly answer that query.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "[ABL+22] already discuss in their work:",
                "sec_num": null
            },
            {
                "text": "\u2022 Alternatively, models may be learning from the syntactic rather than semantic structure of abstracts. For example, a model may correctly answer that a person from Korea is called a \"Korean\" by learning from an abstract which says \"A person from Bulgaria is Bulgarian.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "[ABL+22] already discuss in their work:",
                "sec_num": null
            },
            {
                "text": "More broadly, our results highlight a difference between fact tracing and behavior tracing. In other words, finding a data source that supports a given model-generated text is a different task than identifying the actual data sources that caused the model to generate this text in the first place. While we may be able to address the former problem with model-independent techniques such as information retrieval or web search, the latter requires methods that remain faithful to (and thus, dependent on) the model being studied. Our results here indicate that TRAK can be an effective tool for the latter problem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "[ABL+22] already discuss in their work:",
                "sec_num": null
            },
            {
                "text": "Our evaluation thus far has demonstrated that data attribution scores computed with TRAK can predict how a given model's output changes as a function of the composition of the corresponding model's training set. While the capability to make such predictions is useful in its own right, prior work has shown that this primitive also enables many downstream applications [KL17; JDW+19; AV20]. For example, prior works leverage datamodel scores to identify brittle predictions [IPE+22] and to compare different learning algorithms [SPI+22] . We now show that using TRAK in place of datamodel scores can significantly speed up these downstream applications too.",
                "cite_spans": [
                    {
                        "start": 474,
                        "end": 482,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 528,
                        "end": 536,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Accelerating datamodel applications",
                "sec_num": "5.3"
            },
            {
                "text": "Ilyas et al. [IPE+22] use datamodel scores to provide lower bounds on the brittleness of a given example-that is, given an example of interest z, they identify a subset of the training set whose removal from the training data causes the resulting re-trained model to misclassify z. The brittleness estimation algorithm that Ilyas et al. [IPE+22] leverage hinges on the fact that the datamodel attribution function \u03c4 DM (z) can accurately predict model outputs, i.e., achieve high LDS. Motivated by TRAK's good performance on the linear datamodeling task (see, e.g., Figure 2 ), we examine estimating the brittleness of CIFAR-10 examples using TRAK scores in place of datamodel ones (but otherwise following the procedure of Ilyas et al. [IPE+22] ). Our results (see Figure 10 ) indicate that TRAK scores computed from an ensemble of just 100 models are about as effective at estimating brittleness as datamodel scores computed from 50,000 models. Thus, TRAK scores can be a viable (and orders of magnitude faster) alternative to datamodels for estimating prediction brittleness. The number of models used by each attribution method is specified in parentheses, e.g., TRAK (100) indicates that TRAK scores were computed using an ensemble of 100 trained models.",
                "cite_spans": [
                    {
                        "start": 13,
                        "end": 21,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 337,
                        "end": 345,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 737,
                        "end": 745,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 573,
                        "end": 574,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 773,
                        "end": 775,
                        "text": "10",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Estimating prediction brittleness",
                "sec_num": "5.3.1"
            },
            {
                "text": "A useful way to leverage datamodels is to view them as data representations. More specifically, following Ilyas et al. [IPE+22] , for an example of interest z, one can view the datamodel attribution \u03c4 DM (z) as an embedding of z into R n , where n is the size of the training dataset. Analyzing examples in such induced datamodel representation spaces turns out to enable uncovering dataset biases and model-specific subpopulations [IPE+22] . Furthermore, this representation space is not specific to a particular model instance or architecture-it is globally aligned in the sense that for the same example z, the attribution score \u03c4 DM (z) i of a given train example i has a consistent interpretation across different learning pipelines. Shah et al. [SPI+22] leverage the properties of the datamodel representation space to perform model-agnostic learning algorithm comparison (called MODELDIFF): given two learning algorithms, they show how to use datamodels to identify distinguishing features, i.e., features that are used by one learning algorithm but not the other.",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 127,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 432,
                        "end": 440,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 751,
                        "end": 759,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning algorithm comparisons",
                "sec_num": "5.3.2"
            },
            {
                "text": "Once again, motivated by TRAK's good performance on the LDS metric, we investigate whether TRAK scores can substitute for datamodel scores in this context. To this end, we revisit one of the case studies from Shah et al. [SPI+22] -the one that compares image classifiers trained with and without data augmentation, and identifies features that distinguish these two classes of models. When applied to this case study, MODELDIFF computed with TRAK scores recovers similar distinguishing features to the ones originally found by Shah et al. [SPI+22] (using datamodel scores)-see Figure D .6 for more details. Also, employing TRAK scores in place of datamodel scores reduces the total computational cost by a factor of 100, showing, once again, that TRAK can dramatically accelerate downstream tasks that rely on accurate attribution scores.",
                "cite_spans": [
                    {
                        "start": 221,
                        "end": 229,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    },
                    {
                        "start": 539,
                        "end": 547,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 584,
                        "end": 585,
                        "text": "D",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Learning algorithm comparisons",
                "sec_num": "5.3.2"
            },
            {
                "text": "In this section, we highlight and discuss how TRAK connects to prior works on training data attribution, the neural tangent kernel, and kernel approximation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "There is a sizable body of work on data attribution methods. Here we discuss approaches most similar to ours, but we refer the reader back to Section 2 for an overview of prior work on data attribution methods and to [HL22b] for an even more extensive survey.",
                "cite_spans": [
                    {
                        "start": 217,
                        "end": 224,
                        "text": "[HL22b]",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "In the setting of generalized linear models, Wojnowicz et al. [WCZ+16] speed up classical influence estimation (Definition 3.1) by leveraging random projections. Also, Khanna et al. [KKG+19] employ a similar estimator based on the Fisher matrix for data attribution and subset selection. Their experiments are limited though to small neural networks and linear models. Most similarly to our approach, Achille et al. [AGR+21] leverage the linearized model for approximating influence functions (among other applications). However, their approach introduces several changes to the model of interest (such as modifying activations, loss, and regularization) and focuses on finetuning in smaller-scale settings, whereas TRAK can be applied directly to the original model (and at scale).",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 70,
                        "text": "[WCZ+16]",
                        "ref_id": "BIBREF91"
                    },
                    {
                        "start": 182,
                        "end": 190,
                        "text": "[KKG+19]",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 416,
                        "end": 424,
                        "text": "[AGR+21]",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "Similarly to us, prior works also investigate the tradeoffs between scalability and efficacy of data attribution methods. For instance, Jia et al. [JWS+21] study these tradeoffs by proposing new metrics and comparing according to them leave-one-out methods (e.g., influence functions) and Shapley values. They put forth, in particular, a new estimator for Shapley values that is based on approximating the original model with a k-nearest neighbors model over the pre-trained embeddings-this can be viewed as an alternative to working with the linearized model.",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 155,
                        "text": "[JWS+21]",
                        "ref_id": "BIBREF47"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "As discussed in Section 2, a major line of work uses Hessian-based influence functions for data attribution [KL17; KAT+19; BPF21]. In particular, the influence function effectively computes-up to an error that can be bounded-the one-step Newton approximation with respect to the full model parameters [KAT+19] . Recall that TRAK also leverages the one-step Newton approximation in order to estimate leave-one-out influences for logistic regression (see Section 3). However, in contrast to the influence function approach, the Hessian matrix we leverage (the matrix X RX in Definition 3.1) is positive semi-definite as it is computed with respect to the linearized model rather than the original model. As a result, computing TRAK does not require the use of additional regularization (beyond the one implicitly induced by our use of random projections), which is practically necessary in the influence function approach. Prior works also leverage a similar Hessian matrix based on the generalized Gauss-Newton matrix [BNL+22] or the equivalent Fisher information matrix [TBG+21] , which are guaranteed to be positive semi-definite.",
                "cite_spans": [
                    {
                        "start": 301,
                        "end": 309,
                        "text": "[KAT+19]",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 1017,
                        "end": 1025,
                        "text": "[BNL+22]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 1070,
                        "end": 1078,
                        "text": "[TBG+21]",
                        "ref_id": "BIBREF87"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "Neural tangent kernel. The neural tangent kernel (NTK) [JGH18] and its generalizations [YL21] are widely studied as a tool for theoretically analyzing generalization [ADH+19] , optimization [WLL+19] , and robustness [GCL+19] of (overparameterized) neural networks. While these works focus on neural networks in the their large or infinite-width limit, a line of recent works [MLL20; AGR+21; Lon21; ABP22; WHS22; MWY+22; ABS+23; MGF22] studies instead the finite-width empirical NTK (eNTK). Our TRAK estimator is partly motivated by the observation from this line of work that kernel regression with the eNTK provides a good approximation to the original model.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 62,
                        "text": "[JGH18]",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 87,
                        "end": 93,
                        "text": "[YL21]",
                        "ref_id": "BIBREF97"
                    },
                    {
                        "start": 166,
                        "end": 174,
                        "text": "[ADH+19]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 190,
                        "end": 198,
                        "text": "[WLL+19]",
                        "ref_id": "BIBREF93"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "While we leverage the eNTK approximation for data attribution, prior works leveraged the NTK and eNTK for various other applications, such as studying generalization [BHL22] , sample selection for active learning [HZK+22] , model selection [DAR+21] , federated learning [YWK+22] , and fast domain adaptation [MTM+21] . Our reduction to the linear case (Step 1 in Section 3.2) is analogous to the approach of Bachmann et al. [BHL22] that leverages formulas for the leave-one-out error of kernel methods coupled with the NTK approximation to estimate the generalization error. Another related work is that of Zhang and Zhang [ZZ22] , who theoretically characterize the accuracy of the Hessian-based influence function in the NTK regime (i.e., large-width limit).",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 173,
                        "text": "[BHL22]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 213,
                        "end": 221,
                        "text": "[HZK+22]",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 240,
                        "end": 248,
                        "text": "[DAR+21]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 270,
                        "end": 278,
                        "text": "[YWK+22]",
                        "ref_id": "BIBREF98"
                    },
                    {
                        "start": 308,
                        "end": 316,
                        "text": "[MTM+21]",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 424,
                        "end": 431,
                        "text": "[BHL22]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 623,
                        "end": 629,
                        "text": "[ZZ22]",
                        "ref_id": "BIBREF101"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "Finally, although the work on NTK popularized the idea of leveraging gradients as features, similar ideas can be traced back to works on the Fisher kernel and related ideas [ZDS17] .",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 180,
                        "text": "[ZDS17]",
                        "ref_id": "BIBREF99"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "Kernel methods and random projections. Our application of random projections to improve computational efficiency of kernel approximation is a widely used idea in kernel methods [Blu06; RR07] . Aside from computational advantages, this technique can also provide insight into empirical phenomena. For example, Malladi et al. [MWY+22] use the kernel view along with random projections as a lens to explain the efficacy of subspace-based finetuning methods.",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 184,
                        "text": "[Blu06;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 185,
                        "end": 190,
                        "text": "RR07]",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 324,
                        "end": 332,
                        "text": "[MWY+22]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data attribution.",
                "sec_num": null
            },
            {
                "text": "In our work, we formalize the problem of data attribution and introduce a new method, TRAK, that is effective and efficiently scalable. We then demonstrate the usefulness of TRAK in a variety of large-scale settings: image classifiers trained on CIFAR and ImageNet, language models (BERT and mT5), and image-text models (CLIP).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion & Conclusion",
                "sec_num": "7"
            },
            {
                "text": "Still, TRAK is not without limitations: in particular, it requires the model to be differentiable, and its effectiveness also depends on the suitability of the linear approximation. That said, the success of the applying the NTK on language modeling tasks [MWY+22] as well as our own experiments both suggest that this approximation is likely to continue to work for larger models. TRAK presents a unique opportunity to reap the benefits of data attribution in previously untenable domains, such as large generative models. In Appendix G, we further discuss possible avenues for future work.",
                "cite_spans": [
                    {
                        "start": 256,
                        "end": 264,
                        "text": "[MWY+22]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion & Conclusion",
                "sec_num": "7"
            },
            {
                "text": "A.1 Datasets and models CIFAR. We construct the CIFAR-2 dataset as the subset of CIFAR-10 [Kri09] consisting of only the \"cat\" and \"dog\" classes. We initially used CIFAR-2 as the main test bed when designing TRAK, as it is a binary classification task and also smaller in size. On both CIFAR-2 and CIFAR-10, we train a ResNet-9 architecture. 15 For CIFAR-2, we use (max) learning rate 0.4, momentum 0.9, weight decay 5e-4, and train for 100 epochs using a cyclic learning rate schedule with a single peak at epoch 5. For CIFAR-10, we replace the learning rate with 0.5 and train for 24 epochs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Experimental Setup",
                "sec_num": null
            },
            {
                "text": "Our code release includes a notebook16 that can reproduce the CIFAR-2 results end-to-end.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Experimental Setup",
                "sec_num": null
            },
            {
                "text": "ImageNet. We use the full 1000-class ImageNet dataset and train a modified ResNet-18 architecture. Models are trained from scratch for 15 epochs, cyclic learning rate with peak at epoch 2 and initial learning rate 5.2, momentum 0.8, weight decay 4e-5, and label smoothing 0.05.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Experimental Setup",
                "sec_num": null
            },
            {
                "text": "We finetune a pre-trained BERT model (bert-base-cased17 ) on the QNLI (Questionanswering Natural Language Inference) task from the GLUE benchmark. We use the default training script18 from HuggingFace with a few modifications: we use SGD (20 epochs, learning rate starting at 1e-3) instead of AdamW, and we remove the last tanh non-linearity before the classification layer. Removing the last non-linearity prevents the model outputs in saturating, resulting in higher LDS. (That said, we find that TRAK scores can be still computed on the models with non-linearity; this was only for improving evaluation.) We restrict the training set to 50,000 examples, approximately half of the full training set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "QNLI.",
                "sec_num": null
            },
            {
                "text": "We use an open-source implementation 19 of CLIP. The model uses a ResNet-50 for the image encoder and a Transformer for the text encoder (for captions). We train for 100 epochs using the Adam optimizer with batch size 600, a cosine learning rate schedule with starting learning rate 0.001, weight decay 0.1, and momentum 0.9. All images are resized to a resolution of 224 \u00d7 224. We use random resize crop, random horizontal flip, and Gaussian blur as data augmentations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CLIP on MS COCO.",
                "sec_num": null
            },
            {
                "text": "In the counterfactual evaluation, we consider a normalized notion of cosine similarity, r = r/(r 95r 5 ), where r is the raw correlation between image and caption embeddings and r \u03b1 is the \u03b1-percentile of image-caption similarities across the entire dataset. Results remain similar with other choices of metric.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CLIP on MS COCO.",
                "sec_num": null
            },
            {
                "text": "Fact tracing mT5 on FTRACE-TREX. We follow the setup exactly as in Akyurek et al. [ABL+22] as we describe in Section 5.2, other than using a smaller architecture (mt5-small). See Appendix F for more details.",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 90,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CLIP on MS COCO.",
                "sec_num": null
            },
            {
                "text": "We train the standard ResNet-18 architecture on the above dataset, either using standard data augmentation (random resized cropping and random horizontal flips) or with no data augmentation (only center cropping, same as used on when evaluating). The goal of the case study from Shah et al. [SPI+22] is to distinguish the above two learning algorithms in terms of the feature priors of the resulting trained models. To run MODELDIFF, follow the setup in Shah et al. [SPI+22] exactly; we refer to the work for more details of the case study and implementation details.",
                "cite_spans": [
                    {
                        "start": 291,
                        "end": 299,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    },
                    {
                        "start": 466,
                        "end": 474,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MODELDIFF on LIVING17. The LIVING17 dataset [STM21] is an image classification dataset derived from the ImageNet dataset and consists of 17 classes, each comprised of four original ImageNet classes.",
                "sec_num": null
            },
            {
                "text": "TRAK only has two hyperparmeters: the projection dimension k and the number of models M. The following hyperparameters were used unless specified otherwise: ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 TRAK hyperparameters",
                "sec_num": null
            },
            {
                "text": "Dataset",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 TRAK hyperparameters",
                "sec_num": null
            },
            {
                "text": "An optional hyperparameter is needed if we use soft-thresholding (Step 5).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft-thresholding.",
                "sec_num": null
            },
            {
                "text": "Among the four tasks we evaluate the LDS on, we find that soft-thresholding is only helpful for the non-binary classification tasks (i.e., CIFAR-10 and ImageNet, but not CIFAR-2 and QNLI); intuitively, this may be due to the fact that the underlying model output function depends on fewer examples (i.e., the attribution vector is sparser) when there are more classes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft-thresholding.",
                "sec_num": null
            },
            {
                "text": "For both CIFAR-10 and ImageNet, we use a single sparsity threshold-i.e., for each test example, we choose the soft-thresholding parameter \u03bb s.t. the resulting TRAK score vector has exactly k nonzero entries, and use the same k for all test examples. To choose k, for CIFAR-10 we cross-validate using the same M models that we used to compute TRAK scores, when M \u2265 20; in other words, we avoid \"cheating\" by using additional models for cross-validation. For ImageNet, we simply choose k = 1000 since there are on average 1,300 training examples per class.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft-thresholding.",
                "sec_num": null
            },
            {
                "text": "We provide details on baselines used in our evaluation in Section 4. Though most of the existing approximation-based methods only use a single model checkpoint in their original formulation, we average the methods over multiple independent checkpoints to help increase its performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.3 Baselines",
                "sec_num": null
            },
            {
                "text": "\u03c4(z j ) i = \u2207L(z j ; \u03b8 ) H -1 \u03b8 \u2207L(z i ; \u03b8 ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "where H \u03b8 is the empirical Hessian w.r.t. the training set. We use an existing PyTorch implementation20 that uses the stochastic approximation of inverse-Hessian-vector products using the LISSA [ABH17] algorithm as done in Koh and Liang [KL17] . As in the original work, we compute the gradients only with respect to the last linear layer; using additional layers caused the inversion algorithm to either diverge or to run out of memory. For hyperparameters, we use similar values as done in prior work; we use r = 1, d = 5000, and damping factor of 0.01. We find that additional repeats (r, the number of independent trials to average each iHvp estimate) does not help, while increasing the depth (d, the number of iterations used by LISSA) helps significantly.",
                "cite_spans": [
                    {
                        "start": 237,
                        "end": 243,
                        "text": "[KL17]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "Influence functions based on the Arnoldi iteration. This variant of influence functions from Schioppa et al. [SZV+22] is based on approximating the top eigenspace of the Hessian using the Arnoldi iteration [Arn51] . We use the original implementation in JAX. 21 We normalize the gradients as recommended in the original paper. While much faster than the original formulation in Koh and Liang [KL17] , we find that the attribution scores not very predictive (according to the LDS).",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 117,
                        "text": "[SZV+22]",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 206,
                        "end": 213,
                        "text": "[Arn51]",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 392,
                        "end": 398,
                        "text": "[KL17]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "TracIn. We use the TracInCP estimator from [PLS+20] , defined as",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 51,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "\u03c4(z j ) i = T \u2211 t=1 \u03b7 t \u2022 \u2207L(z j ; \u03b8 t ) \u2022 \u2207L(z i ; \u03b8 t ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "where \u03b8 t is the checkpoint from the epoch t and \u03b7 t is the corresponding learning rate \u03b7 t . We also average over trajectories of multiple independently trained models, which increases its performance. We approximate the dot products using random projections of dimensions 500-1000 as we do for TRAK, as the estimator is intractable otherwise. We found that increasing the number of samples (epochs) from the training trajectory does not lead to much improvement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "Gradient Aggregated Similarity (GAS). This is a \"renormalized\" version of the TracInCP [HL22b] based on using the cosine similarity instead of raw dot products. In general, its performance is indistinguishable from that of TracIn.",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 94,
                        "text": "[HL22b]",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "Representation similarity. We use the signed 2 dot product in representation space (feature embeddings of the penultimate layer), where the sign indicates whether the labels match. We also experimented with cosine similarity but the resulting performance was similar.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "Empirical influences. We use the subsampling-based approximation to leave-one-out influences as used by [FZ20] , which is a difference-in-means estimator given by",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 110,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "\u03c4(z j ) i = E S z i f (z j ; \u03b8) -E S z i f (z j ; \u03b8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "where the first (second) expectation is over training subsets that include (exclude) example z i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "Datamodels. We use the 1 -regularized regression-based estimators from Ilyas et al. [IPE+22] , using up to 60,000 models for CIFAR-2 and 300,000 models for CIFAR-10 (trained on different random 50% subsets of the full training set).",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 92,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Influence functions. The standard Hessian-based influence functions yield the attribution scores",
                "sec_num": null
            },
            {
                "text": "For all of our experiments, we use NVIDIA A100 GPUs each with 40GB of memory and 12 CPU cores. We evaluate the computational cost of attribution methods using two metrics, total wall-time and the total number of trained models used; see Section 4 for motivation behind these metrics. For most attribution methods, one or more of the following components dominate their total runtime:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "\u2022 TRAIN_TIME: the time to train one model (from scratch)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "\u2022 GRAD_TIME: the time to compute gradients of one model (including computing random projections) for the entire dataset under consideration (both train and test sets). This time may vary depending on size of the projection dimension, but our fast implementation (Appendix B) can handle dimensions of up to 80,000 without much increase in runtime.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "The total compute time for each method was approximated as follows, where M is the number of models used:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "\u2022 TRAK: M \u00d7 (TRAIN_TIME + GRAD_TIME), as we have to compute gradients for each of the trained models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "\u2022 Datamodel [IPE+22] and Empirical Influence [FZ20]: M \u00d7 TRAIN_TIME. The additional cost of estimating datamodels or influences from the trained models (which simply involves solving a linear system) is negligible compared to the cost of training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "\u2022 LISSA based influence functions [KL17]: These approaches are costly because they use thousands of Hessian-vector product iterations to approximate a single inverse-Hessianvector product (which is needed for each target example). Hence, we computed these attribution scores for a much smaller sample of validation set (50 to 100). We measured the empirical runtime on this small sample and extrapolated to the size of the entire (test) dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.4 Hardware and wall-time measurements",
                "sec_num": null
            },
            {
                "text": "We ran the authors' original code22 on CIFAR models of the same architecture (after translating them to JAX) and measured the runtime.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Influence function based on the Arnoldi iteration [SZV+22]:",
                "sec_num": null
            },
            {
                "text": "\u2022 TracIn [PLS+20] and GAS [HL22a]: M \u00d7 (TRAIN_TIME + GRAD_TIME \u00d7 T), where T is the number of checkpoints used per model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Influence function based on the Arnoldi iteration [SZV+22]:",
                "sec_num": null
            },
            {
                "text": "We release an easy-to-use library, trak,23 , which computes TRAK scores using Algorithm 1. Computing TRAK involves the following four steps: (i) training models (or alternatively, acquiring checkpoints), (ii) computing gradients, (iii) projecting gradients with a random projection matrix (Rademacher or Gaussian), and (iv) aggregating into the final estimator (Equation (15)).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B TRAK implementation",
                "sec_num": null
            },
            {
                "text": "Step (i) is handled by the user, while steps (ii)-(iv) are handled automatically by our library.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B TRAK implementation",
                "sec_num": null
            },
            {
                "text": "Step (ii) is implemented using the functorch library to compute per-example gradients. Step (iii) is either implemented using matrix multiplication on GPU or by a faster custom CUDA kernel, which is described below. Step (iv) just involves a few simple matrix operations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B TRAK implementation",
                "sec_num": null
            },
            {
                "text": "One of the most costly operation of TRAK is the random projection of the gradients onto a smaller, more manageable vector space. While CPUs are not equipped to handle this task on large models (e.g., LLMs) at sufficient speed, at least on paper, GPUs have more than enough raw compute.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Fast random projections on GPU",
                "sec_num": null
            },
            {
                "text": "In practice, however, challenges arise. First, storing the projection matrix entirely is highly impractical. For example, a matrix for a model with 300 million weights and an output of 1024 dimensions would require in excess of 1TB of storage. One solution is to generate the projection in blocks (across the output dimension). This solution is possible (and offered in our implementation) but is still radically inefficient. Indeed, even if the generation of the matrix is done by block it still has to be read and written once onto the GPU RAM. This severely limits the performance as memory throughput becomes the bottleneck.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Fast random projections on GPU",
                "sec_num": null
            },
            {
                "text": "Our approach. Our solution is to generate the coefficients of the projection as needed (in some situations more than once) and never store them. As a result, the bandwidth of the RAM is solely used to retrieve the values of the gradients and write the results at the end. This forces us to use pseudo-randomness but this is actually preferrable since a true random matrix would make experiments impossible to reproduce exactly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Fast random projections on GPU",
                "sec_num": null
            },
            {
                "text": "Our implementation is written in C++/CUDA and targets NVIDIA GPUs of compute capability above or equal 7.0 (V100 and newer). It supports (and achieve better performance) batches of multiple inputs, and either normally distributed coefficients or -1, 1 with equal probabilities. Implementation details. We decompose the input vectors into K blocks, where each block is projected independently to increase parallelism. The final result is obtained by summing each partial projection. To reduce memory usage, we keep K to roughly 100.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Fast random projections on GPU",
                "sec_num": null
            },
            {
                "text": "We further increase parallelism by spawning a thread for each entry of the output blocks, but this comes at the cost of reading the input multiple times. To mitigate this issue, we use Shared Memory offered by GPUs to share and reduce the frequency of data being pulled from global memory. We also use Shared Memory to reduce the cost of generating random coefficients, which can be reused for all the inputs of a batch.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Fast random projections on GPU",
                "sec_num": null
            },
            {
                "text": "Finally, we take advantage of Tensor Cores to maximize throughput and efficiency, as they were designed to excel at matrix multiplications. These interventions yield a fast and power-efficient implementation of random projection. On our hardware, we achieved speed-ups in excess of 200x compared to our \"block-by-block\" strategy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Fast random projections on GPU",
                "sec_num": null
            },
            {
                "text": "The key formula we use in TRAK is the estimate for the leave-one-out (LOO) influence in logistic regression (Definition 3.1). Here, we reproduce the derivation of this estimate from Pregibon [Pre81] then extend it to incorporate example-dependent bias terms.",
                "cite_spans": [
                    {
                        "start": 191,
                        "end": 198,
                        "text": "[Pre81]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "Convergence condition for logistic regression. Assume that we optimized the logistic regression instance via Newton-Raphson, i.e., the parameters are iteratively updated as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 t+1 \u2190 \u03b8 t + H -1 \u03b8 t \u2207 \u03b8 L( \u03b8 t )",
                        "eq_num": "(23)"
                    }
                ],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "where H \u03b8 is the Hessian and \u2207 \u03b8 L( \u03b8) is the gradient associated with the total training loss L( \u03b8) = \u2211 z i \u2208S L(z i ; \u03b8). In the case of logistic regression, the above update is given by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "\u03b8 t+1 \u2190 \u03b8 t + (X RX) -1 X q (24)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "where q = 1p is the vector of the probabilities for the incorrect class evaluated at \u03b8 t and R = diag( p(1p) is the corresponding matrix. Upon convergence, the final parameters \u03b8 satisfy the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(X RX) -1 X q = 0 (",
                        "eq_num": "25"
                    }
                ],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "where q is the incorrect-class probability vector corresponding to \u03b8 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "The one-step Newton approximation. We estimate the counterfactual parameters \u03b8 -i that would have resulted from training on the same training set excluding example i by simply taking a single Newton step starting from the same global optimum \u03b8 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 -i = \u03b8 + (X -i R -i X -i ) -1 X -i q -i ,",
                        "eq_num": "(26)"
                    }
                ],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "where the subscript -i denotes the corresponding matrices and vectors without the i-th training example. Rearranging and using (25),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "\u03b8 -\u03b8 -i = -(X -i R -i X -i ) -1 X -i q -i \u03b8 -\u03b8 -i = (X RX) -1 X q -(X -i R -i X -i ) -1 X -i q -i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "Using the Sherman-Morrison formula to simplify above, 24 we have",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "\u03b8 -\u03b8 -i = (X RX) -1 x i 1 -x i (X RX) -1 x i \u2022 p i (1 -p i ) q i = (X RX) -1 x i 1 -x i (X RX) -1 x i \u2022 p i (1 -p i ) (1 -p i ) (27)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "The above formula estimates the change in the parameter vector itself. To estimate the change in prediction at a given example x, we take the inner product of the above expression with vector x to get the formula in Definition 3.1. The approximation here is in assuming the updates converge in one step. Prior works [KAT+19] quantify the fidelity of such approximation under some assumptions. The effectiveness of TRAK across a variety of settings suggests that the approximation is accurate in regimes that arise in practice.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "Incorporating bias terms. The above derivation is commonly done for the case of standard logistic regression, but it also directly extends to the case where the individual predictions incorporate example-dependent bias terms b i that are independent of \u03b8. In particular, note that the likelihood function after linearization in Step 1 is given by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(z i ; \u03b8) = \u03c3(-y i \u2022 (\u2207 \u03b8 f (z i ; \u03b8 ) \u2022 \u03b8 + b i ))",
                        "eq_num": "(28)"
                    }
                ],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "where \u03c3(\u2022) is the sigmoid function. Because the Hessian and the gradients of the training loss only depend on \u03b8 through p(z i ; \u03b8), and because b i 's are independent of \u03b8, the computation going from Equation (23) to Equation ( 24) is not affected. The rest the derivation also remains identical as the bias terms are already incorporated into p and q .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "Generalization to other settings. While our derivations in this paper focus on the case of logistic regression, more generally, TRAK can be easily adapted to any choice of model output function as long as the training loss L is a convex function of the model output f . The corresponding entries in the Q = diag(1p i ) matrix in Definition 3.1 is then replaced by \u2202L/\u2202 f (z i ). The R matrix and the leverage scores also change accordingly, though we do not include them in our estimator (that said, including them may improve the estimator in settings beyond classification).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "However, in general one needs care in choosing an appropriate model output function in order to maximize the performance on the linear datamodeling prediction task. If the chosen model output is not well approximated by a linear function of training examples, then that puts an upper bound on the predictive performance of any attribution method in our framework. We discuss appropriate choices of model output functions further in Appendix C.4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.1 The one-step Newton approximation for leave-one-out influence",
                "sec_num": null
            },
            {
                "text": "In Step 2 of TRAK, we use random projections to reduce the dimension of the gradient vectors. Here, we justify this approximation when our model is trained via gradient descent. Similar analysis has been used prior, e.g., by Malladi et al. [MWY+22] .",
                "cite_spans": [
                    {
                        "start": 240,
                        "end": 248,
                        "text": "[MWY+22]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.2 Random projections preserve gradient flow",
                "sec_num": null
            },
            {
                "text": "In the limit of small learning rate, the time-evolution of model output f (z; \u03b8) under gradient descent (or gradient flow) is captured by the following differential equation [JGH18] :",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 181,
                        "text": "[JGH18]",
                        "ref_id": "BIBREF45"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.2 Random projections preserve gradient flow",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d f (z; \u03b8) dt = \u2211 i \u2202L(z i ; \u03b8) \u2202 f (z i ; \u03b8) \u2022 (\u2207 f (z i ; \u03b8) \u2022 \u2207 f (z; \u03b8)) \u2248 \u2211 i \u2202L(z i ; \u03b8) \u2202 f (z i ; \u03b8) \u2022 (g i \u2022 g(z))",
                        "eq_num": "(29)"
                    }
                ],
                "section": "C.2 Random projections preserve gradient flow",
                "sec_num": null
            },
            {
                "text": "where g i and g(z) are the gradients of the final model corresponding to examples z i and z as before.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.2 Random projections preserve gradient flow",
                "sec_num": null
            },
            {
                "text": "The approximation is due to assuming that the gradients do not change over time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.2 Random projections preserve gradient flow",
                "sec_num": null
            },
            {
                "text": "If we treat the outputs { f (z i ; \u03b8)} i as time-varying variables, then their time evolution is entirely described by the above system of differential equations (one for each i, replacing z with z i above). Importantly, the above equations only depend on the gradients through their inner products. Hence, as long as we preserve the inner products to sufficient accuracy, the resulting system has approximately the same evolution as the original one. This justifies replacing the gradient features with their random projections.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.2 Random projections preserve gradient flow",
                "sec_num": null
            },
            {
                "text": "In Step 4 of our algorithm, we ensemble the attribution scores over multiple models. As we investigate in Appendix E.2, this significantly improves TRAK's performance. An important design choice is training each model on a different random subset of the training set. This choice is motivated by the following connection between TRAK scores and empirical influences [FZ20] . Recall that we designed TRAK to optimize the linear datamodeling score. As we discuss in Section 2, datamodels can be viewed as an \"oracle\" for optimizing the same metric. Further, as Ilyas et al. [IPE+22] observes, datamodels can be viewed as a regularized version of empirical influences [FZ20] , which are defined as a difference-in-means estimator,",
                "cite_spans": [
                    {
                        "start": 366,
                        "end": 372,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 572,
                        "end": 580,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 665,
                        "end": 671,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c4(z j ) i = E S \u223cD [ f (z j ; \u03b8 (S ))|z i \u2208 S ] -E S \u223cD [ f (z j ; \u03b8 (S ))|z i \u2208 S ] (",
                        "eq_num": "30"
                    }
                ],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": "where D is the uniform distribution over \u03b1-fraction subsets of training set S. Assuming the expectation over \u03b1-fraction subsets is identical to that over subsets of one additional element, we can rearrange the above expression as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c4(z j ) i = E S \u223cD [ f (z j ; \u03b8 (S \u222a {z i })) -f (z j ; \u03b8 (S ))].",
                        "eq_num": "(31)"
                    }
                ],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": "The above expression is simply the expectation of leave-one-out influence over different random subsets. As the estimate from step 3 of our algorithm is specific to a single training set, we need to average over different subsets in order to approximate the above quantity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": "In principle, the estimates computed from \u03b8 (S ) only apply to the training examples included in the subset S , since the underlying formula (Definition 3.1) concerns examples that were included for the original converged parameter \u03b8 . Hence, when averaging over the models, each model should only update the TRAK scores corresponding to examples in S . However, we found that the estimates are marginally better when we update the estimates for the entire training set S (i.e., even those that were not trained on).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.3 Subsampling the training set",
                "sec_num": null
            },
            {
                "text": "A possible concern is that we overfit to a particular regime of \u03b1 used in evaluating with the LDS. In Figure D .1, we evaluate TRAK scores (computing using \u03b1 = 0.5) in other regimes and find that they continue to be highly predictive (though with some degradation in correlation). More generally, our various counterfactual evaluations using the full training set (CIFAR-10 brittleness estimates in Figure 10 , the CLIP counterfactuals in Figure 8 ) indicate that TRAK scores remain predictive near the \u03b1 = 1 regime.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 109,
                        "end": 110,
                        "text": "D",
                        "ref_id": null
                    },
                    {
                        "start": 406,
                        "end": 408,
                        "text": "10",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 446,
                        "end": 447,
                        "text": "8",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Generalization across different \u03b1's.",
                "sec_num": null
            },
            {
                "text": "We study linear predictors derived from attribution scores, as linearity is a latent assumption for many popular attribution methods. Linearity also motivates our choices of model output functions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.4 Linearity and model output function",
                "sec_num": null
            },
            {
                "text": "Latent assumption of linearity. Our evaluation of data attribution methods cast them as linear predictors. While not always immediate, linearity is a latent assumption behind most of the prior methods that we evaluate in this paper. Datamodels and Shapley values satisfy additivity by construction [GZ19; JDW+19]. The approach based on influence functions [KL17; KAT+19] typically uses the sum of LOO influences to estimate influences for groups of examples. Similarly, empirical (or subsampled) influences [FZ20] also correspond to a first-order Taylor approximation of the model output function. The TracIn estimator also implicitly assumes linearity [PLS+20] .",
                "cite_spans": [
                    {
                        "start": 507,
                        "end": 513,
                        "text": "[FZ20]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 653,
                        "end": 661,
                        "text": "[PLS+20]",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.4 Linearity and model output function",
                "sec_num": null
            },
            {
                "text": "That said, others works also incorporate additional corrections beyond the first order linear terms [BYF19] and find the resulting predictions better approximate the true influences.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 107,
                        "text": "[BYF19]",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.4 Linearity and model output function",
                "sec_num": null
            },
            {
                "text": "Choice of model output function f . In our experiments, we choose the model output function suitable for the task at hand: for classification and language modeling, we used a notion of margin that is equivalent to the logit function, while for CLIP, we used a similar one based on the CLIP loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.4 Linearity and model output function",
                "sec_num": null
            },
            {
                "text": "Our particular choice of the logit function (log p/(1p)) in the multi-class classification case was motivated by theoretical [SGB+23] and empirical [IPE+22] observations from prior works. In particular, this choice of model output function is well approximated by linear datamodels, both in practice and in theory. A slightly different definition of margin used in Ilyas et al. [IPE+22] -where the margin is computed as the logit for the correct class minus the second highest class-can also be as an approximation to the one used here.",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 133,
                        "text": "[SGB+23]",
                        "ref_id": "BIBREF81"
                    },
                    {
                        "start": 148,
                        "end": 156,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 378,
                        "end": 386,
                        "text": "[IPE+22]",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.4 Linearity and model output function",
                "sec_num": null
            },
            {
                "text": "More generally, choosing a good f boils down to linearizing (w.r.t. \u03b8) as much of the model output as possible, but not too much. On one extreme, choosing f (z) = z (i.e., linearizing nothing, as there is no dependence on \u03b8) means that the one-step Newton approximation has to capture all of the non-linearity in both the model and the dependence of L on f ; this is essentially the same approximation used by the Hessian-based influence function. On the other extreme, if we choose f = L, we linearize too much, which does not work well as L in general is highly non-linear as a function of f .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C.4 Linearity and model output function",
                "sec_num": null
            },
            {
                "text": "Generalization across \u03b1's. In Figure D .1 left, we compare the linear datamodeling scores (LDS) evaluated on \u03b1 = 0.5 sub-sampled training sets to those evaluated on \u03b1 = 0.75. (The numbers are overall lower as these are evaluated on data where only one model was trained on each subset,instead of averaging over 5 models; hence, there is more noise in the data.) As we observe, the LDS scores on different \u03b1's are highly correlated, suggesting that TRAK scores computed on a single \u03b1 generalize well. We quantify various data attribution methods in terms of both their predictiveness-as measured by the linear datamodeling score-as well as their computational efficiency-as measured by either the total computation time (wall-time measured in minutes on a single A100 GPU; see Appendix A.4 for details) or the number of trained models used to compute the attribution scores. The errors indicate 95% bootstrap confidence intervals. Sampling-based methods (datamodels and empirical influences) can outperform TRAK when allowed to use more computation, but this leads to a significant increase in computational cost.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 37,
                        "end": 38,
                        "text": "D",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "D.1 Correlation distribution",
                "sec_num": null
            },
            {
                "text": "We display more examples identified with TRAK scores in ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D.3 TRAK examples",
                "sec_num": null
            },
            {
                "text": "A: There were 46,917 households, out of which 7,835 (16.7%) had children under the age of 18 living in them, 13,092 (27.9%) were opposite-sex married couples living together, 3,510 (7.5%) had a female householder with no husband present, 1,327 (2.8%) had a male householder with no wife present. (Yes) Q: Roughly how many same-sex couples were there? A: There were 46,917 households, out of which 7,835 (16.7%) had children under the age of 18 living in them, 13,092 (27.9%) were oppositesex married couples living together, 3,510 (7.5%) had a female householder with no husband present, 1,327 (2.8%) had a male householder with no wife present. (3) counterfactually testing the inferred feature associated with the subpopulation. Shah et al. [SPI+22] find that models trained with data augmentation latch onto the presence of spider webs as a spurious correlation to predict the class spider. Here, we recover their result by using TRAK scores instead of datamodel scores in step (1); doing so reduces the computational cost of MODELDIFF by 100x.",
                "cite_spans": [
                    {
                        "start": 743,
                        "end": 751,
                        "text": "[SPI+22]",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q: What percent of household have children under 18?",
                "sec_num": null
            },
            {
                "text": "We perform a number of ablation studies to understand how different components of TRAK affect its performance. Specifically, we study the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "\u2022 The dimension of the random projection, k. Section 3.2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "\u2022 The number of models ensembled, M. Section 3.2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "\u2022 Proxies for ensembles to further improve TRAK's computational efficiency.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "\u2022 The role of different terms in the influence estimation formula (Equation ( 17)).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "\u2022 Alternative choice of the kernel (using last layer representations).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "\u2022 Alternative methods of ensembling over models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "As in Section 4, we evaluate the linear datamodeling score (LDS) on models trained on the CIFAR-2, CIFAR-10, and QNLI datasets. Note that the LDS is in some cases lower than the counterparts in Figure 2 as we use a smaller projected dimension (k) and do not use soft-thresholding in these experiments.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 201,
                        "end": 202,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E Ablation Studies",
                "sec_num": null
            },
            {
                "text": "Recall that when we compute TRAK we reduce the dimensionality of the gradient features using random projections (Step 2 of Section 3.2). Intuitively, as the resulting dimension k increases, the corresponding projection better preserves inner products, but is also more expensive to compute. We now study how the choice of the projection dimension k affects TRAK's attribution performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Dimension of the random projection",
                "sec_num": null
            },
            {
                "text": "Figure E .1 (Left) shows that as we increase the dimension, the LDS initially increases as expected; random projections to a higher dimension preserve the inner product more accurately, providing a better approximation of the gradient features. However, beyond a certain point, increasing projection dimension decreases the LDS. We hypothesize that using random projections to a lower dimension has a regularizing effect that competes with the increase in approximation error. 25Finally, the dimension at which LDS peaks increases as we increase the number of models M used to compute TRAK.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "E",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E.1 Dimension of the random projection",
                "sec_num": null
            },
            {
                "text": "An important component of computing TRAK is ensembling over multiple independently trained models (Step 4 in Section 3.2). In our experiments, we average TRAK's attribution scores over ensembles of size ranging from 1 to 100. Here, we quantify the importance of this procedure on TRAK's performance. shows that TRAK enjoys a significantly better data attribution performance with more models. That said, even without ensembling (i.e., using a single model), TRAK still performs better (e.g., LDS of 0.096 on CIFAR-2) than all prior gradient-based methods that we evaluate. 0 2,000 4,000 6,000 8,000 Each line corresponds to a different value of M \u2208 {10, 20, ..., 100} (the number of models TRAK is averaged over); darker lines correspond to higher M. As we increase the projected dimension, the LDS initially increases. However, beyond a certain dimension, the LDS begins to decrease. The \"optimal\" dimension (i.e., the peak in the above graph) increases with higher M. Right: The impact of ensembling more models on TRAK's performance on CIFAR-2. The performance of TRAK as a function of the number of models used in the ensembling step. TRAK scores are computed with random projections of dimension k = 4000.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Number of models used in the ensemble",
                "sec_num": null
            },
            {
                "text": "In Appendix E.2 we saw that ensembling leads to significantly higher efficacy (in terms of LDS). In many settings, however, it is computationally expensive to train several independent models to make an ensemble. Hence, we study whether there is a cheaper alternative to training multiple independent models that does not significantly sacrifice efficacy. To this end, we explore two avenues of approximating the full ensembling step while dramatically reducing the time required for model training. In particular, we investigate:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.3 Proxies for model ensembles in compute-constrained settings",
                "sec_num": null
            },
            {
                "text": "1. using multiple checkpoints from each training trajectory;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.3 Proxies for model ensembles in compute-constrained settings",
                "sec_num": null
            },
            {
                "text": "2. using checkpoints from early training, long before the model has converged.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.3 Proxies for model ensembles in compute-constrained settings",
                "sec_num": null
            },
            {
                "text": "Multiple checkpoints from each training trajectory. We compute TRAK scores using a fixed number of checkpoints, but while varying the number of independently-trained models. For example, for 100 checkpoints, we can use the final checkpoints from 100 independently-trained models, the last two checkpoints from 50 independently-trained models, etc. We observe (see Table E .3) that TRAK achieves comparable LDS when we use last T checkpoints along the trajectory of the same models as a proxy for independently-trained models in the ensembling step.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 370,
                        "end": 371,
                        "text": "E",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "E.3 Proxies for model ensembles in compute-constrained settings",
                "sec_num": null
            },
            {
                "text": "Using checkpoints from early training. We explore whether each of the models in the ensemble has to be fully trained to convergence. In particular, we study the effect of using checkpoints from early epochs on the LDS. While TRAK benefits from using later-epoch gradient features, it maintains its efficacy even when we use gradient features from training runs long before reaching convergence (see ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.3 Proxies for model ensembles in compute-constrained settings",
                "sec_num": null
            },
            {
                "text": "The TRAK estimator (Equation ( 17)) has a number of different components. We label each component (of the single model estimator) as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "\u03c4(z) i = \u03c6(z) reweighting (\u03a6 R\u03a6) -1 \u03c6(z i ) \u2022 loss gradient 1 1 + e f (z i ) 1 - h i leverage score",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "We ablate each of the terms above and re-evaluate the resulting variant of TRAK on CIFAR-2. Our results in Table E .4 indicate the following:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 113,
                        "end": 114,
                        "text": "E",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "\u2022 Reweighting: Experiment 6 shows that this matrix is a critical part of TRAK's performance. Conceptually, this matrix distinguishes our estimator from prior gradient based similarity metrics such as TracIn.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "\u2022 Diagonal term R: The full reweighting matrix includes a diagonal term R. Although it is theoretically motivated by Definition 3.1, including this term results in lower LDS, so we do not include it (Experiments 2,4).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "\u2022 Loss gradient: This term corresponds to the Q matrix (Equation ( 14)) and encodes the probability of the incorrect class, 1p i ; the name is based on the derivation in Appendix C.1, where this term corresponds to scalar associated with the gradient of the loss. Intuitively, this term helps reweight training examples based on on models' confidence on them. Experiment 5 shows that this term improves the performance substantially.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "\u2022 Leverage score: This term does not impact the LDS meaningfully, so we do not include it (Experiments 1,2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "\u2022 Averaging \"out\" vs \"in\": Averaging the estimator and the loss gradient term separately, then re-scaling by the average loss gradient results in higher LDS (Experiment 3). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 Role of different terms.",
                "sec_num": null
            },
            {
                "text": "To understand how the choice of the kernel impacts the performance of TRAK, we also compute a version of TRAK using feature representations of the penultimate layer in place of the projected gradients. This choice is equivalent to restricting the gradient features to those of the last linear layer. As Table E .5 shows, this method significantly improves on all existing baselines based on gradient approximations,26 but still underperforms significantly relative to TRAK. This gap suggests that the eNTK is capturing additional information that is not captured by penultimate layer representations. Moreover, the larger gap on CIFAR-10 compared to CIFAR-2 and QNLI (both of which are binary classificaiton tasks) hints that the gap will only widen on more complex tasks. We note that TRAK applied only to the last layer is almost equivalent to the influence function approximation. Indeed, they perform similarly (e.g., the influence function approximation also achieves a LDS of 0.19 on QNLI). We compare TRAK computed using the eNTK (i.e., using features derived from full gradients) with TRAK computed using the kernel derived from last layer feature representations. The attribution scores are ensembled over M = 100 models.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 309,
                        "end": 310,
                        "text": "E",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "E.5 Choice of the kernel",
                "sec_num": null
            },
            {
                "text": "There are different ways to ensemble a kernel method given multiple kernels {K i } i : (i) we can average the Gram matrices corresponding to each kernel first and then predict using the averaged",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.6 Ensembling vs. Averaging the eNTK",
                "sec_num": null
            },
            {
                "text": "The training set of FTRACE-TREX is sourced from the TREX dataset [EVR+18] , with each training example excerpted from a DBPedia abstract [HLA+13] and annotated with a list of facts it expresses. 27 The test set of FTRACE-TREX is sourced from the LAMA dataset [PRR+19], and each test example is a sentence that expresses a single fact-every training example that expresses the same fact is called a \"proponent\" of this test example. Now, given a test example expressing some fact, the goal of fact tracing (as defined by the FTRACE-TREX benchmark) is to correctly identify the corresponding proponents from the training set. More precisely, Akyurek et al. [ABL+22] propose the following evaluation methodology, which we follow exactly (with the exception that, due to computational constraints, we use a smaller 300M-parameter mt5-small model instead of the 580M-parameter mt5-base). We first finetune the pretrained language model [RSR+20] on the training set of FTRACE-TREX. Then, we iterate through the FTRACE-TREX test set and find the examples on which the pre-trained model is incorrect and the finetuned model is correct,28 which Akyurek et al. [ABL+22] refer to as the \"novel facts\" learned by the model after finetuning. For each novel fact identified, we collect a set of candidate training examples, comprising all proponents as well as 300 \"distractors\" from the training set. Akyurek et al. [ABL+22] propose to evaluate different attribution methods based on how well they identify the ground-truth proponents among each candidate set.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 73,
                        "text": "[EVR+18]",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 137,
                        "end": 145,
                        "text": "[HLA+13]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 655,
                        "end": 663,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 931,
                        "end": 939,
                        "text": "[RSR+20]",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 1151,
                        "end": 1159,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1403,
                        "end": 1411,
                        "text": "[ABL+22]",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Fact Tracing F.1 The FTRACE-TREX Dataset",
                "sec_num": null
            },
            {
                "text": "Concretely, given an attribution method \u03c4(\u2022), we compute attribution scores \u03c4(z) for each of the novel facts in the test set. For each novel fact, we sort the corresponding candidate examples by their score \u03c4(z) i . Finally, we compute the mean reciprocal rank (MRR), a standard information retrieval metric, of ground-truth proponents across the set of novel facts, defined as MRR = \u2211 z\u2208 novel facts 1 min i \u2208 proponents(z) rank(\u03c4(z), i) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Fact Tracing F.1 The FTRACE-TREX Dataset",
                "sec_num": null
            },
            {
                "text": "We finetune the pre-trained language model using the masked language modeling objective [DCL+19] . In particular, for each training example z i \u2208 [K] L (where K is the vocabulary size and L is the maximum passage length), we mask out a subject or object within the passage. (E.g., a training example \"Paris is the capital of France\" might become an input-label pair [\"__ is the capital of France\", \"Paris\"]). We then treat the language modeling problem as multiple separate K-way classification tasks. Each task corresponds to predicting a single token of the masked-out text, given (as input) the entire passage minus the token being predicted. The loss function is the average cross-entropy loss on this sequence of classification tasks.",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 96,
                        "text": "[DCL+19]",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.2 Fine-tuning details",
                "sec_num": null
            },
            {
                "text": "We make the notion of \"importance\" more precise later (in Definition",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "2.3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In many settings, the non-determinism of training makes this model output function a random variable, but we treat it as deterministic to simplify our notation. We handle non-determinism explicitly in Section",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "3.2.3 Note that this additivity assumption can be explicit or implicit. Shapley values[Sha51] and datamodels[IPE+22], for example, take additivity as an axiom. Meanwhile, attribution methods based on influence functions [HRR+11; KL17; KAT+19] implicitly use a first-order Taylor approximation of the loss function with respect to the vector of training example loss weights, which is precisely equivalent to an additivity assumption.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In practice, we can estimate the LDS with 100-500 models, as the average rank correlation (over a sufficient number of test examples) converges fairly quickly with sample size.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that we focus on logistic regression for simplicity-more generally one can adapt TRAK to any setting where the training loss is convex in the model output; see Appendix C.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that for the special case of binary classifiers, the model output function that we define (i.e., f (z; \u03b8) = f ((x, y); \u03b8)) depends only on the input x, and not on the label y. When we generalize TRAK to more complex losses in Section 3.3, the model output function will involve both x and y.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In Appendix C.2 we discuss why preserving inner products suffices to preserve the structure of the logistic regression.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that in our linearization (10), the predicted probability is also a function of the bias terms b i . We can avoid having to compute these bias terms by simply using the predicted probability from the true model (i.e., the neural network) instead of the linearized one.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that the corresponding \"labels\" for this logistic regression are actually identically equal to one-to see this, compare (21) to (18). This does not change the resulting attributions, however, as Definition 3.1 only depends on labels through its dependence on the correct-class probability p * i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For many data attribution methods, such as influence function-based methods or TRAK, there is an extra step of computing per-example gradients through the model of interest. However, this step is generally fully parallelizable, and usually bounded by the time it takes to train a model from scratch.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use the average of cosine similarities between the image embeddings and between the text embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In masked language modeling[RSR+20], the language model is asked to predict the tokens corresponding to a masked-out portion of the input. In FTRACE-TREX, either a subject or object in the abstract is masked out.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that while our finding that BM25 outperforms TracIn matches that of Akyurek et al.[ABL+22], our exact numbers are incomparable due to the mismatch in model classes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "See Appendix F.4 for a detailed account of our experiment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/wbaek/torchskeleton/blob/master/bin/dawnbench/cifar10.py",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/MadryLab/trak/blob/main/examples/cifar2_correlation.ipynb",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://huggingface.co/bert-base-cased",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_ glue.py",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/mlfoundations/open_clip",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/alstonlo/torch-influence",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/google-research/jax-influence",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/google-research/jax-influence",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/MadryLab/trak",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This is used also, for instance, to derive the LOO formulas for standard linear regression.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Indeed, we can view our approach of first projecting features to a lower dimension and then performing linear regression in the compressed feature space, as an instance of compressed linear regression[MM09] and also related to principal components regression[THM17]. These approaches are known to have a regularizing effect, so TRAK may also benefit from that effect.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that as with the eNTK, the use of multiple models here is crucial: only using a single model gives a correlation of 0.006.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "See[ABL+22] for more details on the annotation methodology.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "To decide whether a model is \"correct\" on a given test example, we use MT5 as a conditional generation model. That is, we feed in a masked version of the query, e.g., \"__ is the capital of France,\" and mark the model as \"correct\" if the conditional generation matches the masked word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In particular, recall that in order for a test example to be categorized as a \"novel fact,\" it must be both (a) incorrectly handled by the pre-trained mt5-small model and (b) correctly handled by a finetuned model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://chat.openai.com/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Ekin Akyurek for help installing and using the FTRACE-TREX benchmark.Work supported in part by the NSF grants CNS-1815221 and DMS-2134108, and Open Philanthropy. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001120C0015.Research was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "kernel (i.e., work with K = 1 n \u2211 K i ), (ii) we can average their induced features (with respect to some fixed basis of functions) and use the corresponding kernel, or (iii) we can average the predictions derived from each kernel [ABS+23] . TRAK's algorithm follows the third approach (Step 4).Here we ensemble using the first approach instead (i.e., using the averaged eNTK). We do this by first averaging the Gram matrices corresponding to each models' eNTK, using the Cholesky decomposition to extract features from the averaged Gram matrix (G = LL ), then using resulting features L into the same influence formula (Step 3). We find that computing TRAK with this average eNTK gives a significantly underperforming estimator (LDS of 0.120 on CIFAR-2) than averaging after computing the estimator from each eNTK (LDS of 0.499). This gap suggests that the underlying model is better approximated as an ensemble of kernel predictors rather than a predictor based on a single kernel.",
                "cite_spans": [
                    {
                        "start": 231,
                        "end": 239,
                        "text": "[ABS+23]",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Appendices",
                "sec_num": null
            },
            {
                "text": "To summarize the results of our ablation, TRAK performs best when averaging over a sufficient number of models (though computationally cheaper alternatives also work); gradients computed at later epochs; and random projections to sufficiently high-but not too high-dimension. Using the reweighting matrix in Equation ( 17), as well as deriving the features from the full model gradient are also both critical to TRAK's predictive performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.7 Summary",
                "sec_num": null
            },
            {
                "text": "The model output function we use, more precisely, is given by:In particular, to compute this model output function, we compute the model output function (19) for each one of the V-way classification problems separately, then define our model output function as the sum of these computed outputs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F.3 Computing TRAK for masked language modeling",
                "sec_num": null
            },
            {
                "text": "To understand the possible roots of TRAK's underperformance relative to BM25 on FTRACE-TREX, we carry out a counterfactual analysis. Specifically, for a subset of the FTRACE-TREX test set, we create three corresponding counterfactual training sets. Then, starting from a pre-trained mt5-small model (the same model that we finetuned in (B) above to identify novel facts), we finetune several models on each counterfactual training set, and compute their average accuracy on the selected subset of 50 novel facts. Note that, by construction, we know that on this subset (i) the pre-trained model has an accuracy of 0%; and (ii) finetuning on the entire FTRACE-TREX training set (i.e., with no examples removed) yields models with 100% accuracy. 29 As for the counterfactual training sets, one should note that:\u2022 Counterfactual training set (c) is missing all of the proponents for our subset of 50 novel factswe would thus expect the corresponding finetuned model to have very low accuracy. In particular, there is ostensibly no direct evidence for any of the novel facts of interest anywhere in this counterfactual training set.\u2022 Being constructed with BM25, counterfactual training set (b) has high lexical overlap with the novel facts of interest. Since BM25 performs well on the FTRACE-TREX benchmark, we would also expect the resulting models to have low accuracy.In Figure 9 , we report the resulting models' average performance on the set of 50 selected novel facts. What we find is that, counter to the above intuition, only the TRAK-based counterfactual training set is able to significantly change model behavior. That is, the counterfactual effect of removing the most important images as identified by TRAK on the selected subset of novel facts is significantly higher than both (a) that of removing the most important images according to BM25; and (b) that of removing the ground-truth proponents of the facts as indicated by the FTRACE-TREX benchmark.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1378,
                        "end": 1379,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "F.4 Counterfactual experiment setup",
                "sec_num": null
            },
            {
                "text": "Prior works have demonstrated the potential of leveraging data attribution for a variety of downstream applications, ranging from explaining predictions [KL17; KSH22], cleaning datasets [JDW+19] , removing poisoned examples [LZL+22] to quantifying uncertainty [AV20] . Given the effectiveness of TRAK, we expect that using it in place of existing attribution methods will improve the performance in many of these downstream applications. Moreover, given its computational efficiency, TRAK can expand the settings in which these prior data attribution methods are feasible. Indeed, we already saw some examples in Section 5.3. We highlight a few promising directions in particular:Fact tracing and attribution for generative models. Fact tracing, which we studied in Section 5.2, is a problem of increasing relevancy as large language models are widely deployed. Leveraging TRAK for fact tracing, or attribution more broadly, may help understand the capabilities or improve the trustworthiness of recent models such as GPT-3 [BMR+20] and ChatGPT, 30 by tracing their outputs back to sources in a way that is faithful to the actual model. More broadly, attribution for generative models (e.g., stable diffusion [HJA20; RBL+22]) is an interesting direction for future work.Optimizing datasets. TRAK scores allow one to quantify the impact of individual training examples on model predictions on a given target example. By aggregating this information, we can optimize what data we train the models on, for instance, to choose coresets or to select new data for active learning. Given the trend of training models on ever increasing size of datasets [HBM+22] , filtering data based on their TRAK scores can also help models achieve with the benefits of scale without the computational cost.Another advantage of TRAK is that it is fully differentiable in the input (note that the associated gradients are different from the gradients with respect to model parameters that we use when computing TRAK). One potential direction is to leverage this differentiability for dataset distillation. Given the effectiveness of the NTK for this problem [NNX+21] , there is potential in leveraging TRAK-which uses the eNTK-in this setting.",
                "cite_spans": [
                    {
                        "start": 186,
                        "end": 194,
                        "text": "[JDW+19]",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 224,
                        "end": 232,
                        "text": "[LZL+22]",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 260,
                        "end": 266,
                        "text": "[AV20]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1646,
                        "end": 1654,
                        "text": "[HBM+22]",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 2136,
                        "end": 2144,
                        "text": "[NNX+21]",
                        "ref_id": "BIBREF67"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "G.1 Further applications of TRAK",
                "sec_num": null
            },
            {
                "text": "Empirical NTK. TRAK leverages the empirical NTK to approximate the original model. Better understanding of when this approximation is accurate may give insights into improving TRAK's efficacy. For example, incorporating higher order approximations [HY20; BL20] beyond the linear approximation used in TRAK is a possible direction.Training dynamics and optimization. Prior works [LM20; LBD+20] suggest that neural network training can exhibit two stages or regimes: in the first stage, the features learned by the network evolve rapidly; in the second stage, the features remain approximately invariant and the overall optimization trajectory is more akin a convex setting. We can view our use of the final eNTK as modeling this second stage. Understanding the extent to which the first stage (which TRAK does not model) accounts for the remaining gap between true model outputs and TRAK's predictions may help us understand the limits of method as well as improve Another direction is to study whether properly accounting for other optimization components used during training, such as mini-batches, momentum, or weight decay, can improve our estimator.Ensembles. As we saw in Appendix E.2, computing TRAK over an ensemble of models significantly improves its efficacy. In particular, our results suggest that the eNTK's derived from independently trained models capture non-overlapping information. Better understanding of the role of ensembling here may us better understand the mechanisms underlying ensembles in other contexts and can also provide practical insights for improving TRAK's efficiency. For instance, understanding when model checkpoints from a single trajectory can approximate the full ensemble (Appendix E.3) can be valuable in settings where it is expensive to even finetune several models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "G.2 Understanding and improving the TRAK estimator",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Second-order stochastic optimization for machine learning in linear time",
                "authors": [
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Bullins",
                        "suffix": ""
                    },
                    {
                        "first": "Elad",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "The Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Naman Agarwal, Brian Bullins, and Elad Hazan. \"Second-order stochastic optimiza- tion for machine learning in linear time\". In: The Journal of Machine Learning Research. 2017.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Towards Tracing Factual Knowledge in Language Models Back to the Training Data",
                "authors": [
                    {
                        "first": "Ekin",
                        "middle": [],
                        "last": "Akyurek",
                        "suffix": ""
                    },
                    {
                        "first": "Tolga",
                        "middle": [],
                        "last": "Bolukbasi",
                        "suffix": ""
                    },
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Binbin",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Tenney",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Andreas",
                        "suffix": ""
                    },
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Guu",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Findings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob An- dreas, and Kelvin Guu. \"Towards Tracing Factual Knowledge in Language Models Back to the Training Data\". In: Findings of EMNLP. 2022.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Neural networks as kernel learners: The silent alignment effect",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Atanasov",
                        "suffix": ""
                    },
                    {
                        "first": "Blake",
                        "middle": [],
                        "last": "Bordelon",
                        "suffix": ""
                    },
                    {
                        "first": "Cengiz",
                        "middle": [],
                        "last": "Pehlevan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. \"Neural networks as kernel learners: The silent alignment effect\". In: ICLR. 2022.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Atanasov",
                        "suffix": ""
                    },
                    {
                        "first": "Blake",
                        "middle": [],
                        "last": "Bordelon",
                        "suffix": ""
                    },
                    {
                        "first": "Sabarish",
                        "middle": [],
                        "last": "Sainathan",
                        "suffix": ""
                    },
                    {
                        "first": "Cengiz",
                        "middle": [],
                        "last": "Pehlevan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan. \"The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes\". In: ICLR. 2023.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
                "authors": [
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Arora",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [
                            "S"
                        ],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ruosong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Machine Learning (ICML)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. \"Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neu- ral Networks\". In: International Conference on Machine Learning (ICML). 2019.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Lqf: Linear quadratic fine-tuning",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Achille",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Golatkar",
                        "suffix": ""
                    },
                    {
                        "first": "Avinash",
                        "middle": [],
                        "last": "Ravichandran",
                        "suffix": ""
                    },
                    {
                        "first": "Marzia",
                        "middle": [],
                        "last": "Polito",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Soatto",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alessandro Achille, Aditya Golatkar, Avinash Ravichandran, Marzia Polito, and Stefano Soatto. \"Lqf: Linear quadratic fine-tuning\". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The principle of minimized iterations in the solution of the matrix eigenvalue problem",
                "authors": [
                    {
                        "first": "Walter",
                        "middle": [],
                        "last": "Edwin",
                        "suffix": ""
                    },
                    {
                        "first": "Arnoldi",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1951,
                "venue": "Quarterly of applied mathematics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Walter Edwin Arnoldi. \"The principle of minimized iterations in the solution of the matrix eigenvalue problem\". In: Quarterly of applied mathematics. 1951.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Discriminative jackknife: Quantifying uncertainty in deep learning via higher-order influence functions",
                "authors": [
                    {
                        "first": "Ahmed",
                        "middle": [],
                        "last": "Alaa",
                        "suffix": ""
                    },
                    {
                        "first": "Mihaela",
                        "middle": [],
                        "last": "Van",
                        "suffix": ""
                    },
                    {
                        "first": "Der",
                        "middle": [],
                        "last": "Schaar",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ahmed Alaa and Mihaela Van Der Schaar. \"Discriminative jackknife: Quantifying uncertainty in deep learning via higher-order influence functions\". In: International Conference on Machine Learning. 2020.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Generalization through the lens of leave-one-out error",
                "authors": [
                    {
                        "first": "Gregor",
                        "middle": [],
                        "last": "Bachmann",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Hofmann",
                        "suffix": ""
                    },
                    {
                        "first": "Aur\u00e9lien",
                        "middle": [],
                        "last": "Lucchi",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2203.03443.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gregor Bachmann, Thomas Hofmann, and Aur\u00e9lien Lucchi. \"Generalization through the lens of leave-one-out error\". In: arXiv preprint arXiv:2203.03443. 2022.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Beyond linearization: On quadratic and higher-order approximation of wide neural networks",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yu Bai and Jason D Lee. \"Beyond linearization: On quadratic and higher-order ap- proximation of wide neural networks\". In: ICLR. 2020.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Random projection, margins, kernels, and feature-selection",
                "authors": [
                    {
                        "first": "Avrim",
                        "middle": [],
                        "last": "Blum",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Lecture notes in computer science",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Avrim Blum. \"Random projection, margins, kernels, and feature-selection\". In: Lecture notes in computer science. Springer, 2006.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Language models are few-shot learners",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Tom B Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Melanie",
                        "middle": [],
                        "last": "Ryder",
                        "suffix": ""
                    },
                    {
                        "first": "Jared",
                        "middle": [],
                        "last": "Subbiah",
                        "suffix": ""
                    },
                    {
                        "first": "Prafulla",
                        "middle": [],
                        "last": "Kaplan",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Neelakantan",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Shyam",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2005.14165"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \"Language models are few-shot learners\". In: arXiv preprint arXiv:2005.14165 (2020).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "If Influence Functions are the Answer, Then What is the Question?",
                "authors": [
                    {
                        "first": "Juhan",
                        "middle": [],
                        "last": "Bae",
                        "suffix": ""
                    },
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Alston",
                        "middle": [],
                        "last": "Lo",
                        "suffix": ""
                    },
                    {
                        "first": "Marzyeh",
                        "middle": [],
                        "last": "Ghassemi",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [],
                        "last": "Grosse",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2209.05364.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. \"If In- fluence Functions are the Answer, Then What is the Question?\" In: ArXiv preprint arXiv:2209.05364. 2022.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Influence Functions in Deep Learning Are Fragile",
                "authors": [
                    {
                        "first": "Samyadeep",
                        "middle": [],
                        "last": "Basu",
                        "suffix": ""
                    },
                    {
                        "first": "Phillip",
                        "middle": [],
                        "last": "Pope",
                        "suffix": ""
                    },
                    {
                        "first": "Soheil",
                        "middle": [],
                        "last": "Feizi",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samyadeep Basu, Phillip Pope, and Soheil Feizi. \"Influence Functions in Deep Learn- ing Are Fragile\". In: International Conference on Learning Representations (ICLR). 2021.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models",
                "authors": [
                    {
                        "first": "Bernd",
                        "middle": [],
                        "last": "Bohnet",
                        "suffix": ""
                    },
                    {
                        "first": "Pat",
                        "middle": [],
                        "last": "Vinh Q Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Roee",
                        "middle": [],
                        "last": "Verga",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Aharoni",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Andor",
                        "suffix": ""
                    },
                    {
                        "first": "Baldini",
                        "middle": [],
                        "last": "Livio",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Soares",
                        "suffix": ""
                    },
                    {
                        "first": "Kuzman",
                        "middle": [],
                        "last": "Eisenstein",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Ganchev",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Herzig",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Hui",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.08037.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. \"At- tributed Question Answering: Evaluation and Modeling for Attributed Large Lan- guage Models\". In: Arxiv preprint arXiv:2212.08037. 2022.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Second-Order Group Influence Functions for Black-Box Predictions",
                "authors": [
                    {
                        "first": "Samyadeep",
                        "middle": [],
                        "last": "Basu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuchen",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    },
                    {
                        "first": "Soheil",
                        "middle": [],
                        "last": "Feizi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Machine Learning (ICML)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samyadeep Basu, Xuchen You, and Soheil Feizi. \"Second-Order Group Influence Functions for Black-Box Predictions\". In: International Conference on Machine Learning (ICML). 2019.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Reproducible scaling laws for contrastive language-image learning",
                "authors": [
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Cherti",
                        "suffix": ""
                    },
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Beaumont",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [],
                        "last": "Wightman",
                        "suffix": ""
                    },
                    {
                        "first": "Mitchell",
                        "middle": [],
                        "last": "Wortsman",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Ilharco",
                        "suffix": ""
                    },
                    {
                        "first": "Cade",
                        "middle": [],
                        "last": "Gordon",
                        "suffix": ""
                    },
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Schuhmann",
                        "suffix": ""
                    },
                    {
                        "first": "Ludwig",
                        "middle": [],
                        "last": "Schmidt",
                        "suffix": ""
                    },
                    {
                        "first": "Jenia",
                        "middle": [],
                        "last": "Jitsev",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.07143.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Il- harco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. \"Re- producible scaling laws for contrastive language-image learning\". In: arXiv preprint arXiv:2212.07143. 2022.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Quantifying memorization across neural language models",
                "authors": [
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Carlini",
                        "suffix": ""
                    },
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Jagielski",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Tramer",
                        "suffix": ""
                    },
                    {
                        "first": "Chiyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2202.07646.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. \"Quantifying memorization across neural language models\". In: arXiv preprint arXiv:2202.07646. 2022.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Careful Data Curation Stabilizes In-context Learning",
                "authors": [
                    {
                        "first": "Ting-Yun",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.10378.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ting-Yun Chang and Robin Jia. \"Careful Data Curation Stabilizes In-context Learning\". In: Arxiv preprint arXiv:2212.10378. 2022.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Residuals and influence in regression",
                "authors": [
                    {
                        "first": "Dennis",
                        "middle": [],
                        "last": "Cook",
                        "suffix": ""
                    },
                    {
                        "first": "Sanford",
                        "middle": [],
                        "last": "Weisberg",
                        "suffix": ""
                    }
                ],
                "year": 1982,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R Dennis Cook and Sanford Weisberg. Residuals and influence in regression. New York: Chapman and Hall, 1982.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "A linearized framework and a new benchmark for model selection for fine-tuning",
                "authors": [
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Deshpande",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Achille",
                        "suffix": ""
                    },
                    {
                        "first": "Avinash",
                        "middle": [],
                        "last": "Ravichandran",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Luca",
                        "middle": [],
                        "last": "Zancato",
                        "suffix": ""
                    },
                    {
                        "first": "Charless",
                        "middle": [],
                        "last": "Fowlkes",
                        "suffix": ""
                    },
                    {
                        "first": "Rahul",
                        "middle": [],
                        "last": "Bhotika",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Soatto",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Perona",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2102.00084.2021"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless Fowlkes, Rahul Bhotika, Stefano Soatto, and Pietro Perona. \"A linearized framework and a new benchmark for model selection for fine-tuning\". In: arXiv preprint arXiv:2102.00084. 2021.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \"Bert: Pre- training of deep bidirectional transformers for language understanding\". In: (2019).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
                "authors": [
                    {
                        "first": "Katherine",
                        "middle": [
                            "A"
                        ],
                        "last": "Alexander D'amour",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Heller",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Moldovan",
                        "suffix": ""
                    },
                    {
                        "first": "Babak",
                        "middle": [],
                        "last": "Adlam",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Alipanahi",
                        "suffix": ""
                    },
                    {
                        "first": "Christina",
                        "middle": [],
                        "last": "Beutel",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Deaton",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [
                            "D"
                        ],
                        "last": "Eisenstein",
                        "suffix": ""
                    },
                    {
                        "first": "Farhad",
                        "middle": [],
                        "last": "Hoffman",
                        "suffix": ""
                    },
                    {
                        "first": "Neil",
                        "middle": [],
                        "last": "Hormozdiari",
                        "suffix": ""
                    },
                    {
                        "first": "Shaobo",
                        "middle": [],
                        "last": "Houlsby",
                        "suffix": ""
                    },
                    {
                        "first": "Ghassen",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Jerfel",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Karthikesalingam",
                        "suffix": ""
                    },
                    {
                        "first": "Yi-An",
                        "middle": [],
                        "last": "Lucic",
                        "suffix": ""
                    },
                    {
                        "first": "Cory",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Mclean",
                        "suffix": ""
                    },
                    {
                        "first": "Akinori",
                        "middle": [],
                        "last": "Mincu",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Mitani",
                        "suffix": ""
                    },
                    {
                        "first": "Zachary",
                        "middle": [],
                        "last": "Montanari",
                        "suffix": ""
                    },
                    {
                        "first": "Vivek",
                        "middle": [],
                        "last": "Nado",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Natarajan",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "F"
                        ],
                        "last": "Nielson",
                        "suffix": ""
                    },
                    {
                        "first": "Rajiv",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    },
                    {
                        "first": "Kim",
                        "middle": [],
                        "last": "Raman",
                        "suffix": ""
                    },
                    {
                        "first": "Rory",
                        "middle": [],
                        "last": "Ramasamy",
                        "suffix": ""
                    },
                    {
                        "first": "Jessica",
                        "middle": [],
                        "last": "Sayres",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Schrouff",
                        "suffix": ""
                    },
                    {
                        "first": "Shannon",
                        "middle": [],
                        "last": "Seneviratne",
                        "suffix": ""
                    },
                    {
                        "first": "Harini",
                        "middle": [],
                        "last": "Sequeira",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Suresh",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Veitch",
                        "suffix": ""
                    },
                    {
                        "first": "Xuezhi",
                        "middle": [],
                        "last": "Vladymyrov",
                        "suffix": ""
                    },
                    {
                        "first": "Kellie",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Webster",
                        "suffix": ""
                    },
                    {
                        "first": "Taedong",
                        "middle": [],
                        "last": "Yadlowsky",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaohua",
                        "middle": [],
                        "last": "Yun",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sculley",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2011.03395.2020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexander D'Amour, Katherine A. Heller, Dan Moldovan, Ben Adlam, Babak Ali- panahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yi-An Ma, Cory Y. McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Mar- tin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. \"Underspecification Presents Challenges for Credibility in Modern Machine Learning\". In: Arxiv preprint arXiv:2011.03395. 2020.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "De-noising by soft-thresholding",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "David",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Donoho",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "IEEE Transactions on Information Theory",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David L Donoho. \"De-noising by soft-thresholding\". In: IEEE Transactions on Informa- tion Theory. 1995.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "T-rex: A large scale alignment of natural language with knowledge base triples",
                "authors": [
                    {
                        "first": "Hady",
                        "middle": [],
                        "last": "Elsahar",
                        "suffix": ""
                    },
                    {
                        "first": "Pavlos",
                        "middle": [],
                        "last": "Vougiouklis",
                        "suffix": ""
                    },
                    {
                        "first": "Arslen",
                        "middle": [],
                        "last": "Remaci",
                        "suffix": ""
                    },
                    {
                        "first": "Christophe",
                        "middle": [],
                        "last": "Gravier",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Hare",
                        "suffix": ""
                    },
                    {
                        "first": "Frederique",
                        "middle": [],
                        "last": "Laforest",
                        "suffix": ""
                    },
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Simperl",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. \"T-rex: A large scale alignment of natural language with knowledge base triples\". In: Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). 2018.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Ilharco",
                        "suffix": ""
                    },
                    {
                        "first": "Mitchell",
                        "middle": [],
                        "last": "Wortsman",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Vaishaal",
                        "middle": [],
                        "last": "Shankar",
                        "suffix": ""
                    },
                    {
                        "first": "Achal",
                        "middle": [],
                        "last": "Dave",
                        "suffix": ""
                    },
                    {
                        "first": "Ludwig",
                        "middle": [],
                        "last": "Schmidt",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. \"Data Determines Distributional Robustness in Con- trastive Language Image Pre-training (CLIP)\". In: ICML. 2022.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation",
                "authors": [
                    {
                        "first": "Vitaly",
                        "middle": [],
                        "last": "Feldman",
                        "suffix": ""
                    },
                    {
                        "first": "Chiyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
                "volume": "33",
                "issue": "",
                "pages": "2881--2891",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vitaly Feldman and Chiyuan Zhang. \"What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation\". In: Advances in Neural Information Processing Systems (NeurIPS). Vol. 33. 2020, pp. 2881-2891.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Convergence of Adversarial Training in Overparametrized Networks",
                "authors": [
                    {
                        "first": "Ruiqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Tianle",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Haochuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Liwei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Cho-Jui",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.07916"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, and Jason D Lee. \"Convergence of Adversarial Training in Overparametrized Networks\". In: arXiv preprint arXiv:1906.07916 (2019).",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Badnets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain",
                "authors": [
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Brendan",
                        "middle": [],
                        "last": "Dolan-Gavitt",
                        "suffix": ""
                    },
                    {
                        "first": "Siddharth",
                        "middle": [],
                        "last": "Garg",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1708.06733"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. \"Badnets: Identifying Vulnera- bilities in the Machine Learning Model Supply Chain\". In: arXiv preprint arXiv:1708.06733 (2017).",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Geirhos",
                        "suffix": ""
                    },
                    {
                        "first": "Patricia",
                        "middle": [],
                        "last": "Rubisch",
                        "suffix": ""
                    },
                    {
                        "first": "Claudio",
                        "middle": [],
                        "last": "Michaelis",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Bethge",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [
                            "A"
                        ],
                        "last": "Wichmann",
                        "suffix": ""
                    },
                    {
                        "first": "Wieland",
                        "middle": [],
                        "last": "Brendel",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Learning Representations (ICLR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wich- mann, and Wieland Brendel. \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.\" In: International Conference on Learning Representations (ICLR). 2019.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Data shapley: Equitable valuation of data for machine learning",
                "authors": [
                    {
                        "first": "Amirata",
                        "middle": [],
                        "last": "Ghorbani",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Machine Learning (ICML)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Amirata Ghorbani and James Zou. \"Data shapley: Equitable valuation of data for machine learning\". In: International Conference on Machine Learning (ICML). 2019.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "What makes ImageNet good for transfer learning?",
                "authors": [
                    {
                        "first": "Minyoung",
                        "middle": [],
                        "last": "Huh",
                        "suffix": ""
                    },
                    {
                        "first": "Pulkit",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [
                            "A"
                        ],
                        "last": "Efros",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1608.08614"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. \"What makes ImageNet good for transfer learning?\" In: arXiv preprint arXiv:1608.08614 (2016).",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Training compute-optimal large language models",
                "authors": [
                    {
                        "first": "Jordan",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Borgeaud",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Mensch",
                        "suffix": ""
                    },
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Buchatskaya",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Eliza",
                        "middle": [],
                        "last": "Rutherford",
                        "suffix": ""
                    },
                    {
                        "first": "Diego",
                        "middle": [],
                        "last": "De Las",
                        "suffix": ""
                    },
                    {
                        "first": "Lisa",
                        "middle": [
                            "Anne"
                        ],
                        "last": "Casas",
                        "suffix": ""
                    },
                    {
                        "first": "Johannes",
                        "middle": [],
                        "last": "Hendricks",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [],
                        "last": "Welbl",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2203.15556.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. \"Training compute-optimal large language models\". In: arXiv preprint arXiv:2203.15556. 2022.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Denoising Diffusion Probabilistic Models",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "Ajay",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Pieter",
                        "middle": [],
                        "last": "Abbeel",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Neural Information Processing Systems (NeurIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. \"Denoising Diffusion Probabilistic Models\". In: Neural Information Processing Systems (NeurIPS). 2020.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation",
                "authors": [
                    {
                        "first": "Zayd",
                        "middle": [],
                        "last": "Hammoudeh",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Lowd",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2201.10055.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zayd Hammoudeh and Daniel Lowd. \"Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation\". In: arXiv preprint arXiv:2201.10055. 2022.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Training Data Influence Analysis and Estimation: A Survey",
                "authors": [
                    {
                        "first": "Zayd",
                        "middle": [],
                        "last": "Hammoudeh",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Lowd",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.04612.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zayd Hammoudeh and Daniel Lowd. \"Training Data Influence Analysis and Estima- tion: A Survey\". In: arXiv preprint arXiv:2212.04612. 2022.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Integrating NLP using linked data",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Hellmann",
                        "suffix": ""
                    },
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Lehmann",
                        "suffix": ""
                    },
                    {
                        "first": "S\u00f6ren",
                        "middle": [],
                        "last": "Auer",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Br\u00fcmmer",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "The Semantic Web-ISWC 2013: 12th International Semantic Web Conference",
                "volume": "",
                "issue": "",
                "pages": "98--113",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Hellmann, Jens Lehmann, S\u00f6ren Auer, and Martin Br\u00fcmmer. \"Integrating NLP using linked data\". In: The Semantic Web-ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part II 12. Springer. 2013, pp. 98-113.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Robust statistics: the approach based on influence functions",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Frank R Hampel",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Elvezio",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Ronchetti",
                        "suffix": ""
                    },
                    {
                        "first": "Werner",
                        "middle": [
                            "A"
                        ],
                        "last": "Rousseeuw",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Stahel",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "196",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics: the approach based on influence functions. Vol. 196. John Wiley & Sons, 2011.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy",
                "authors": [
                    {
                        "first": "Jiaoyang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Horng-Tzer",
                        "middle": [],
                        "last": "Yau",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 37th International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiaoyang Huang and Horng-Tzer Yau. \"Dynamics of Deep Neural Networks and Neural Tangent Hierarchy\". In: Proceedings of the 37th International Conference on Machine Learning. 2020.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Evaluation of similaritybased explanations",
                "authors": [
                    {
                        "first": "Kazuaki",
                        "middle": [],
                        "last": "Hanawa",
                        "suffix": ""
                    },
                    {
                        "first": "Sho",
                        "middle": [],
                        "last": "Yokoi",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Hara",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Inui",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, and Kentaro Inui. \"Evaluation of similarity- based explanations\". In: International Conference on Learning Representations (ICLR). 2021.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "A framework and benchmark for deep batch active learning for regression",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Holzm\u00fcller",
                        "suffix": ""
                    },
                    {
                        "first": "Johannes",
                        "middle": [],
                        "last": "Viktor Zaverkin",
                        "suffix": ""
                    },
                    {
                        "first": "Ingo",
                        "middle": [],
                        "last": "K\u00e4stner",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Steinwart",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2203.09410"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "David Holzm\u00fcller, Viktor Zaverkin, Johannes K\u00e4stner, and Ingo Steinwart. \"A frame- work and benchmark for deep batch active learning for regression\". In: arXiv preprint arXiv:2203.09410 (2022).",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Deep Residual Learning for Image Recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. 2015.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Datamodels: Predicting Predictions from Training Data",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Ilyas",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Sung",
                        "suffix": ""
                    },
                    {
                        "first": "Logan",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Engstrom",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksander",
                        "middle": [],
                        "last": "Leclerc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Madry",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "International Conference on Machine Learning (ICML)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. \"Datamodels: Predicting Predictions from Training Data\". In: International Conference on Machine Learning (ICML). 2022.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Adversarial Examples Are Not Bugs, They Are Features",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Ilyas",
                        "suffix": ""
                    },
                    {
                        "first": "Shibani",
                        "middle": [],
                        "last": "Santurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitris",
                        "middle": [],
                        "last": "Tsipras",
                        "suffix": ""
                    },
                    {
                        "first": "Logan",
                        "middle": [],
                        "last": "Engstrom",
                        "suffix": ""
                    },
                    {
                        "first": "Brandon",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksander",
                        "middle": [],
                        "last": "Madry",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. \"Adversarial Examples Are Not Bugs, They Are Features\". In: Neural Information Processing Systems (NeurIPS). 2019.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Towards Efficient Data Valuation Based on the Shapley Value",
                "authors": [
                    {
                        "first": "Ruoxi",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Dao",
                        "suffix": ""
                    },
                    {
                        "first": "Boxin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Frances",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Ann",
                        "middle": [],
                        "last": "Hubis",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [],
                        "last": "Hynes",
                        "suffix": ""
                    },
                    {
                        "first": "Nezihe",
                        "middle": [
                            "Merve"
                        ],
                        "last": "G\u00fcrel",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ce",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Dawn",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Costas",
                        "middle": [
                            "J"
                        ],
                        "last": "Spanos",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve G\u00fcrel, Bo Li, Ce Zhang, Dawn Song, and Costas J. Spanos. \"Towards Efficient Data Val- uation Based on the Shapley Value\". In: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics. 2019.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
                "authors": [
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Jacot",
                        "suffix": ""
                    },
                    {
                        "first": "Franck",
                        "middle": [],
                        "last": "Gabriel",
                        "suffix": ""
                    },
                    {
                        "first": "Clement",
                        "middle": [],
                        "last": "Hongler",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Neural Information Processing Systems (NeurIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arthur Jacot, Franck Gabriel, and Clement Hongler. \"Neural Tangent Kernel: Con- vergence and Generalization in Neural Networks\". In: Neural Information Processing Systems (NeurIPS). 2018.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Extensions of Lipschitz mappings into a Hilbert space",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "William",
                        "suffix": ""
                    },
                    {
                        "first": "Joram",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lindenstrauss",
                        "suffix": ""
                    }
                ],
                "year": 1984,
                "venue": "Contemporary mathematics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "William B Johnson and Joram Lindenstrauss. \"Extensions of Lipschitz mappings into a Hilbert space\". In: Contemporary mathematics. 1984.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Scalability vs. Utility: Do We Have to Sacrifice One for the Other in Data Importance Quantification?",
                "authors": [
                    {
                        "first": "Ruoxi",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuehui",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Jiacen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Dao",
                        "suffix": ""
                    },
                    {
                        "first": "Bhavya",
                        "middle": [],
                        "last": "Kailkhura",
                        "suffix": ""
                    },
                    {
                        "first": "Ce",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dawn",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, and Dawn Song. \"Scalability vs. Utility: Do We Have to Sacrifice One for the Other in Data Importance Quantification?\" In: Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "On the accuracy of influence functions for measuring group effects",
                "authors": [
                    {
                        "first": "Pang",
                        "middle": [],
                        "last": "Wei Koh",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Siang",
                        "middle": [],
                        "last": "Ang",
                        "suffix": ""
                    },
                    {
                        "first": "Hubert Hk",
                        "middle": [],
                        "last": "Teo",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pang Wei Koh, Kai-Siang Ang, Hubert HK Teo, and Percy Liang. \"On the accuracy of influence functions for measuring group effects\". In: Neural Information Processing Systems (NeurIPS). 2019.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Interpreting black box predictions using fisher kernels",
                "authors": [
                    {
                        "first": "Rajiv",
                        "middle": [],
                        "last": "Khanna",
                        "suffix": ""
                    },
                    {
                        "first": "Been",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Joydeep",
                        "middle": [],
                        "last": "Ghosh",
                        "suffix": ""
                    },
                    {
                        "first": "Sanmi",
                        "middle": [],
                        "last": "Koyejo",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "The 22nd International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rajiv Khanna, Been Kim, Joydeep Ghosh, and Sanmi Koyejo. \"Interpreting black box predictions using fisher kernels\". In: The 22nd International Conference on Artificial Intelligence and Statistics. 2019.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Understanding Black-box Predictions via Influence Functions",
                "authors": [
                    {
                        "first": "Pang",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Koh",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pang Wei Koh and Percy Liang. \"Understanding Black-box Predictions via Influence Functions\". In: International Conference on Machine Learning. 2017.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Learning Multiple Layers of Features from Tiny Images",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Krizhevsky. \"Learning Multiple Layers of Features from Tiny Images\". In: Technical report. 2009.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Resolving Training Biases via Influence-based Data Relabeling",
                "authors": [
                    {
                        "first": "Shuming",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Yanyan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Linpeng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "International Conference on Learning Representations (ICLR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuming Kong, Yanyan Shen, and Linpeng Huang. \"Resolving Training Biases via Influence-based Data Relabeling\". In: International Conference on Learning Representa- tions (ICLR). 2022.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "The large learning rate phase of deep learning: the catapult mechanism",
                "authors": [
                    {
                        "first": "Aitor",
                        "middle": [],
                        "last": "Lewkowycz",
                        "suffix": ""
                    },
                    {
                        "first": "Yasaman",
                        "middle": [],
                        "last": "Bahri",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Jascha",
                        "middle": [],
                        "last": "Sohl-Dickstein",
                        "suffix": ""
                    },
                    {
                        "first": "Guy",
                        "middle": [],
                        "last": "Gur-Ari",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2003.02218.2020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur- Ari. \"The large learning rate phase of deep learning: the catapult mechanism\". In: arXiv preprint arXiv:2003.02218. 2020.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Influence Selection for Active Learning",
                "authors": [
                    {
                        "first": "Zhuoming",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Huaping",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Weijia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jifeng",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Conghui",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICCV",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He. \"Influence Selection for Active Learning\". In: ICCV. 2021.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Deduplicating Training Data Makes Language Models Better",
                "authors": [
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Nystrom",
                        "suffix": ""
                    },
                    {
                        "first": "Chiyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Carlini",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. \"Deduplicating Training Data Makes Language Models Better\". In: Annual Meeting of the Association for Computational Linguistics (ACL). 2022.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "A unified approach to interpreting model predictions",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Lundberg",
                        "suffix": ""
                    },
                    {
                        "first": "Su-In",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Neural Information Processing Systems (NeurIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott Lundberg and Su-In Lee. \"A unified approach to interpreting model predictions\". In: Neural Information Processing Systems (NeurIPS). 2017.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "The two regimes of deep network training",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Leclerc",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksander",
                        "middle": [],
                        "last": "Madry",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2002.10376.2020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Leclerc and Aleksander Madry. \"The two regimes of deep network train- ing\". In: arXiv preprint arXiv:2002.10376. 2020.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Microsoft coco: Common objects in context",
                "authors": [
                    {
                        "first": "Tsung-Yi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Maire",
                        "suffix": ""
                    },
                    {
                        "first": "Serge",
                        "middle": [],
                        "last": "Belongie",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Hays",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Perona",
                        "suffix": ""
                    },
                    {
                        "first": "Deva",
                        "middle": [],
                        "last": "Ramanan",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Doll\u00e1r",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence",
                        "suffix": ""
                    },
                    {
                        "first": "Zitnick",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "European conference on computer vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra- manan, Piotr Doll\u00e1r, and C Lawrence Zitnick. \"Microsoft coco: Common objects in context\". In: European conference on computer vision (ECCV). 2014.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Properties of the after kernel",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Philip",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2105.10585.2021"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Philip M Long. \"Properties of the after kernel\". In: arXiv preprint arXiv:2105.10585. 2021.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments",
                "authors": [
                    {
                        "first": "Jinkun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Anqi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Mathias",
                        "middle": [],
                        "last": "Lecuyer",
                        "suffix": ""
                    },
                    {
                        "first": "Jinyang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Aurojit",
                        "middle": [],
                        "last": "Panda",
                        "suffix": ""
                    },
                    {
                        "first": "Siddhartha",
                        "middle": [],
                        "last": "Sen",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2206.10013"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jinkun Lin, Anqi Zhang, Mathias Lecuyer, Jinyang Li, Aurojit Panda, and Siddhartha Sen. \"Measuring the Effect of Training Data on Deep Learning Predictions via Ran- domized Experiments\". In: arXiv preprint arXiv:2206.10013 (2022).",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "New insights and perspectives on the natural gradient method",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Martens",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "The Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James Martens. \"New insights and perspectives on the natural gradient method\". In: The Journal of Machine Learning Research. 2020.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Behind the Scenes of Gradient Descent: A Trajectory Analysis via Basis Function Decomposition",
                "authors": [
                    {
                        "first": "Jianhao",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Lingjun",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Salar",
                        "middle": [],
                        "last": "Fattahi",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2210.00346.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jianhao Ma, Lingjun Guo, and Salar Fattahi. \"Behind the Scenes of Gradient De- scent: A Trajectory Analysis via Basis Function Decomposition\". In: arXiv preprint arXiv:2210.00346. 2022.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Gradients as features for deep representation learning",
                "authors": [
                    {
                        "first": "Fangzhou",
                        "middle": [],
                        "last": "Mu",
                        "suffix": ""
                    },
                    {
                        "first": "Yingyu",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Yin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fangzhou Mu, Yingyu Liang, and Yin Li. \"Gradients as features for deep representa- tion learning\". In: ICLR. 2020.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Compressed least-squares regression",
                "authors": [
                    {
                        "first": "Odalric",
                        "middle": [],
                        "last": "Maillard",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Munos",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Odalric Maillard and R\u00e9mi Munos. \"Compressed least-squares regression\". In: Ad- vances in Neural Information Processing Systems. 2009.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Fast adaptation with linearized neural networks",
                "authors": [
                    {
                        "first": "Wesley",
                        "middle": [],
                        "last": "Maddox",
                        "suffix": ""
                    },
                    {
                        "first": "Shuai",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Pablo",
                        "middle": [],
                        "last": "Moreno",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Gordon Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Damianou",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Conference on Artificial Intelligence and Statistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wesley Maddox, Shuai Tang, Pablo Moreno, Andrew Gordon Wilson, and Andreas Damianou. \"Fast adaptation with linearized neural networks\". In: International Con- ference on Artificial Intelligence and Statistics. 2021.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "A kernel-based view of language model fine-tuning",
                "authors": [
                    {
                        "first": "Sadhika",
                        "middle": [],
                        "last": "Malladi",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Wettig",
                        "suffix": ""
                    },
                    {
                        "first": "Dingli",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Arora",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2210.05643.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. \"A kernel-based view of language model fine-tuning\". In: arXiv preprint arXiv:2210.05643. 2022.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "Dataset distillation with infinitely wide convolutional networks",
                "authors": [
                    {
                        "first": "Timothy",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Roman",
                        "middle": [],
                        "last": "Novak",
                        "suffix": ""
                    },
                    {
                        "first": "Lechao",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Jaehoon",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "34",
                "issue": "",
                "pages": "5186--5198",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. \"Dataset distillation with infinitely wide convolutional networks\". In: Advances in Neural Information Processing Systems 34 (2021), pp. 5186-5198.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth",
                "authors": [
                    {
                        "first": "Thao",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Maithra",
                        "middle": [],
                        "last": "Raghu",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Kornblith",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thao Nguyen, Maithra Raghu, and Simon Kornblith. \"Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth\". In: International Conference on Learning Representations (ICLR). 2021.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Estimating Training Data Influence by Tracing Gradient Descent",
                "authors": [
                    {
                        "first": "Garima",
                        "middle": [],
                        "last": "Pruthi",
                        "suffix": ""
                    },
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Mukund",
                        "middle": [],
                        "last": "Sundararajan",
                        "suffix": ""
                    },
                    {
                        "first": "Satyen",
                        "middle": [],
                        "last": "Kale",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Neural Information Processing Systems (NeurIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Garima Pruthi, Frederick Liu, Mukund Sundararajan, and Satyen Kale. \"Estimating Training Data Influence by Tracing Gradient Descent\". In: Neural Information Processing Systems (NeurIPS). 2020.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "Logistic Regression Diagnostics",
                "authors": [
                    {
                        "first": "Daryl",
                        "middle": [],
                        "last": "Pregibon",
                        "suffix": ""
                    }
                ],
                "year": 1981,
                "venue": "The Annals of Statistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daryl Pregibon. \"Logistic Regression Diagnostics\". In: The Annals of Statistics. 1981.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Language Models as Knowledge Bases?",
                "authors": [
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Anton",
                        "middle": [],
                        "last": "Bakhtin",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. \"Language Models as Knowledge Bases?\" In: Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "High-resolution image synthesis with latent diffusion models",
                "authors": [
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Rombach",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Blattmann",
                        "suffix": ""
                    },
                    {
                        "first": "Dominik",
                        "middle": [],
                        "last": "Lorenz",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Esser",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Ommer",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "10684--10695",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Om- mer. \"High-resolution image synthesis with latent diffusion models\". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684- 10695.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "ImageNet Large Scale Visual Recognition Challenge",
                "authors": [
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Russakovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Satheesh",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiheng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrej",
                        "middle": [],
                        "last": "Karpathy",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Khosla",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Bernstein",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "C"
                        ],
                        "last": "Berg",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International Journal of Computer Vision (IJCV",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. \"ImageNet Large Scale Visual Recognition Challenge\". In: International Journal of Computer Vision (IJCV). 2015.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Learning transferable visual models from natural language supervision",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jong",
                        "middle": [
                            "Wook"
                        ],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Hallacy",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Goh",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Pamela",
                        "middle": [],
                        "last": "Mishkin",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.00020.2021"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. \"Learning transferable visual models from natural language supervision\". In: arXiv preprint arXiv:2103.00020. 2021.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "A scalable estimate of the extra-sample prediction error via approximate leave-one-out",
                "authors": [
                    {
                        "first": "Kamiar",
                        "middle": [],
                        "last": "Rahnama",
                        "suffix": ""
                    },
                    {
                        "first": "Rad",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Arian",
                        "middle": [],
                        "last": "Maleki",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1801.10243.2018"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kamiar Rahnama Rad and Arian Maleki. \"A scalable estimate of the extra-sample prediction error via approximate leave-one-out\". In: ArXiv preprint arXiv:1801.10243. 2018.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "Random features for large-scale kernel machines",
                "authors": [
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Rahimi",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Recht",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ali Rahimi and Benjamin Recht. \"Random features for large-scale kernel machines\". In: Advances in neural information processing systems. 2007.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\". In: Journal of Machine Learning Research (JMLR) (2020).",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Okapi at TREC-3",
                "authors": [
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Stephen E Robertson",
                        "suffix": ""
                    },
                    {
                        "first": "Susan",
                        "middle": [],
                        "last": "Walker",
                        "suffix": ""
                    },
                    {
                        "first": "Micheline",
                        "middle": [
                            "M"
                        ],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Hancock-Beaulieu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gatford",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. \"Okapi at TREC-3\". In: Nist Special Publication. 1995.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "Is a caption worth a thousand images? a controlled study for representation learning",
                "authors": [
                    {
                        "first": "Shibani",
                        "middle": [],
                        "last": "Santurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Dubois",
                        "suffix": ""
                    },
                    {
                        "first": "Rohan",
                        "middle": [],
                        "last": "Taori",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Tatsunori",
                        "middle": [],
                        "last": "Hashimoto",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2207.07635.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori Hashimoto. \"Is a caption worth a thousand images? a controlled study for representation learning\". In: arXiv preprint arXiv:2207.07635. 2022.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "Empirical analysis of the hessian of over-parametrized neural networks",
                "authors": [
                    {
                        "first": "Levent",
                        "middle": [],
                        "last": "Sagun",
                        "suffix": ""
                    },
                    {
                        "first": "Utku",
                        "middle": [],
                        "last": "Evci",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Ugur G\u00fcney",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Dauphin",
                        "suffix": ""
                    },
                    {
                        "first": "L\u00e9on",
                        "middle": [],
                        "last": "Bottou",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1706.04454.2017"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Levent Sagun, Utku Evci, V Ugur G\u00fcney, Yann Dauphin, and L\u00e9on Bottou. \"Empirical analysis of the hessian of over-parametrized neural networks\". In: arXiv preprint arXiv:1706.04454. 2017.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Understanding Influence Functions and Datamodels via Harmonic Analysis",
                "authors": [
                    {
                        "first": "Nikunj",
                        "middle": [],
                        "last": "Saunshi",
                        "suffix": ""
                    },
                    {
                        "first": "Arushi",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Braverman",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Arora",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nikunj Saunshi, Arushi Gupta, Mark Braverman, and Sanjeev Arora. \"Understanding Influence Functions and Datamodels via Harmonic Analysis\". In: ICLR. 2023.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "Notes on the n-Person Game-II: The Value of an n-Person Game, The RAND Corporation, The RAND Corporation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ls Shapley",
                        "suffix": ""
                    }
                ],
                "year": 1951,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "LS Shapley. \"Notes on the n-Person Game-II: The Value of an n-Person Game, The RAND Corporation, The RAND Corporation\". In: Research Memorandum. 1951.",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "The Proof and Measurement of Association between Two Things",
                "authors": [
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Spearman",
                        "suffix": ""
                    }
                ],
                "year": 1904,
                "venue": "The American Journal of Psychology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Charles Spearman. \"The Proof and Measurement of Association between Two Things\". In: The American Journal of Psychology. 1904.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "ModelDiff: A Framework for Comparing Learning Algorithms",
                "authors": [
                    {
                        "first": "Harshay",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "Sung",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Ilyas",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksander",
                        "middle": [],
                        "last": "Madry",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2211.12491.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Harshay Shah, Sung Min Park, Andrew Ilyas, and Aleksander Madry. \"ModelDiff: A Framework for Comparing Learning Algorithms\". In: arXiv preprint arXiv:2211.12491. 2022.",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "Breeds: Benchmarks for subpopulation shift",
                "authors": [
                    {
                        "first": "Shibani",
                        "middle": [],
                        "last": "Santurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitris",
                        "middle": [],
                        "last": "Tsipras",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksander",
                        "middle": [],
                        "last": "Madry",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. \"Breeds: Benchmarks for subpopulation shift\". In: International Conference on Learning Representations (ICLR). 2021.",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "Scaling up influence functions",
                "authors": [
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Schioppa",
                        "suffix": ""
                    },
                    {
                        "first": "Polina",
                        "middle": [],
                        "last": "Zablotskaia",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Vilar",
                        "suffix": ""
                    },
                    {
                        "first": "Artem",
                        "middle": [],
                        "last": "Sokolov",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "36",
                "issue": "",
                "pages": "8179--8186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. \"Scaling up influence functions\". In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 8. 2022, pp. 8179-8186.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Interactive label cleaning with example-based explanations",
                "authors": [
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Teso",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Bontempelli",
                        "suffix": ""
                    },
                    {
                        "first": "Fausto",
                        "middle": [],
                        "last": "Giunchiglia",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Passerini",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefano Teso, Andrea Bontempelli, Fausto Giunchiglia, and Andrea Passerini. \"In- teractive label cleaning with example-based explanations\". In: Advances in Neural Information Processing Systems. 2021.",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "Lamda: Language models for dialog applications",
                "authors": [
                    {
                        "first": "Romal",
                        "middle": [],
                        "last": "Thoppilan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "De"
                        ],
                        "last": "Freitas",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Apoorv",
                        "middle": [],
                        "last": "Kulshreshtha",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Heng-Tze",
                        "suffix": ""
                    },
                    {
                        "first": "Alicia",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Taylor",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Leslie",
                        "middle": [],
                        "last": "Bos",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Baker",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2201.08239.2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. \"Lamda: Language models for dialog applications\". In: ArXiv preprint arXiv:2201.08239. 2022.",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "Random projections for large-scale regression",
                "authors": [
                    {
                        "first": "Gian-Andrea",
                        "middle": [],
                        "last": "Thanei",
                        "suffix": ""
                    },
                    {
                        "first": "Christina",
                        "middle": [],
                        "last": "Heinze",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolai",
                        "middle": [],
                        "last": "Meinshausen",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Big and Complex Data Analysis: Methodologies and Applications",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gian-Andrea Thanei, Christina Heinze, and Nicolai Meinshausen. \"Random projec- tions for large-scale regression\". In: Big and Complex Data Analysis: Methodologies and Applications. 2017.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Attention is All you Need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. \"Attention is All you Need\". In: Advances in Neural Information Processing Systems (2017).",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "Influence sketching: Finding influential samples in large-scale regressions",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Wojnowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Cruz",
                        "suffix": ""
                    },
                    {
                        "first": "Xuan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Wolff",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Caleb",
                        "middle": [],
                        "last": "Crable",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "2016 IEEE International Conference on Big Data (Big Data)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Wojnowicz, Ben Cruz, Xuan Zhao, Brian Wallace, Matt Wolff, Jay Luan, and Caleb Crable. \"Influence sketching: Finding influential samples in large-scale regres- sions\". In: 2016 IEEE International Conference on Big Data (Big Data). 2016.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Steinhardt",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Wei, Wei Hu, and Jacob Steinhardt. \"More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize\". In: ICML. 2022.",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "Regularization matters: Generalization and optimization of neural nets vs their induced kernel",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Qiang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Tengyu",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. \"Regularization matters: Gen- eralization and optimization of neural nets vs their induced kernel\". In: Advances in Neural Information Processing Systems. 2019.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Amanpreet",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.07461"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. \"GLUE: A multi-task benchmark and analysis platform for natural language understanding\". In: arXiv preprint arXiv:1804.07461 (2018).",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
                "authors": [
                    {
                        "first": "Linting",
                        "middle": [],
                        "last": "Xue",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Constant",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Mihir",
                        "middle": [],
                        "last": "Kale",
                        "suffix": ""
                    },
                    {
                        "first": "Rami",
                        "middle": [],
                        "last": "Al-Rfou",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Siddhant",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Barua",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Sid- dhant, Aditya Barua, and Colin Raffel. \"mT5: A massively multilingual pre-trained text-to-text transformer\". In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
                "links": null
            },
            "BIBREF96": {
                "ref_id": "b96",
                "title": "Representer Point Selection for Explaining Deep Neural Networks",
                "authors": [
                    {
                        "first": "Chih-Kuan",
                        "middle": [],
                        "last": "Yeh",
                        "suffix": ""
                    },
                    {
                        "first": "Joon Sik",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [
                            "E H"
                        ],
                        "last": "Yen",
                        "suffix": ""
                    },
                    {
                        "first": "Pradeep",
                        "middle": [],
                        "last": "Ravikumar",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chih-Kuan Yeh, Joon Sik Kim, Ian E. H. Yen, and Pradeep Ravikumar. \"Represen- ter Point Selection for Explaining Deep Neural Networks\". In: Neural Information Processing Systems (NeurIPS). 2018.",
                "links": null
            },
            "BIBREF97": {
                "ref_id": "b97",
                "title": "Tensor Programs IIb: Architectural Universality Of Neural Tangent Kernel Training Dynamics",
                "authors": [
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Etai",
                        "middle": [],
                        "last": "Littwin",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 38th International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Greg Yang and Etai Littwin. \"Tensor Programs IIb: Architectural Universality Of Neural Tangent Kernel Training Dynamics\". In: Proceedings of the 38th International Conference on Machine Learning. 2021.",
                "links": null
            },
            "BIBREF98": {
                "ref_id": "b98",
                "title": "TCT: Convexifying federated learning using bootstrapped neural tangent kernels",
                "authors": [
                    {
                        "first": "Yaodong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Praneeth",
                        "middle": [],
                        "last": "Sai",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Karimireddy",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaodong Yu, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, and Michael I Jordan. \"TCT: Convexifying federated learning using bootstrapped neural tangent kernels\". In: NeurIPS. 2022.",
                "links": null
            },
            "BIBREF99": {
                "ref_id": "b99",
                "title": "Holographic Feature Representations of Deep Networks",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Martin A Zinkevich",
                        "suffix": ""
                    },
                    {
                        "first": "Dale",
                        "middle": [],
                        "last": "Davies",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "UAI. 2017",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin A Zinkevich, Alex Davies, and Dale Schuurmans. \"Holographic Feature Representations of Deep Networks.\" In: UAI. 2017.",
                "links": null
            },
            "BIBREF100": {
                "ref_id": "b100",
                "title": "The unreasonable effectiveness of deep features as a perceptual metric",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Phillip",
                        "middle": [],
                        "last": "Isola",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [
                            "A"
                        ],
                        "last": "Efros",
                        "suffix": ""
                    },
                    {
                        "first": "Eli",
                        "middle": [],
                        "last": "Shechtman",
                        "suffix": ""
                    },
                    {
                        "first": "Oliver",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. \"The unreasonable effectiveness of deep features as a perceptual metric\". In: Computer Vision and Pattern Recognition (CVPR). 2018.",
                "links": null
            },
            "BIBREF101": {
                "ref_id": "b101",
                "title": "Rethinking Influence Functions of Neural Networks in the Over-Parameterized Regime",
                "authors": [
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shihua",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rui Zhang and Shihua Zhang. \"Rethinking Influence Functions of Neural Networks in the Over-Parameterized Regime\". In: Proceedings of the AAAI Conference on Artificial Intelligence. 2022.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "fig_num": "23",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure2: TRAK achieves state-of-the-art tradeoffs between attribution efficacy and efficiency. We use TRAK to attribute ResNet-9 classifiers trained on CIFAR-2 and CIFAR-10; ResNet-18 classifiers trained on ImageNet; and BERT-base models finetuned on QNLI. The x-axis indicates the computational cost measured as the number of trained models that a given method uses to compute attribution scores. The y-axis indicates the method's efficacy as measured by the linear datamodeling score (LDS). Error bars indicate 95% bootstrap confidence intervals."
            },
            "FIGREF2": {
                "fig_num": null,
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure 7: Attributing CLIP trained on MS COCO. The first column shows two target image-caption pairs from the validation set of MS COCO. The second two columns display the nearest neighbors to the target in CLIP embedding space (using the average of image and text cosine similarities). The next two columns show the train set samples that, according to TRAK, are most helpful for aligning the image embedding to the caption embedding. Similarly, the last two columns display the train samples that are the most detracting from aligning the image and caption embeddings. In Appendix D.3, we display more examples and also compare to TracIn."
            },
            "FIGREF3": {
                "fig_num": "8",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure 8: Which training inputs can we remove from the training set so as the resulting CLIP model no longer associates a target image with its caption? We measure how the cosine similarity between target image and caption embeddings is affected when we re-train a CLIP model after removing the most influential training examples-as identified by TRAK, TracIn, and CLIP similarity distance. We report the decrease in cosine similarity, averaged over 100 randomly selected image-caption pairs from the validation set. Error bars represent 95% confidence intervals."
            },
            "FIGREF4": {
                "fig_num": "9",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure 9: Identifying counterfactually important examples for learning facts on FTRACE-TREX. Given a set of queries that the language model (mt5-small) originally answers correctly after training, we compare how three different interventions-removing abstracts with the highest TRAK scores, removing the most similar abstracts according to BM25, and removing the ground-truth proponents as indicated by FTRACE-TREX-affect the resulting model's accuracy on the queries. The yaxis shows the decrease in accuracy (on the query set, relative to the original model) after each intervention; results are averaged over 50 queries and eight independent models. Error bars represent 95% confidence intervals."
            },
            "FIGREF5": {
                "fig_num": "10",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure 10: Using TRAK scores to identify brittle model predictions. Following the methodology of Ilyas et al. [IPE+22], we apply different data attribution methods to estimate the brittleness of model predictions on examples from the CIFAR-10 validation set.The number of models used by each attribution method is specified in parentheses, e.g., TRAK (100) indicates that TRAK scores were computed using an ensemble of 100 trained models."
            },
            "FIGREF6": {
                "fig_num": "1",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure D.1: (Left)The LDS of CIFAR-2 TRAK scores computed with \u03b1 = 0.5 models then evaluated on either models trained with either \u03b1 = 0.5 or \u03b1 = 0.75. Each point corresponds to a validation example. (Right) The LDS of CIFAR-2 datamodel scores compared with that of TRAK. Here, the LDS is measured on two different estimators."
            },
            "FIGREF7": {
                "fig_num": null,
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure D.3 (ImageNet), Figure D.4 (QNLI), and Figure D.5 (CLIP on MS COCO)."
            },
            "FIGREF8": {
                "fig_num": null,
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure D.4: Top TRAK attributions for QNLI examples. Yes/No indicates the label (entailment vs. no entailment)."
            },
            "FIGREF9": {
                "fig_num": "6",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure D.5: Top attributions for CLIP models trained on MS COCO. We display random test examples and their corresponding most helpful (highest-scoring) and most detracting (lowest-scoring) training examples according to TRAK, CLIP similarity distance, and TracIn."
            },
            "FIGREF10": {
                "fig_num": "1",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Right)"
            },
            "FIGREF11": {
                "fig_num": "1",
                "uris": null,
                "type_str": "figure",
                "num": null,
                "text": "Figure E.1: Left: The impact of the dimension of random projection on TRAK's performance on CIFAR-2. Each line corresponds to a different value of M \u2208 {10, 20, ..., 100} (the number of models TRAK is averaged over); darker lines correspond to higher M. As we increase the projected dimension, the LDS initially increases. However, beyond a certain dimension, the LDS begins to decrease. The \"optimal\" dimension (i.e., the peak in the above graph) increases with higher M. Right: The impact of ensembling more models on TRAK's performance on CIFAR-2. The performance of TRAK as a function of the number of models used in the ensembling step. TRAK scores are computed with random projections of dimension k = 4000."
            },
            "TABREF1": {
                "content": "<table><tr><td>far, our analysis ignores the fact</td></tr><tr><td>that in many modern settings, training is non-deterministic. That is, applying the same learning</td></tr><tr><td>algorithm to the same training dataset (i.e., changing only the random seed) can yield models with</td></tr><tr><td>(often significantly) differing behavior [NRK21; DHM+20]. Non-determinism poses a problem for</td></tr><tr><td>data attribution because by definition, we cannot explain such seed-based differences in terms of</td></tr><tr><td>the training data.</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": ";BNL+22]. As noted in prior works [TBG+21; BNL+22], this approximation is a more convenient choice than the full Hessian as it is guaranteed to be positive semi-definite."
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"7\">Method TRAK 100 TRAK 20 TracIn [PLS+20] IF [KL17] GAS [HL22a] random</td></tr><tr><td>\u03c1(\u03c4, \u03c4 DM )</td><td>0.26</td><td>0.19</td><td>0.00</td><td>0.03</td><td>0.03</td><td>-0.03</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "Comparing TRAK and datamodel scores. Recall from Section 2.2 that one can view datamodels[IPE+22] as an \"oracle\" of sorts for the linear datamodeling score (LDS) objective. It turns out, as we show in Table4, that TRAK scores correlate with datamodel scores, while scores of other attribution methods do not. (We define correlation here as the Spearman rank correlation between the vectors \u03c4 TRAK (z) and \u03c4 DM (z), averaged over multiple examples of interest z.) Correlation with datamodel scores. We measure the correlation between the attribution scores computed by different methods \u03c4 and those given by datamodels \u03c4 DM[IPE+22] on the CIFAR-10 dataset. Specifically, for each test example of interest z, we compute the Spearman rank correlation (\u03c1) between \u03c4(z) i and \u03c4 DM (z) i over training examples i that have nonzero datamodel weight \u03c4 DM (z) i and then average the resulting correlation over 1000 randomly chosen examples of interest."
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "In particular, we apply TRAK to fact tracing: the problem of tracing a language model's factual assertion back to the corresponding training examples. On the FTRACE-TREX fact tracing benchmark, TRAK significantly outperforms the best gradient-based baseline (TracIn) used in prior work. Furthermore, while TRAK performs worse than an information retrieval baseline (BM25 [RWJ+95]), we demonstrate that this is likely a shortcoming of the benchmark rather than of TRAK. In particular, removing training examples traced by"
            },
            "TABREF6": {
                "content": "<table><tr><td>Dataset</td><td/><td colspan=\"4\">TRAK TracIn [PLS+20] Infl. [KL17] Datamodels [IPE+22]</td></tr><tr><td>CIFAR-2</td><td># models</td><td>5</td><td>100</td><td>-</td><td>1,000</td></tr><tr><td/><td>Time (min.)</td><td>3</td><td>100</td><td>-</td><td>500</td></tr><tr><td/><td>LDS</td><td>0.203(3)</td><td>0.056(2)</td><td>-</td><td>0.162(5)</td></tr><tr><td colspan=\"2\">CIFAR-10 # models</td><td>20</td><td>20</td><td>1</td><td>5,000</td></tr><tr><td/><td>Time (min.)</td><td>20</td><td>60</td><td>20,000</td><td>2,500</td></tr><tr><td/><td>LDS</td><td>0.271(4)</td><td>0.056(7)</td><td>0.037(13)</td><td>0.199(4)</td></tr><tr><td>QNLI</td><td># models</td><td>10</td><td>1</td><td>1</td><td>20,000</td></tr><tr><td/><td>Time (min.)</td><td>640</td><td>284</td><td>18,000</td><td>176,000</td></tr><tr><td/><td>LDS</td><td>0.416(10)</td><td>0.077(29)</td><td>0.114(43)</td><td>0.344(32)</td></tr><tr><td colspan=\"2\">ImageNet # models</td><td>100</td><td>1</td><td>20</td><td>30,000</td></tr><tr><td/><td>Time (min.)</td><td>2920</td><td>76</td><td>&gt;100,000</td><td>525,000</td></tr><tr><td/><td>LDS</td><td>0.188(6)</td><td>0.008(6)</td><td>0.037(6)</td><td>0.1445(6)</td></tr><tr><td>Table D.2:</td><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "Comparison of different data attribution methods."
            },
            "TABREF7": {
                "content": "<table><tr><td>Example</td><td/><td/><td colspan=\"3\">Highest TRAK score (+)</td><td colspan=\"3\">Lowest TRAK score (-)</td></tr><tr><td colspan=\"3\">Q: What was a major success, especially in rebuilding Warsaw? A: Like many cities</td><td colspan=\"3\">Q: In 1998, the deal was renewed for what amount over four years? A: Tele-</td><td colspan=\"3\">Q: Who was a controversial figure due to a corked-bat incident? A: Already a</td></tr><tr><td colspan=\"3\">in Central and Eastern Europe, infrastruc-</td><td colspan=\"3\">vision money had also become much</td><td colspan=\"3\">controversial figure in the clubhouse af-</td></tr><tr><td colspan=\"3\">ture in Warsaw suffered considerably dur-</td><td colspan=\"3\">more important; the Football League re-</td><td colspan=\"3\">ter his corked-bat incident, Sammy's ac-</td></tr><tr><td colspan=\"6\">More positive ing its time as an Eastern Bloc economy -ceived \u00a36.3 million for a two-year agree-though it is worth mentioning that the ini-ment in 1986, but when that deal was re-Held-out Example tial Three-Year Plan to rebuild Poland (es-pecially Warsaw) was a major success, but newed in 1988, the price rose to \u00a344 mil-lion over four years. (Yes)</td><td colspan=\"3\">tions alienated much of his once strong More negative fan base as well as the few teammates still on good terms with him, (many teammates grew tired of Sosa playing</td></tr><tr><td colspan=\"3\">cannon what followed was very much the opposite. cannon cannon (Yes) Q: What is the name associated with the eight areas that make up a part of south-ern California? A: Southern California</td><td colspan=\"3\">cannon Q: Was was the name given to the cannon sundial Alsace provincinal court? A: The province had a single provincial court</td><td colspan=\"3\">canoe loud salsa music in the locker room) brass and possibly tarnished his place in Cubs' lore for years to come. (No) sundial Q: What do six of the questions asses? A: For each question on the scale that measures homosexuality there is a cor-</td></tr><tr><td colspan=\"3\">black swan consists of one Combined Statistical Area, black swan black swan eight Metropolitan Statistical Areas, one in-ternational metropolitan area, and multiple metropolitan divisions. (Yes)</td><td colspan=\"3\">black swan (Landgericht) and a central administra-black swan goose tion with its seat at Hagenau. (Yes)</td><td colspan=\"3\">goose responding question that measures het-goose goose erosexuality giving six matching pairs of questions. (No)</td></tr><tr><td colspan=\"9\">Q: Q: What kind of signs were removed form club Barcelona? A: All signs of re-</td></tr><tr><td/><td/><td/><td/><td/><td/><td colspan=\"3\">gional nationalism, including language,</td></tr><tr><td>bluetick</td><td colspan=\"4\">bluetick bluetick bluetick bluetick</td><td colspan=\"4\">black-and-tan coonhound short-haired German pointer flag and other signs of separatism were English springer Boston bull banned throughout Spain. (Yes)</td></tr><tr><td colspan=\"5\">lifeboat Q: What was the percentage of a female lifeboat lifeboat lifeboat lifeboat householder with no husband present? A: There were 158,349 households, of which</td><td>drilling platform</td><td>tractor</td><td>drilling platform</td><td>drilling platform</td></tr><tr><td colspan=\"3\">68,511 (43.3%) had children under the</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">age of 18 living in them, 69,284 (43.8%)</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">were opposite-sex married couples living to-</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"5\">siamang gether, 30,547 (19.3%) had a female house-siamang siamang siamang siamang holder with no husband present, 11,698 (7.4%) had a male householder with no wife present. (Yes)</td><td>titi</td><td>fur coat</td><td>patas</td><td>bearskin</td></tr><tr><td>platypus</td><td colspan=\"4\">platypus platypus platypus platypus</td><td>cleaver</td><td>electric ray</td><td colspan=\"2\">cleaver centipede</td></tr><tr><td>sandal</td><td>sandal</td><td>sandal</td><td>sandal</td><td>sandal</td><td>clog</td><td>clog</td><td>Band Aid</td><td>clog</td></tr><tr><td>cinema</td><td>cinema</td><td>cinema</td><td>cinema</td><td>cinema</td><td>bakery</td><td>tobacco shop</td><td>bakery</td><td>tobacco shop</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "What words are inscribed on the mace of parliament? A: The words There shall be a Scottish Parliament, which are the first words of the Scotland Act, are inscribed around the head of the mace, which has a formal ceremonial role in the meetings of Parliament, reinforcing the authority of the Parliament in its ability to make laws. (No) Whose name is on the gate-house fronting School Yard? A: His name is borne by the big gate-house in the west range of the cloisters, fronting School Yard, perhaps the most famous image of the school. (No)"
            },
            "TABREF8": {
                "content": "<table><tr><td colspan=\"2\"># training epochs LDS (M = 100)</td><td colspan=\"2\"># independent models LDS</td></tr><tr><td>1</td><td>0.100</td><td>5</td><td>0.329</td></tr><tr><td>5</td><td>0.204</td><td>6</td><td>0.340</td></tr><tr><td>10</td><td>0.265</td><td>10</td><td>0.350</td></tr><tr><td>15</td><td>0.293</td><td>100</td><td>0.355</td></tr><tr><td>25</td><td>0.308</td><td/><td/></tr><tr><td>TRAK.</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "Table E.2). Leveraing this can further improve the computational efficiency of 2: The performance of TRAK on CIFAR-10 as a function of the epoch at which we terminate model training. In all cases, TRAK scores are computed with projection dimension k = 1000 and M = 100 independently trained models."
            },
            "TABREF9": {
                "content": "<table/>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "3: TRAK maintains its efficacy when we use multiple checkpoints from different epochs of the same training run instead of checkpoints from independently-trained models (CIFAR-10). In all cases, M = 100 checkpoints and projection dimension k = 4000 are used to compute TRAK scores."
            },
            "TABREF10": {
                "content": "<table><tr><td colspan=\"3\">Experiment Reweighting Loss Diagonal R Leverage Averaging Correlation</td></tr><tr><td>0</td><td>out</td><td>0.499</td></tr><tr><td>1</td><td>out</td><td>0.499</td></tr><tr><td>2</td><td>out</td><td>0.430</td></tr><tr><td>3</td><td>in</td><td>0.416</td></tr><tr><td>4</td><td>out</td><td>0.403</td></tr><tr><td>5</td><td>out</td><td>0.391</td></tr><tr><td>6</td><td>out</td><td>0.056</td></tr><tr><td>Table E.4:</td><td/><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "Ablating the contribution of each term in the TRAK estimator. For these experiments, we use random projections of dimenseion k = 2000."
            },
            "TABREF11": {
                "content": "<table><tr><td/><td colspan=\"2\">Kernel representation Linear Datamodeling Score (LDS)</td></tr><tr><td>CIFAR-2</td><td>eNTK</td><td>0.516</td></tr><tr><td>CIFAR-2</td><td>penultimate layer</td><td>0.198</td></tr><tr><td>CIFAR-10</td><td>eNTK</td><td>0.413</td></tr><tr><td>CIFAR-10</td><td>penultimate layer</td><td>0.120</td></tr><tr><td>QNLI</td><td>eNTK</td><td>0.589</td></tr><tr><td>QNLI</td><td>penultimate layer</td><td>0.195</td></tr><tr><td>Table E.5:</td><td/><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null,
                "text": "Choice of the kernel in TRAK."
            }
        }
    }
}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to UIUC AI Chatbot","text":""},{"location":"#learning-to-write-documentation","title":"Learning to write documentation","text":"<p>It's just markdown, with truly OPTIONAL extra features.</p> <pre><code>import numpy as np\nfor i in range(10):\n  print(i)\n</code></pre>"},{"location":"#callout-boxes","title":"Callout boxes","text":"<p>Tip: Use callout boxes to highlight important information.</p> <p>It's critical to avoid this.</p> how to use Admonitions, or 'callout' boxes<pre><code>!!! danger \"Danger: don't do this!\"\n\n    It's critical to avoid this.\n</code></pre> Types of Admonitions, or 'callout' boxes<pre><code>note\nabstract\ninfo\ntip\nsuccess\nquestion\nwarning\nfailure\ndanger\nbug\nexample\nquote\n</code></pre>"},{"location":"#contribute-docs","title":"Contribute docs","text":"<p>Here's the reference for creating new docs: https://squidfunk.github.io/mkdocs-material/reference/</p>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#top-level-api-reference","title":"Top Level API Reference","text":""},{"location":"api_reference/#ai_ta_backend.main.add_canvas_users","title":"<code>add_canvas_users()</code>","text":"<p>Add users from canvas to the course</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/addCanvasUsers', methods=['GET'])\ndef add_canvas_users():\n  \"\"\"\n  Add users from canvas to the course\n  \"\"\"\n  print(\"In /addCanvasUsers\")\n\n  canvas = CanvasAPI()\n  canvas_course_id: str = request.args.get('course_id')\n  course_name: str = request.args.get('course_name')\n\n  success_or_failure = canvas.add_users(canvas_course_id, course_name)\n\n  response = jsonify({\"outcome\": success_or_failure})\n\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.delete","title":"<code>delete()</code>","text":"<p>Delete a single file from all our database: S3, Qdrant, and Supabase (for now). Note, of course, we still have parts of that file in our logs.</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/delete', methods=['DELETE'])\ndef delete():\n  \"\"\"\n  Delete a single file from all our database: S3, Qdrant, and Supabase (for now).\n  Note, of course, we still have parts of that file in our logs.\n  \"\"\"\n  course_name: str = request.args.get('course_name', default='', type=str)\n  s3_path: str = request.args.get('s3_path', default='', type=str)\n  source_url: str = request.args.get('url', default='', type=str)\n\n  if course_name == '' or (s3_path == '' and source_url == ''):\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'course_name' and ('s3_path' or 'source_url') must be provided. Course name: `{course_name}`, S3 path: `{s3_path}`, source_url: `{source_url}`\"\n    )\n\n  start_time = time.monotonic()\n  ingester = Ingest()\n  # background execution of tasks!!\n  executor.submit(ingester.delete_data, course_name, s3_path, source_url)\n  print(f\"From {course_name}, deleted file: {s3_path}\")\n  print(f\"\u23f0 Runtime of FULL delete func: {(time.monotonic() - start_time):.2f} seconds\")\n  del ingester\n\n  # we need instant return. Delets are \"best effort\" assume always successful... sigh :(\n  response = jsonify({\"outcome\": 'success'})\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.getAll","title":"<code>getAll()</code>","text":"<p>Get all course materials based on the course_name</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/getAll', methods=['GET'])\ndef getAll() -&gt; Response:\n  \"\"\"Get all course materials based on the course_name\n  \"\"\"\n  course_name: List[str] | str = request.args.get('course_name', default='', type=str)\n\n  if course_name == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=f\"Missing the one required parameter: 'course_name' must be provided. Course name: `{course_name}`\")\n\n  ingester = Ingest()\n  distinct_dicts = ingester.getAll(course_name)\n  del ingester\n\n  response = jsonify({\"distinct_files\": distinct_dicts})\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.getContextStuffedPrompt","title":"<code>getContextStuffedPrompt()</code>","text":"<p>Get a stuffed prompt for a given user question and course name. Args :    search_query (str)   course_name (str) : used for metadata filtering Returns : str   a very long \"stuffed prompt\" with question + summaries of 20 most relevant documents.</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/getContextStuffedPrompt', methods=['GET'])\ndef getContextStuffedPrompt() -&gt; Response:\n  \"\"\"\n  Get a stuffed prompt for a given user question and course name.\n  Args : \n    search_query (str)\n    course_name (str) : used for metadata filtering\n  Returns : str\n    a very long \"stuffed prompt\" with question + summaries of 20 most relevant documents.\n  \"\"\"\n  print(\"In /getContextStuffedPrompt\")\n\n  ingester = Ingest()\n  search_query: str = request.args.get('search_query', default='', type=str)\n  course_name: str = request.args.get('course_name', default='', type=str)\n  top_n: int = request.args.get('top_n', default=-1, type=int)\n  top_k_to_search: int = request.args.get('top_k_to_search', default=-1, type=int)\n\n  if search_query == '' or course_name == '' or top_n == -1 or top_k_to_search == -1:\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'search_query', 'course_name', 'top_n', and 'top_k_to_search' must be provided. Search query: `{search_query}`, Course name: `{course_name}`, Top N: `{top_n}`, Top K to search: `{top_k_to_search}`\"\n    )\n\n  start_time = time.monotonic()\n  stuffed_prompt = ingester.get_context_stuffed_prompt(search_query, course_name, top_n, top_k_to_search)\n  print(f\"\u23f0 Runtime of EXTREME prompt stuffing: {(time.monotonic() - start_time):.2f} seconds\")\n  del ingester\n\n  response = jsonify({\"prompt\": stuffed_prompt})\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.getTopContexts","title":"<code>getTopContexts()</code>","text":"<p>Get most relevant contexts for a given search query.</p> <p>Return value</p>"},{"location":"api_reference/#ai_ta_backend.main.getTopContexts--get-arguments","title":"GET arguments","text":"<p>course name (optional) str     A json response with TBD fields. search_query top_n</p>"},{"location":"api_reference/#ai_ta_backend.main.getTopContexts--returns","title":"Returns","text":"<p>JSON     A json response with TBD fields. Metadata fileds * pagenumber_or_timestamp * readable_filename * s3_pdf_path</p> <p>Example:  [   {     'readable_filename': 'Lumetta_notes',      'pagenumber_or_timestamp': 'pg. 19',      's3_pdf_path': '/courses//Lumetta_notes.pdf',      'text': 'In FSM, we do this...'   },  ]"},{"location":"api_reference/#ai_ta_backend.main.getTopContexts--raises","title":"Raises","text":"<p>Exception     Testing how exceptions are handled.</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/getTopContexts', methods=['GET'])\ndef getTopContexts() -&gt; Response:\n  \"\"\"Get most relevant contexts for a given search query.\n\n  Return value\n\n  ## GET arguments\n  course name (optional) str\n      A json response with TBD fields.\n  search_query\n  top_n\n\n  Returns\n  -------\n  JSON\n      A json response with TBD fields.\n  Metadata fileds\n  * pagenumber_or_timestamp\n  * readable_filename\n  * s3_pdf_path\n\n  Example: \n  [\n    {\n      'readable_filename': 'Lumetta_notes', \n      'pagenumber_or_timestamp': 'pg. 19', \n      's3_pdf_path': '/courses/&lt;course&gt;/Lumetta_notes.pdf', \n      'text': 'In FSM, we do this...'\n    }, \n  ]\n\n  Raises\n  ------\n  Exception\n      Testing how exceptions are handled.\n  \"\"\"\n  print(\"In getRopContexts in Main()\")\n  search_query: str = request.args.get('search_query', default='', type=str)\n  course_name: str = request.args.get('course_name', default='', type=str)\n  token_limit: int = request.args.get('token_limit', default=3000, type=int)\n  if search_query == '' or course_name == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'search_query' and 'course_name' must be provided. Search query: `{search_query}`, Course name: `{course_name}`\"\n    )\n\n  print(\"NUM ACTIVE THREADS (top of getTopContexts):\", threading.active_count())\n\n  ingester = Ingest()\n  found_documents = ingester.getTopContexts(search_query, course_name, token_limit)\n  print(\"NUM ACTIVE THREADS (after instantiating Ingest() class in getTopContexts):\", threading.active_count())\n  del ingester\n\n  response = jsonify(found_documents)\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.getTopContextsWithMQR","title":"<code>getTopContextsWithMQR()</code>","text":"<p>Get relevant contexts for a given search query, using Multi-query retrieval + filtering method.</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/getTopContextsWithMQR', methods=['GET'])\ndef getTopContextsWithMQR() -&gt; Response:\n  \"\"\"\n  Get relevant contexts for a given search query, using Multi-query retrieval + filtering method.\n  \"\"\"\n  search_query: str = request.args.get('search_query', default='', type=str)\n  course_name: str = request.args.get('course_name', default='', type=str)\n  token_limit: int = request.args.get('token_limit', default=3000, type=int)\n  if search_query == '' or course_name == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'search_query' and 'course_name' must be provided. Search query: `{search_query}`, Course name: `{course_name}`\"\n    )\n\n  posthog = Posthog(sync_mode=True, project_api_key=os.environ['POSTHOG_API_KEY'], host='https://app.posthog.com')\n  posthog.capture('distinct_id_of_the_user',\n                  event='filter_top_contexts_invoked',\n                  properties={\n                      'user_query': search_query,\n                      'course_name': course_name,\n                      'token_limit': token_limit,\n                  })\n\n  ingester = Ingest()\n  found_documents = ingester.getTopContextsWithMQR(search_query, course_name, token_limit)\n  del ingester\n  posthog.shutdown()\n\n  response = jsonify(found_documents)\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.get_stuffed_prompt","title":"<code>get_stuffed_prompt()</code>","text":"<p>Get most relevant contexts for a given search query.</p>"},{"location":"api_reference/#ai_ta_backend.main.get_stuffed_prompt--get-arguments","title":"GET arguments","text":"<p>course name (optional) str     A json response with TBD fields. search_query top_n</p>"},{"location":"api_reference/#ai_ta_backend.main.get_stuffed_prompt--returns","title":"Returns","text":"<p>String</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/get_stuffed_prompt', methods=['GET'])\ndef get_stuffed_prompt() -&gt; Response:\n  \"\"\"Get most relevant contexts for a given search query.\n\n  ## GET arguments\n  course name (optional) str\n      A json response with TBD fields.\n  search_query\n  top_n\n\n  Returns\n  -------\n    String\n\n  \"\"\"\n  course_name: str = request.args.get('course_name', default='', type=str)\n  search_query: str = request.args.get('search_query', default='', type=str)\n  token_limit: int = request.args.get('token_limit', default=-1, type=int)\n  if course_name == '' or search_query == '' or token_limit == -1:\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'course_name', 'search_query', and 'token_limit' must be provided. Course name: `{course_name}`, Search query: `{search_query}`, Token limit: `{token_limit}`\"\n    )\n\n  print(\"In /getTopContexts: \", search_query)\n  if search_query is None:\n    return jsonify({\"error\": \"No parameter `search_query` provided. It is undefined.\"})\n  if token_limit is None:\n    token_limit = 3_000\n  else:\n    token_limit = int(token_limit)\n\n  ingester = Ingest()\n  prompt = ingester.get_stuffed_prompt(search_query, course_name, token_limit)\n  del ingester\n\n  response = jsonify(prompt)\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.index","title":"<code>index()</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>int</code> <p>description. Defaults to 1.</p> required <p>Returns:</p> Name Type Description <code>JSON</code> <code>Response</code> <p>description</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/')\ndef index() -&gt; Response:\n  \"\"\"_summary_\n\n  Args:\n      test (int, optional): _description_. Defaults to 1.\n\n  Returns:\n      JSON: _description_\n  \"\"\"\n  response = jsonify({\"Choo Choo\": \"Welcome to your Flask app \ud83d\ude85\"})\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.ingest","title":"<code>ingest()</code>","text":"<p>Recursively ingests anything from S3 filepath and below.  Pass a s3_paths filepath (not URL) into our S3 bucket.</p> <p>Ingests all files, not just PDFs. </p> <p>Parameters:</p> Name Type Description Default <code>s3_paths</code> <p>str | List[str]</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Response</code> <p>Success or Failure message. Failure message if any failures. TODO: email on failure.</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/ingest', methods=['GET'])\ndef ingest() -&gt; Response:\n  \"\"\"Recursively ingests anything from S3 filepath and below. \n  Pass a s3_paths filepath (not URL) into our S3 bucket.\n\n  Ingests all files, not just PDFs. \n\n  args:\n    s3_paths: str | List[str]\n\n  Returns:\n      str: Success or Failure message. Failure message if any failures. TODO: email on failure.\n  \"\"\"\n  s3_paths: List[str] | str = request.args.get('s3_paths', default='')\n  readable_filename: List[str] | str = request.args.get('readable_filename', default='')\n  course_name: List[str] | str = request.args.get('course_name', default='')\n  base_url: List[str] | str | None = request.args.get('base_url', default=None)\n  url: List[str] | str | None = request.args.get('url', default=None)\n\n  print(\n      f\"In top of /ingest route. course: {course_name}, s3paths: {s3_paths}, readable_filename: {readable_filename}, base_url: {base_url}, url: {url}\"\n  )\n\n  if course_name == '' or s3_paths == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'course_name' and 's3_path' must be provided. Course name: `{course_name}`, S3 path: `{s3_paths}`\"\n    )\n\n  print(\"NUM ACTIVE THREADS (top of /ingest):\", threading.active_count())\n\n  ingester = Ingest()\n  if readable_filename == '':\n    success_fail_dict = ingester.bulk_ingest(s3_paths, course_name, base_url=base_url, url=url)\n  else:\n    success_fail_dict = ingester.bulk_ingest(s3_paths,\n                                             course_name,\n                                             readable_filename=readable_filename,\n                                             base_url=base_url,\n                                             url=url)\n  print(f\"Bottom of /ingest route. success or fail dict: {success_fail_dict}\")\n  del ingester\n\n  response = jsonify(success_fail_dict)\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.ingest_canvas","title":"<code>ingest_canvas()</code>","text":"<p>Ingest course content from Canvas</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/ingestCanvas', methods=['GET'])\ndef ingest_canvas():\n  \"\"\"\n  Ingest course content from Canvas\n  \"\"\"\n  print(\"made it to ingest\")\n  canvas = CanvasAPI()\n  canvas_course_id: str = request.args.get('course_id')\n  course_name: str = request.args.get('course_name')\n\n  # Retrieve the checkbox values from the request and create the content_ingest_dict\n  # Set default values to True if not provided in the request\n  content_ingest_dict = {\n      'files': request.args.get('files', 'true').lower() == 'true',\n      'pages': request.args.get('pages', 'true').lower() == 'true',\n      'modules': request.args.get('modules', 'true').lower() == 'true',\n      'syllabus': request.args.get('syllabus', 'true').lower() == 'true',\n      'assignments': request.args.get('assignments', 'true').lower() == 'true',\n      'discussions': request.args.get('discussions', 'true').lower() == 'true'\n  }\n\n  if canvas_course_id == '' or course_name == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'course_id' and 'course_name' must be provided. course_id: `{canvas_course_id}`, course_name: `{course_name}`\"\n    )\n\n  success_or_failure = canvas.ingest_course_content(canvas_course_id, course_name, content_ingest_dict)\n  response = jsonify({\"outcome\": success_or_failure})\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.ingest_web_text","title":"<code>ingest_web_text()</code>","text":"<p>Ingests web text data provided in the POST request body.</p> Expects JSON data containing <ul> <li>url: The URL of the web text to ingest.</li> <li>base_url: The base URL of the web text to ingest.</li> <li>title: The title of the web text.</li> <li>content: The content of the web text.</li> <li>course_name: The name of the course associated with the web text.</li> </ul> <p>Returns:</p> Name Type Description <code>str</code> <code>Response</code> <p>Success or Failure message. Failure message if any failures. TODO: email on failure.</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/ingest-web-text', methods=['POST'])\ndef ingest_web_text() -&gt; Response:\n  \"\"\"Ingests web text data provided in the POST request body.\n\n  Expects JSON data containing:\n    - url: The URL of the web text to ingest.\n    - base_url: The base URL of the web text to ingest.\n    - title: The title of the web text.\n    - content: The content of the web text.\n    - course_name: The name of the course associated with the web text.\n\n  Returns:\n      str: Success or Failure message. Failure message if any failures. TODO: email on failure.\n  \"\"\"\n  data = request.get_json()\n  url: str = data.get('url', '')\n  base_url: str = data.get('base_url', '')\n  title: str = data.get('title', '')\n  content: str = data.get('content', '')\n  course_name: str = data.get('courseName', '')\n\n  print(f\"In top of /ingest-web-text. course: {course_name}, base_url: {base_url}, url: {url}\")\n\n  if course_name == '' or url == '' or title == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: course_name, url or title. Course name: `{course_name}`, url: `{url}`, content: `{content}`, title: `{title}`\"\n    )\n\n  if content == '':\n    print(f\"Content is empty. Skipping ingestion of {url}\")\n    response = jsonify({\"outcome\": \"success\"})\n    response.headers.add('Access-Control-Allow-Origin', '*')\n    return response\n\n  print(\"NUM ACTIVE THREADS (top of /ingest-web-text):\", threading.active_count())\n\n  ingester = Ingest()\n  success_fail = ingester.ingest_single_web_text(course_name, base_url, url, content, title)\n  del ingester\n\n  print(f\"Bottom of /ingest route. success or fail dict: {success_fail}\")\n\n  response = jsonify(success_fail)\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.mit_download_course","title":"<code>mit_download_course()</code>","text":"<p>Web scraper built for</p> Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/mit-download', methods=['GET'])\ndef mit_download_course() -&gt; Response:\n  \"\"\" Web scraper built for \n  \"\"\"\n  url: str = request.args.get('url', default='', type=str)\n  course_name: str = request.args.get('course_name', default='', type=str)\n  local_dir: str = request.args.get('local_dir', default='', type=str)\n\n  if url == '' or course_name == '' or local_dir == '':\n    # proper web error \"400 Bad request\"\n    abort(\n        400,\n        description=\n        f\"Missing one or more required parameters: 'url', 'course_name', and 'local_dir' must be provided. url: `{url}`, course_name: `{course_name}`, local_dir: `{local_dir}`\"\n    )\n\n  success_fail = mit_course_download(url, course_name, local_dir)\n\n  response = jsonify(success_fail)\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.main.resource_report","title":"<code>resource_report()</code>","text":"<p>Print server resources.</p>"},{"location":"api_reference/#ai_ta_backend.main.resource_report--httpsmanpagesdebianorgbookwormmanpages-devgetrlimit2enhtml","title":"https://manpages.debian.org/bookworm/manpages-dev/getrlimit.2.en.html","text":"Source code in <code>ai_ta_backend/main.py</code> <pre><code>@app.route('/resource-report', methods=['GET'])\ndef resource_report() -&gt; Response:\n  \"\"\"\n  Print server resources.\n  # https://manpages.debian.org/bookworm/manpages-dev/getrlimit.2.en.html\n  \"\"\"\n  import resource\n  from resource import getrusage, RUSAGE_SELF, RUSAGE_CHILDREN\n  import subprocess\n\n  print(\"\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47 &lt;RESOURCE REPORT&gt; \ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\")\n\n  print(\"NUM ACTIVE THREADS (top of /resource-report):\", threading.active_count())\n  try:\n    # result = subprocess.run(['ps', '-u', '$(whoami)', '|', 'wc', '-l'], stdout=subprocess.PIPE)\n    result = subprocess.run('ps -u $(whoami) | wc -l', shell=True, stdout=subprocess.PIPE)\n    print(\"Current active threads: \", result.stdout.decode('utf-8'))\n  except Exception as e:\n    print(\"Error executing ulimit -a: \", e)\n\n  try:\n    with open('/etc/security/limits.conf', 'r') as file:\n      print(\"/etc/security/limits.conf:\\n\", file.read())\n  except Exception as e:\n    print(\"Error reading /etc/security/limits.conf: \", e)\n\n  try:\n    with open('/proc/sys/kernel/threads-max', 'r') as file:\n      print(\"/proc/sys/kernel/threads-max: \", file.read())\n  except Exception as e:\n    print(\"Error reading /proc/sys/kernel/threads-max: \", e)\n\n  # Check container or virtualization platform limits if applicable\n  # This is highly dependent on the specific platform and setup\n  # Here is an example for Docker, adjust as needed for your environment\n  try:\n    result = subprocess.run('docker stats --no-stream', shell=True, stdout=subprocess.PIPE)\n    print(\"Docker stats:\\n\", result.stdout.decode('utf-8'))\n  except Exception as e:\n    print(\"Error getting Docker stats: \", e)\n\n  print(\"RLIMIT_NPROC: \", resource.getrlimit(resource.RLIMIT_NPROC))\n  print(\"RLIMIT_AS (GB): \", [limit / (1024 * 1024 * 1024) for limit in resource.getrlimit(resource.RLIMIT_AS)])\n  print(\"RLIMIT_DATA (GB): \", [limit / (1024 * 1024 * 1024) for limit in resource.getrlimit(resource.RLIMIT_DATA)])\n  print(\"RLIMIT_MEMLOCK (GB): \",\n        [limit / (1024 * 1024 * 1024) for limit in resource.getrlimit(resource.RLIMIT_MEMLOCK)\n        ])  # The maximum address space which may be locked in memory.\n  print(\"RLIMIT_STACK (MB): \", [limit / (1024 * 1024) for limit in resource.getrlimit(resource.RLIMIT_STACK)])\n  print(\"getpagesize (MB): \", resource.getpagesize() / (1024 * 1024))\n\n  print(\"RUSAGE_SELF\", getrusage(RUSAGE_SELF), end=\"\\n\")\n  print(\"RUSAGE_CHILDREN\", getrusage(RUSAGE_CHILDREN), end=\"\\n\")\n\n  try:\n    result = subprocess.run('ulimit -u', shell=True, stdout=subprocess.PIPE)\n    print(\"ulimit -u: \", result.stdout.decode('utf-8'))\n  except Exception as e:\n    print(\"Error executing ulimit -u: \", e)\n\n  try:\n    result = subprocess.run('ulimit -a', shell=True, stdout=subprocess.PIPE)\n    print(f\"ulimit -a:\\n{result.stdout.decode('utf-8')}\")\n  except Exception as e:\n    print(\"Error executing ulimit -a: \", e)\n\n  try:\n    print(\"RUSAGE_THREAD: \", resource.getrlimit(resource.RUSAGE_THREAD))\n  except Exception as e:\n    pass\n    # print(\"Error in RUSAGE_THREAD: \", e)\n\n  print(\"\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46 &lt;/RESOURCE REPORT&gt; \ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\")\n\n  response = jsonify({\"outcome\": \"success\"})\n  response.headers.add('Access-Control-Allow-Origin', '*')\n  return response\n</code></pre>"},{"location":"api_reference/#backend-endpoints","title":"Backend endpoints","text":""},{"location":"api_reference/#database-endpoints-supabase-qdrant","title":"Database endpoints (Supabase, QDrant)","text":""},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest","title":"<code>Ingest</code>","text":"<p>Contains all methods for building and using vector databases.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>class Ingest():\n  \"\"\"\n  Contains all methods for building and using vector databases.\n  \"\"\"\n\n  def __init__(self):\n    \"\"\"\n    Initialize AWS S3, Qdrant, and Supabase.\n    \"\"\"\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    # vector DB\n    self.qdrant_client = QdrantClient(\n        url=os.getenv('QDRANT_URL'),\n        api_key=os.getenv('QDRANT_API_KEY'),\n    )\n\n    self.vectorstore = Qdrant(client=self.qdrant_client,\n                              collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n                              embeddings=OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE))\n\n    # S3\n    self.s3_client = boto3.client(\n        's3',\n        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n    )\n\n    # Create a Supabase client\n    self.supabase_client = supabase.create_client(  # type: ignore\n        supabase_url=os.environ['SUPABASE_URL'], supabase_key=os.environ['SUPABASE_API_KEY'])\n\n    self.llm = AzureChatOpenAI(\n        temperature=0,\n        deployment_name=os.getenv('AZURE_OPENAI_ENGINE'),  #type:ignore\n        openai_api_base=os.getenv('AZURE_OPENAI_ENDPOINT'),  #type:ignore\n        openai_api_key=os.getenv('AZURE_OPENAI_KEY'),  #type:ignore\n        openai_api_version=os.getenv('OPENAI_API_VERSION'),  #type:ignore\n        openai_api_type=OPENAI_API_TYPE)\n\n    self.posthog = Posthog(sync_mode=True,\n                           project_api_key=os.environ['POSTHOG_API_KEY'],\n                           host='https://app.posthog.com')\n\n    return None\n\n  def __del__(self):\n    # Gracefully shutdown the Posthog client -- this was a main cause of dangling threads.\n    # Since I changed Posthog to be sync, no need to shutdown.\n    # try:\n    #   self.posthog.shutdown()\n    # except Exception as e:\n    #   print(\"Failed to shutdown PostHog. Probably fine. Error: \", e)\n    try:\n      self.qdrant_client.close()\n    except Exception as e:\n      print(\"Failed to shutdown Qdrant. Probably fine. Error: \", e)\n    try:\n      del self.supabase_client\n    except Exception as e:\n      print(\"Failed delete supabase_client. Probably fine. Error: \", e)\n    try:\n      del self.s3_client\n    except Exception as e:\n      print(\"Failed to delete s3_client. Probably fine. Error: \", e)\n\n  def bulk_ingest(self, s3_paths: Union[List[str], str], course_name: str, **kwargs) -&gt; Dict[str, List[str]]:\n\n    def _ingest_single(ingest_method: Callable, s3_path, *args, **kwargs):\n      \"\"\"Handle running an arbitrary ingest function for an individual file.\"\"\"\n      # RUN INGEST METHOD\n      ret = ingest_method(s3_path, *args, **kwargs)\n      if ret == \"Success\":\n        success_status['success_ingest'].append(s3_path)\n      else:\n        success_status['failure_ingest'].append(s3_path)\n\n    # \ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47 ADD NEW INGEST METHODS HERE \ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83c\udf89\n    file_ingest_methods = {\n        '.html': self._ingest_html,\n        '.py': self._ingest_single_py,\n        '.pdf': self._ingest_single_pdf,\n        '.txt': self._ingest_single_txt,\n        '.md': self._ingest_single_txt,\n        '.srt': self._ingest_single_srt,\n        '.vtt': self._ingest_single_vtt,\n        '.docx': self._ingest_single_docx,\n        '.ppt': self._ingest_single_ppt,\n        '.pptx': self._ingest_single_ppt,\n        '.xlsx': self._ingest_single_excel,\n        '.xls': self._ingest_single_excel,\n        '.csv': self._ingest_single_csv,\n        '.png': self._ingest_single_image,\n        '.jpg': self._ingest_single_image,\n    }\n\n    # Ingest methods via MIME type (more general than filetype)\n    mimetype_ingest_methods = {\n        'video': self._ingest_single_video,\n        'audio': self._ingest_single_video,\n        'text': self._ingest_single_txt,\n        'image': self._ingest_single_image,\n    }\n    # \ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46 ADD NEW INGEST METHODhe \ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83d\udc46\ud83c\udf89\n\n    print(f\"Top of ingest, Course_name {course_name}. S3 paths {s3_paths}\")\n    success_status = {\"success_ingest\": [], \"failure_ingest\": []}\n    try:\n      if isinstance(s3_paths, str):\n        s3_paths = [s3_paths]\n\n      for s3_path in s3_paths:\n        file_extension = Path(s3_path).suffix\n        with NamedTemporaryFile(suffix=file_extension) as tmpfile:\n          self.s3_client.download_fileobj(Bucket=os.environ['S3_BUCKET_NAME'], Key=s3_path, Fileobj=tmpfile)\n          mime_type = str(mimetypes.guess_type(tmpfile.name, strict=False)[0])\n          mime_category, mime_subcategory = mime_type.split('/')\n\n        if file_extension in file_ingest_methods:\n          # Use specialized functions when possible, fallback to mimetype. Else raise error.\n          ingest_method = file_ingest_methods[file_extension]\n          _ingest_single(ingest_method, s3_path, course_name, **kwargs)\n        elif mime_category in mimetype_ingest_methods:\n          # fallback to MimeType\n          print(\"mime category\", mime_category)\n          ingest_method = mimetype_ingest_methods[mime_category]\n          _ingest_single(ingest_method, s3_path, course_name, **kwargs)\n        else:\n          # No supported ingest... Fallback to attempting utf-8 decoding, otherwise fail.\n          try:\n            self._ingest_single_txt(s3_path, course_name)\n            success_status['success_ingest'].append(s3_path)\n            print(\"\u2705 FALLBACK TO UTF-8 INGEST WAS SUCCESSFUL :) \")\n          except Exception as e:\n            print(\n                f\"We don't have a ingest method for this filetype: {file_extension}. As a last-ditch effort, we tried to ingest the file as utf-8 text, but that failed too. File is unsupported: {s3_path}. UTF-8 ingest error: {e}\"\n            )\n            success_status['failure_ingest'].append(\n                f\"We don't have a ingest method for this filetype: {file_extension} (with generic type {mime_type}), for file: {s3_path}\"\n            )\n\n      return success_status\n    except Exception as e:\n      success_status['failure_ingest'].append(f\"MAJOR ERROR IN /bulk_ingest: Error: {str(e)}\")\n      sentry_sdk.capture_exception(e)\n      return success_status\n\n  def ingest_single_web_text(self, course_name: str, base_url: str, url: str, content: str, title: str):\n    \"\"\"Crawlee integration\n    \"\"\"\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='ingest_single_web_text_invoked',\n                         properties={\n                             'course_name': course_name,\n                             'base_url': base_url,\n                             'url': url,\n                             'content': content,\n                             'title': title\n                         })\n    try:\n      # if not, ingest the text\n      text = [content]\n      metadatas: List[Dict[str, Any]] = [{\n          'course_name': course_name,\n          's3_path': '',\n          'readable_filename': title,\n          'pagenumber': '',\n          'timestamp': '',\n          'url': url,\n          'base_url': base_url,\n      }]\n      self.split_and_upload(texts=text, metadatas=metadatas)\n      self.posthog.capture('distinct_id_of_the_user',\n                           event='ingest_single_web_text_succeeded',\n                           properties={\n                               'course_name': course_name,\n                               'base_url': base_url,\n                               'url': url,\n                               'title': title\n                           })\n\n      return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (web text ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_py(self, s3_path: str, course_name: str, **kwargs):\n    try:\n      file_name = s3_path.split(\"/\")[-1]\n      file_path = \"media/\" + file_name  # download from s3 to local folder for ingest\n\n      self.s3_client.download_file(os.getenv('S3_BUCKET_NAME'), s3_path, file_path)\n\n      loader = PythonLoader(file_path)\n      documents = loader.load()\n\n      texts = [doc.page_content for doc in documents]\n\n      metadatas: List[Dict[str, Any]] = [{\n          'course_name': course_name,\n          's3_path': s3_path,\n          'readable_filename': kwargs.get('readable_filename',\n                                          Path(s3_path).name[37:]),\n          'pagenumber': '',\n          'timestamp': '',\n          'url': '',\n          'base_url': '',\n      } for doc in documents]\n      #print(texts)\n      os.remove(file_path)\n\n      success_or_failure = self.split_and_upload(texts=texts, metadatas=metadatas)\n      print(\"Python ingest: \", success_or_failure)\n      return success_or_failure\n\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (Python ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_vtt(self, s3_path: str, course_name: str, **kwargs):\n    \"\"\"\n    Ingest a single .vtt file from S3.\n    \"\"\"\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        # download from S3 into vtt_tmpfile\n        self.s3_client.download_fileobj(Bucket=os.environ['S3_BUCKET_NAME'], Key=s3_path, Fileobj=tmpfile)\n        loader = TextLoader(tmpfile.name)\n        documents = loader.load()\n        texts = [doc.page_content for doc in documents]\n\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        success_or_failure = self.split_and_upload(texts=texts, metadatas=metadatas)\n        return success_or_failure\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (VTT ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_html(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    print(f\"IN _ingest_html s3_path `{s3_path}` kwargs: {kwargs}\")\n    try:\n      response = self.s3_client.get_object(Bucket=os.environ['S3_BUCKET_NAME'], Key=s3_path)\n      raw_html = response['Body'].read().decode('utf-8')\n\n      soup = BeautifulSoup(raw_html, 'html.parser')\n      title = s3_path.replace(\"courses/\" + course_name, \"\")\n      title = title.replace(\".html\", \"\")\n      title = title.replace(\"_\", \" \")\n      title = title.replace(\"/\", \" \")\n      title = title.strip()\n      title = title[37:]  # removing the uuid prefix\n      text = [soup.get_text()]\n\n      metadata: List[Dict[str, Any]] = [{\n          'course_name': course_name,\n          's3_path': s3_path,\n          'readable_filename': str(title),  # adding str to avoid error: unhashable type 'slice'\n          'url': kwargs.get('url', ''),\n          'base_url': kwargs.get('base_url', ''),\n          'pagenumber': '',\n          'timestamp': '',\n      }]\n\n      success_or_failure = self.split_and_upload(text, metadata)\n      print(f\"_ingest_html: {success_or_failure}\")\n      return success_or_failure\n    except Exception as e:\n      err: str = f\"ERROR IN _ingest_html: {e}\\nTraceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_video(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    \"\"\"\n    Ingest a single video file from S3.\n    \"\"\"\n    print(\"Starting ingest video or audio\")\n    try:\n      # check for file extension\n      file_ext = Path(s3_path).suffix\n      openai.api_key = os.getenv('OPENAI_API_KEY')\n      transcript_list = []\n      with NamedTemporaryFile(suffix=file_ext) as video_tmpfile:\n        # download from S3 into an video tmpfile\n        self.s3_client.download_fileobj(Bucket=os.environ['S3_BUCKET_NAME'], Key=s3_path, Fileobj=video_tmpfile)\n        # extract audio from video tmpfile\n        mp4_version = AudioSegment.from_file(video_tmpfile.name, file_ext[1:])\n\n      # save the extracted audio as a temporary webm file\n      with NamedTemporaryFile(suffix=\".webm\", dir=\"media\", delete=False) as webm_tmpfile:\n        mp4_version.export(webm_tmpfile, format=\"webm\")\n\n      # check file size\n      file_size = os.path.getsize(webm_tmpfile.name)\n      # split the audio into 25MB chunks\n      if file_size &gt; 26214400:\n        # load the webm file into audio object\n        full_audio = AudioSegment.from_file(webm_tmpfile.name, \"webm\")\n        file_count = file_size // 26214400 + 1\n        split_segment = 35 * 60 * 1000\n        start = 0\n        count = 0\n\n        while count &lt; file_count:\n          with NamedTemporaryFile(suffix=\".webm\", dir=\"media\", delete=False) as split_tmp:\n            if count == file_count - 1:\n              # last segment\n              audio_chunk = full_audio[start:]\n            else:\n              audio_chunk = full_audio[start:split_segment]\n\n            audio_chunk.export(split_tmp.name, format=\"webm\")\n\n            # transcribe the split file and store the text in dictionary\n            with open(split_tmp.name, \"rb\") as f:\n              transcript = openai.Audio.transcribe(\"whisper-1\", f)\n            transcript_list.append(transcript['text'])  # type: ignore\n          start += split_segment\n          split_segment += split_segment\n          count += 1\n          os.remove(split_tmp.name)\n      else:\n        # transcribe the full audio\n        with open(webm_tmpfile.name, \"rb\") as f:\n          transcript = openai.Audio.transcribe(\"whisper-1\", f)\n        transcript_list.append(transcript['text'])  # type: ignore\n\n      os.remove(webm_tmpfile.name)\n\n      text = [txt for txt in transcript_list]\n      metadatas: List[Dict[str, Any]] = [{\n          'course_name': course_name,\n          's3_path': s3_path,\n          'readable_filename': kwargs.get('readable_filename',\n                                          Path(s3_path).name[37:]),\n          'pagenumber': '',\n          'timestamp': text.index(txt),\n          'url': '',\n          'base_url': '',\n      } for txt in text]\n\n      self.split_and_upload(texts=text, metadatas=metadatas)\n      return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (VIDEO ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_docx(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        self.s3_client.download_fileobj(Bucket=os.getenv('S3_BUCKET_NAME'), Key=s3_path, Fileobj=tmpfile)\n\n        loader = Docx2txtLoader(tmpfile.name)\n        documents = loader.load()\n\n        texts = [doc.page_content for doc in documents]\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        self.split_and_upload(texts=texts, metadatas=metadatas)\n        return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (DOCX ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_srt(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        # download from S3 into pdf_tmpfile\n        self.s3_client.download_fileobj(Bucket=os.getenv('S3_BUCKET_NAME'), Key=s3_path, Fileobj=tmpfile)\n\n        loader = SRTLoader(tmpfile.name)\n        documents = loader.load()\n\n        texts = [doc.page_content for doc in documents]\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        self.split_and_upload(texts=texts, metadatas=metadatas)\n        return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (SRT ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_excel(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        # download from S3 into pdf_tmpfile\n        self.s3_client.download_fileobj(Bucket=os.getenv('S3_BUCKET_NAME'), Key=s3_path, Fileobj=tmpfile)\n\n        loader = UnstructuredExcelLoader(tmpfile.name, mode=\"elements\")\n        # loader = SRTLoader(tmpfile.name)\n        documents = loader.load()\n\n        texts = [doc.page_content for doc in documents]\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        self.split_and_upload(texts=texts, metadatas=metadatas)\n        return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (Excel/xlsx ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_image(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        # download from S3 into pdf_tmpfile\n        self.s3_client.download_fileobj(Bucket=os.getenv('S3_BUCKET_NAME'), Key=s3_path, Fileobj=tmpfile)\n        \"\"\"\n        # Unstructured image loader makes the install too large (700MB --&gt; 6GB. 3min -&gt; 12 min build times). AND nobody uses it.\n        # The \"hi_res\" strategy will identify the layout of the document using detectron2. \"ocr_only\" uses pdfminer.six. https://unstructured-io.github.io/unstructured/core/partition.html#partition-image\n        loader = UnstructuredImageLoader(tmpfile.name, unstructured_kwargs={'strategy': \"ocr_only\"})\n        documents = loader.load()\n        \"\"\"\n\n        res_str = pytesseract.image_to_string(Image.open(tmpfile.name))\n        print(\"IMAGE PARSING RESULT:\", res_str)\n        documents = [Document(page_content=res_str)]\n\n        texts = [doc.page_content for doc in documents]\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        self.split_and_upload(texts=texts, metadatas=metadatas)\n        return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (png/jpg ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_csv(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        # download from S3 into pdf_tmpfile\n        self.s3_client.download_fileobj(Bucket=os.getenv('S3_BUCKET_NAME'), Key=s3_path, Fileobj=tmpfile)\n\n        loader = CSVLoader(file_path=tmpfile.name)\n        documents = loader.load()\n\n        texts = [doc.page_content for doc in documents]\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        self.split_and_upload(texts=texts, metadatas=metadatas)\n        return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (CSV ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_pdf(self, s3_path: str, course_name: str, **kwargs):\n    \"\"\"\n    Both OCR the PDF. And grab the first image as a PNG.\n      LangChain `Documents` have .metadata and .page_content attributes.\n    Be sure to use TemporaryFile() to avoid memory leaks!\n    \"\"\"\n    print(\"IN PDF ingest: s3_path: \", s3_path, \"and kwargs:\", kwargs)\n\n    try:\n      with NamedTemporaryFile() as pdf_tmpfile:\n        # download from S3 into pdf_tmpfile\n        self.s3_client.download_fileobj(Bucket=os.getenv('S3_BUCKET_NAME'), Key=s3_path, Fileobj=pdf_tmpfile)\n        ### READ OCR of PDF\n        doc = fitz.open(pdf_tmpfile.name)  # type: ignore\n\n        # improve quality of the image\n        zoom_x = 2.0  # horizontal zoom\n        zoom_y = 2.0  # vertical zoom\n        mat = fitz.Matrix(zoom_x, zoom_y)  # zoom factor 2 in each dimension\n\n        pdf_pages_OCRed: List[Dict] = []\n        for i, page in enumerate(doc):  # type: ignore\n\n          # UPLOAD FIRST PAGE IMAGE to S3\n          if i == 0:\n            with NamedTemporaryFile(suffix=\".png\") as first_page_png:\n              pix = page.get_pixmap(matrix=mat)\n              pix.save(first_page_png)  # store image as a PNG\n\n              s3_upload_path = str(Path(s3_path)).rsplit('.pdf')[0] + \"-pg1-thumb.png\"\n              first_page_png.seek(0)  # Seek the file pointer back to the beginning\n              with open(first_page_png.name, 'rb') as f:\n                print(\"Uploading image png to S3\")\n                self.s3_client.upload_fileobj(f, os.getenv('S3_BUCKET_NAME'), s3_upload_path)\n\n          # Extract text\n          text = page.get_text().encode(\"utf8\").decode(\"utf8\", errors='ignore')  # get plain text (is in UTF-8)\n          pdf_pages_OCRed.append(dict(text=text, page_number=i, readable_filename=Path(s3_path).name[37:]))\n\n        metadatas: List[Dict[str, Any]] = [\n            {\n                'course_name': course_name,\n                's3_path': s3_path,\n                'pagenumber': page['page_number'] + 1,  # +1 for human indexing\n                'timestamp': '',\n                'readable_filename': kwargs.get('readable_filename', page['readable_filename']),\n                'url': kwargs.get('url', ''),\n                'base_url': kwargs.get('base_url', ''),\n            } for page in pdf_pages_OCRed\n        ]\n        pdf_texts = [page['text'] for page in pdf_pages_OCRed]\n\n        success_or_failure = self.split_and_upload(texts=pdf_texts, metadatas=metadatas)\n        return success_or_failure\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (PDF ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n    return \"Success\"\n\n  def _ingest_single_txt(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    \"\"\"Ingest a single .txt or .md file from S3.\n    Args:\n        s3_path (str): A path to a .txt file in S3\n        course_name (str): The name of the course\n    Returns:\n        str: \"Success\" or an error message\n    \"\"\"\n    print(\"In text ingest\")\n    try:\n      # NOTE: slightly different method for .txt files, no need for download. It's part of the 'body'\n      response = self.s3_client.get_object(Bucket=os.environ['S3_BUCKET_NAME'], Key=s3_path)\n      print(\"s3 Resonse:\", response)\n      text = response['Body'].read().decode('utf-8')\n      print(\"Text from s3:\", text)\n      text = [text]\n\n      metadatas: List[Dict[str, Any]] = [{\n          'course_name': course_name,\n          's3_path': s3_path,\n          'readable_filename': kwargs.get('readable_filename',\n                                          Path(s3_path).name[37:]),\n          'pagenumber': '',\n          'timestamp': '',\n          'url': '',\n          'base_url': '',\n      }]\n      print(\"Prior to ingest\", metadatas)\n\n      success_or_failure = self.split_and_upload(texts=text, metadatas=metadatas)\n      return success_or_failure\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (TXT ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def _ingest_single_ppt(self, s3_path: str, course_name: str, **kwargs) -&gt; str:\n    \"\"\"\n    Ingest a single .ppt or .pptx file from S3.\n    \"\"\"\n    try:\n      with NamedTemporaryFile() as tmpfile:\n        # download from S3 into pdf_tmpfile\n        #print(\"in ingest PPTX\")\n        self.s3_client.download_fileobj(Bucket=os.environ['S3_BUCKET_NAME'], Key=s3_path, Fileobj=tmpfile)\n\n        loader = UnstructuredPowerPointLoader(tmpfile.name)\n        documents = loader.load()\n\n        texts = [doc.page_content for doc in documents]\n        metadatas: List[Dict[str, Any]] = [{\n            'course_name': course_name,\n            's3_path': s3_path,\n            'readable_filename': kwargs.get('readable_filename',\n                                            Path(s3_path).name[37:]),\n            'pagenumber': '',\n            'timestamp': '',\n            'url': '',\n            'base_url': '',\n        } for doc in documents]\n\n        self.split_and_upload(texts=texts, metadatas=metadatas)\n        return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (PPTX ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n      )\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def list_files_recursively(self, bucket, prefix):\n    all_files = []\n    continuation_token = None\n\n    while True:\n      list_objects_kwargs = {\n          'Bucket': bucket,\n          'Prefix': prefix,\n      }\n      if continuation_token:\n        list_objects_kwargs['ContinuationToken'] = continuation_token\n\n      response = self.s3_client.list_objects_v2(**list_objects_kwargs)\n\n      if 'Contents' in response:\n        for obj in response['Contents']:\n          all_files.append(obj['Key'])\n\n      if response['IsTruncated']:\n        continuation_token = response['NextContinuationToken']\n      else:\n        break\n\n    return all_files\n\n  def ingest_coursera(self, coursera_course_name: str, course_name: str) -&gt; str:\n    \"\"\" Download all the files from a coursera course and ingest them.\n\n    1. Download the coursera content.\n    2. Upload to S3 (so users can view it)\n    3. Run everything through the ingest_bulk method.\n\n    Args:\n        coursera_course_name (str): The name of the coursera course.\n        course_name (str): The name of the course in our system.\n\n    Returns:\n        _type_: Success or error message.\n    \"\"\"\n    certificate = \"-ca 'FVhVoDp5cb-ZaoRr5nNJLYbyjCLz8cGvaXzizqNlQEBsG5wSq7AHScZGAGfC1nI0ehXFvWy1NG8dyuIBF7DLMA.X3cXsDvHcOmSdo3Fyvg27Q.qyGfoo0GOHosTVoSMFy-gc24B-_BIxJtqblTzN5xQWT3hSntTR1DMPgPQKQmfZh_40UaV8oZKKiF15HtZBaLHWLbpEpAgTg3KiTiU1WSdUWueo92tnhz-lcLeLmCQE2y3XpijaN6G4mmgznLGVsVLXb-P3Cibzz0aVeT_lWIJNrCsXrTFh2HzFEhC4FxfTVqS6cRsKVskPpSu8D9EuCQUwJoOJHP_GvcME9-RISBhi46p-Z1IQZAC4qHPDhthIJG4bJqpq8-ZClRL3DFGqOfaiu5y415LJcH--PRRKTBnP7fNWPKhcEK2xoYQLr9RxBVL3pzVPEFyTYtGg6hFIdJcjKOU11AXAnQ-Kw-Gb_wXiHmu63veM6T8N2dEkdqygMre_xMDT5NVaP3xrPbA4eAQjl9yov4tyX4AQWMaCS5OCbGTpMTq2Y4L0Mbz93MHrblM2JL_cBYa59bq7DFK1IgzmOjFhNG266mQlC9juNcEhc'\"\n    always_use_flags = \"-u kastanvday@gmail.com -p hSBsLaF5YM469# --ignore-formats mp4 --subtitle-language en --path ./coursera-dl\"\n\n    try:\n      subprocess.run(\n          f\"coursera-dl {always_use_flags} {certificate} {coursera_course_name}\",\n          check=True,\n          shell=True,  # nosec -- reasonable bandit error suppression\n          stdout=subprocess.PIPE,\n          stderr=subprocess.PIPE)  # capture_output=True,\n      dl_results_path = os.path.join('coursera-dl', coursera_course_name)\n      s3_paths: Union[List, None] = upload_data_files_to_s3(course_name, dl_results_path)\n\n      if s3_paths is None:\n        return \"Error: No files found in the coursera-dl directory\"\n\n      print(\"starting bulk ingest\")\n      start_time = time.monotonic()\n      self.bulk_ingest(s3_paths, course_name)\n      print(\"completed bulk ingest\")\n      print(f\"\u23f0 Runtime: {(time.monotonic() - start_time):.2f} seconds\")\n\n      # Cleanup the coursera downloads\n      shutil.rmtree(dl_results_path)\n\n      return \"Success\"\n    except Exception as e:\n      err: str = f\"Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      return err\n\n  def ingest_github(self, github_url: str, course_name: str) -&gt; str:\n    \"\"\"\n    Clones the given GitHub URL and uses Langchain to load data.\n    1. Clone the repo\n    2. Use Langchain to load the data\n    3. Pass to split_and_upload()\n    Args:\n        github_url (str): The Github Repo URL to be ingested.\n        course_name (str): The name of the course in our system.\n\n    Returns:\n        _type_: Success or error message.\n    \"\"\"\n    try:\n      repo_path = \"media/cloned_repo\"\n      repo = Repo.clone_from(github_url, to_path=repo_path, depth=1, clone_submodules=False)\n      branch = repo.head.reference\n\n      loader = GitLoader(repo_path=\"media/cloned_repo\", branch=str(branch))\n      data = loader.load()\n      shutil.rmtree(\"media/cloned_repo\")\n      # create metadata for each file in data\n\n      for doc in data:\n        texts = doc.page_content\n        metadatas: Dict[str, Any] = {\n            'course_name': course_name,\n            's3_path': '',\n            'readable_filename': doc.metadata['file_name'],\n            'url': f\"{github_url}/blob/main/{doc.metadata['file_path']}\",\n            'pagenumber': '',\n            'timestamp': '',\n        }\n        self.split_and_upload(texts=[texts], metadatas=[metadatas])\n      return \"Success\"\n    except Exception as e:\n      err = f\"\u274c\u274c Error in (GITHUB ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n{traceback.format_exc()}\"\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def split_and_upload(self, texts: List[str], metadatas: List[Dict[str, Any]]):\n    \"\"\" This is usually the last step of document ingest. Chunk &amp; upload to Qdrant (and Supabase.. todo).\n    Takes in Text and Metadata (from Langchain doc loaders) and splits / uploads to Qdrant.\n\n    good examples here: https://langchain.readthedocs.io/en/latest/modules/utils/combine_docs_examples/textsplitter.html\n\n    Args:\n        texts (List[str]): _description_\n        metadatas (List[Dict[str, Any]]): _description_\n    \"\"\"\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='split_and_upload_invoked',\n                         properties={\n                             'course_name': metadatas[0].get('course_name', None),\n                             's3_path': metadatas[0].get('s3_path', None),\n                             'readable_filename': metadatas[0].get('readable_filename', None),\n                             'url': metadatas[0].get('url', None),\n                             'base_url': metadatas[0].get('base_url', None),\n                         })\n\n    print(\"In split and upload\")\n    print(f\"metadatas: {metadatas}\")\n    print(f\"Texts: {texts}\")\n    assert len(texts) == len(\n        metadatas\n    ), f'must have equal number of text strings and metadata dicts. len(texts) is {len(texts)}. len(metadatas) is {len(metadatas)}'\n\n    try:\n      text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n          chunk_size=1000,\n          chunk_overlap=150,\n          separators=[\n              \"\\n\\n\", \"\\n\", \". \", \" \", \"\"\n          ]  # try to split on paragraphs... fallback to sentences, then chars, ensure we always fit in context window\n      )\n      contexts: List[Document] = text_splitter.create_documents(texts=texts, metadatas=metadatas)\n      input_texts = [{'input': context.page_content, 'model': 'text-embedding-ada-002'} for context in contexts]\n\n      # check for duplicates\n      is_duplicate = self.check_for_duplicates(input_texts, metadatas)\n      if is_duplicate:\n        self.posthog.capture('distinct_id_of_the_user',\n                             event='split_and_upload_succeeded',\n                             properties={\n                                 'course_name': metadatas[0].get('course_name', None),\n                                 's3_path': metadatas[0].get('s3_path', None),\n                                 'readable_filename': metadatas[0].get('readable_filename', None),\n                                 'url': metadatas[0].get('url', None),\n                                 'base_url': metadatas[0].get('base_url', None),\n                                 'is_duplicate': True,\n                             })\n        return \"Success\"\n\n      # adding chunk index to metadata for parent doc retrieval\n      for i, context in enumerate(contexts):\n        context.metadata['chunk_index'] = i\n\n      oai = OpenAIAPIProcessor(\n          input_prompts_list=input_texts,\n          request_url='https://api.openai.com/v1/embeddings',\n          api_key=os.getenv('VLADS_OPENAI_KEY'),\n          # request_url='https://uiuc-chat-canada-east.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15',\n          # api_key=os.getenv('AZURE_OPENAI_KEY'),\n          max_requests_per_minute=5_000,\n          max_tokens_per_minute=300_000,\n          max_attempts=20,\n          logging_level=logging.INFO,\n          token_encoding_name='cl100k_base')  # nosec -- reasonable bandit error suppression\n      asyncio.run(oai.process_api_requests_from_file())\n      # parse results into dict of shape page_content -&gt; embedding\n      embeddings_dict: dict[str, List[float]] = {\n          item[0]['input']: item[1]['data'][0]['embedding'] for item in oai.results\n      }\n\n      ### BULK upload to Qdrant ###\n      vectors: list[PointStruct] = []\n      for context in contexts:\n        # !DONE: Updated the payload so each key is top level (no more payload.metadata.course_name. Instead, use payload.course_name), great for creating indexes.\n        upload_metadata = {**context.metadata, \"page_content\": context.page_content}\n        vectors.append(\n            PointStruct(id=str(uuid.uuid4()), vector=embeddings_dict[context.page_content], payload=upload_metadata))\n\n      self.qdrant_client.upsert(\n          collection_name=os.environ['QDRANT_COLLECTION_NAME'],  # type: ignore\n          points=vectors  # type: ignore\n      )\n      ### Supabase SQL ###\n      contexts_for_supa = [{\n          \"text\": context.page_content,\n          \"pagenumber\": context.metadata.get('pagenumber'),\n          \"timestamp\": context.metadata.get('timestamp'),\n          \"chunk_index\": context.metadata.get('chunk_index'),\n          \"embedding\": embeddings_dict[context.page_content]\n      } for context in contexts]\n\n      document = {\n          \"course_name\": contexts[0].metadata.get('course_name'),\n          \"s3_path\": contexts[0].metadata.get('s3_path'),\n          \"readable_filename\": contexts[0].metadata.get('readable_filename'),\n          \"url\": contexts[0].metadata.get('url'),\n          \"base_url\": contexts[0].metadata.get('base_url'),\n          \"contexts\": contexts_for_supa,\n      }\n\n      response = self.supabase_client.table(\n          os.getenv('NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE')).insert(document).execute()  # type: ignore\n\n      # add to Nomic document map\n      if len(response.data) &gt; 0:\n        inserted_data = response.data[0]\n        res = log_to_document_map(inserted_data)\n\n      self.posthog.capture('distinct_id_of_the_user',\n                           event='split_and_upload_succeeded',\n                           properties={\n                               'course_name': metadatas[0].get('course_name', None),\n                               's3_path': metadatas[0].get('s3_path', None),\n                               'readable_filename': metadatas[0].get('readable_filename', None),\n                               'url': metadatas[0].get('url', None),\n                               'base_url': metadatas[0].get('base_url', None),\n                           })\n      print(\"successful END OF split_and_upload\")\n      return \"Success\"\n    except Exception as e:\n      err: str = f\"ERROR IN split_and_upload(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def delete_entire_course(self, course_name: str):\n    \"\"\"Delete entire course.\n\n    Delete materials from S3, Supabase SQL, Vercel KV, and QDrant vector DB\n    Args:\n        course_name (str): _description_\n    \"\"\"\n    print(f\"Deleting entire course: {course_name}\")\n    try:\n      # Delete file from S3\n      print(\"Deleting from S3\")\n      objects_to_delete = self.s3_client.list_objects(Bucket=os.getenv('S3_BUCKET_NAME'),\n                                                      Prefix=f'courses/{course_name}/')\n      for object in objects_to_delete['Contents']:\n        self.s3_client.delete_object(Bucket=os.getenv('S3_BUCKET_NAME'), Key=object['Key'])\n    except Exception as e:\n      err: str = f\"ERROR IN delete_entire_course(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      pass\n\n    try:\n      # Delete from Qdrant\n      # docs for nested keys: https://qdrant.tech/documentation/concepts/filtering/#nested-key\n      # Qdrant \"points\" look like this: Record(id='000295ca-bd28-ac4a-6f8d-c245f7377f90', payload={'metadata': {'course_name': 'zotero-extreme', 'pagenumber_or_timestamp': 15, 'readable_filename': 'Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf', 's3_path': 'courses/zotero-extreme/Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf'}, 'page_content': '18  \\nDunlosky et al.\\n3.3 Effects in representative educational contexts. Sev-\\neral of the large summarization-training studies have been \\nconducted in regular classrooms, indicating the feasibility of \\ndoing so. For example, the study by A. King (1992) took place \\nin the context of a remedial study-skills course for undergrad-\\nuates, and the study by Rinehart et al. (1986) took place in \\nsixth-grade classrooms, with the instruction led by students \\nregular teachers. In these and other cases, students benefited \\nfrom the classroom training. We suspect it may actually be \\nmore feasible to conduct these kinds of training studies in \\nclassrooms than in the laboratory, given the nature of the time \\ncommitment for students. Even some of the studies that did \\nnot involve training were conducted outside the laboratory; for \\nexample, in the Bednall and Kehoe (2011) study on learning \\nabout logical fallacies from Web modules (see data in Table 3), \\nthe modules were actually completed as a homework assign-\\nment. Overall, benefits can be observed in classroom settings; \\nthe real constraint is whether students have the skill to suc-\\ncessfully summarize, not whether summarization occurs in the \\nlab or the classroom.\\n3.4 Issues for implementation. Summarization would be \\nfeasible for undergraduates or other learners who already \\nknow how to summarize. For these students, summarization \\nwould constitute an easy-to-implement technique that would \\nnot take a lot of time to complete or understand. The only \\nconcern would be whether these students might be better \\nserved by some other strategy, but certainly summarization \\nwould be better than the study strategies students typically \\nfavor, such as highlighting and rereading (as we discuss in the \\nsections on those strategies below). A trickier issue would \\nconcern implementing the strategy with students who are not \\nskilled summarizers. Relatively intensive training programs \\nare required for middle school students or learners with learn-\\ning disabilities to benefit from summarization. Such efforts \\nare not misplaced; training has been shown to benefit perfor-\\nmance on a range of measures, although the training proce-\\ndures do raise practical issues (e.g., Gajria &amp; Salvia, 1992: \\n6.511 hours of training used for sixth through ninth graders \\nwith learning disabilities; Malone &amp; Mastropieri, 1991: 2 \\ndays of training used for middle school students with learning \\ndisabilities; Rinehart et al., 1986: 4550 minutes of instruc-\\ntion per day for 5 days used for sixth graders). Of course, \\ninstructors may want students to summarize material because \\nsummarization itself is a goal, not because they plan to use \\nsummarization as a study technique, and that goal may merit \\nthe efforts of training.\\nHowever, if the goal is to use summarization as a study \\ntechnique, our question is whether training students would be \\nworth the amount of time it would take, both in terms of the \\ntime required on the part of the instructor and in terms of the \\ntime taken away from students other activities. For instance, \\nin terms of efficacy, summarization tends to fall in the middle \\nof the pack when compared to other techniques. In direct \\ncomparisons, it was sometimes more useful than rereading \\n(Rewey, Dansereau, &amp; Peel, 1991) and was as useful as note-\\ntaking (e.g., Bretzing &amp; Kulhavy, 1979) but was less powerful \\nthan generating explanations (e.g., Bednall &amp; Kehoe, 2011) or \\nself-questioning (A. King, 1992).\\n3.5 Summarization: Overall assessment. On the basis of the \\navailable evidence, we rate summarization as low utility. It can \\nbe an effective learning strategy for learners who are already \\nskilled at summarizing; however, many learners (including \\nchildren, high school students, and even some undergraduates) \\nwill require extensive training, which makes this strategy less \\nfeasible. Our enthusiasm is further dampened by mixed find-\\nings regarding which tasks summarization actually helps. \\nAlthough summarization has been examined with a wide \\nrange of text materials, many researchers have pointed to fac-\\ntors of these texts that seem likely to moderate the effects of \\nsummarization (e.g'}, vector=None),\n      print(\"deleting from qdrant\")\n      self.qdrant_client.delete(\n          collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n          points_selector=models.Filter(must=[\n              models.FieldCondition(\n                  key=\"course_name\",\n                  match=models.MatchValue(value=course_name),\n              ),\n          ]),\n      )\n    except Exception as e:\n      err: str = f\"ERROR IN delete_entire_course(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      pass\n\n    try:\n      # Delete from Supabase\n      print(\"deleting from supabase\")\n      response = self.supabase_client.from_(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).delete().eq(\n          'course_name', course_name).execute()\n      print(\"supabase response: \", response)\n      return \"Success\"\n    except Exception as e:\n      err: str = f\"ERROR IN delete_entire_course(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n    # todo: delete from Vercel KV to fully make the coure not exist. Last db to delete from (as of now, Aug 15)\n\n  def delete_data(self, course_name: str, s3_path: str, source_url: str):\n    \"\"\"Delete file from S3, Qdrant, and Supabase.\"\"\"\n    print(f\"Deleting {s3_path} from S3, Qdrant, and Supabase for course {course_name}\")\n    # add delete from doc map logic here\n    try:\n      # Delete file from S3\n      bucket_name = os.getenv('S3_BUCKET_NAME')\n\n      # Delete files by S3 path\n      if s3_path:\n        try:\n          self.s3_client.delete_object(Bucket=bucket_name, Key=s3_path)\n        except Exception as e:\n          print(\"Error in deleting file from s3:\", e)\n          sentry_sdk.capture_exception(e)\n        # Delete from Qdrant\n        # docs for nested keys: https://qdrant.tech/documentation/concepts/filtering/#nested-key\n        # Qdrant \"points\" look like this: Record(id='000295ca-bd28-ac4a-6f8d-c245f7377f90', payload={'metadata': {'course_name': 'zotero-extreme', 'pagenumber_or_timestamp': 15, 'readable_filename': 'Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf', 's3_path': 'courses/zotero-extreme/Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf'}, 'page_content': '18  \\nDunlosky et al.\\n3.3 Effects in representative educational contexts. Sev-\\neral of the large summarization-training studies have been \\nconducted in regular classrooms, indicating the feasibility of \\ndoing so. For example, the study by A. King (1992) took place \\nin the context of a remedial study-skills course for undergrad-\\nuates, and the study by Rinehart et al. (1986) took place in \\nsixth-grade classrooms, with the instruction led by students \\nregular teachers. In these and other cases, students benefited \\nfrom the classroom training. We suspect it may actually be \\nmore feasible to conduct these kinds of training  ...\n        try:\n          self.qdrant_client.delete(\n              collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n              points_selector=models.Filter(must=[\n                  models.FieldCondition(\n                      key=\"s3_path\",\n                      match=models.MatchValue(value=s3_path),\n                  ),\n              ]),\n          )\n        except Exception as e:\n          if \"timed out\" in str(e):\n            # Timed out is fine. Still deletes.\n            # https://github.com/qdrant/qdrant/issues/3654#issuecomment-1955074525\n            pass\n          else:\n            print(\"Error in deleting file from Qdrant:\", e)\n            sentry_sdk.capture_exception(e)\n        try:\n          # delete from Nomic\n          response = self.supabase_client.from_(\n              os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).select(\"id, s3_path, contexts\").eq(\n                  's3_path', s3_path).eq('course_name', course_name).execute()\n          data = response.data[0]  #single record fetched\n          nomic_ids_to_delete = []\n          context_count = len(data['contexts'])\n          for i in range(1, context_count + 1):\n            nomic_ids_to_delete.append(str(data['id']) + \"_\" + str(i))\n\n          # delete from Nomic\n          res = delete_from_document_map(course_name, nomic_ids_to_delete)\n        except Exception as e:\n          print(\"Error in deleting file from Nomic:\", e)\n          sentry_sdk.capture_exception(e)\n\n        try:\n          self.supabase_client.from_(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).delete().eq(\n              's3_path', s3_path).eq('course_name', course_name).execute()\n        except Exception as e:\n          print(\"Error in deleting file from supabase:\", e)\n          sentry_sdk.capture_exception(e)\n\n      # Delete files by their URL identifier\n      elif source_url:\n        try:\n          # Delete from Qdrant\n          self.qdrant_client.delete(\n              collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n              points_selector=models.Filter(must=[\n                  models.FieldCondition(\n                      key=\"url\",\n                      match=models.MatchValue(value=source_url),\n                  ),\n              ]),\n          )\n        except Exception as e:\n          if \"timed out\" in str(e):\n            # Timed out is fine. Still deletes.\n            # https://github.com/qdrant/qdrant/issues/3654#issuecomment-1955074525\n            pass\n          else:\n            print(\"Error in deleting file from Qdrant:\", e)\n            sentry_sdk.capture_exception(e)\n        try:\n          # delete from Nomic\n          response = self.supabase_client.from_(\n              os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).select(\"id, url, contexts\").eq(\n                  'url', source_url).eq('course_name', course_name).execute()\n          data = response.data[0]  #single record fetched\n          nomic_ids_to_delete = []\n          context_count = len(data['contexts'])\n          for i in range(1, context_count + 1):\n            nomic_ids_to_delete.append(str(data['id']) + \"_\" + str(i))\n\n          # delete from Nomic\n          res = delete_from_document_map(course_name, nomic_ids_to_delete)\n        except Exception as e:\n          print(\"Error in deleting file from Nomic:\", e)\n          sentry_sdk.capture_exception(e)\n\n        try:\n          # delete from Supabase\n          self.supabase_client.from_(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).delete().eq(\n              'url', source_url).eq('course_name', course_name).execute()\n        except Exception as e:\n          print(\"Error in deleting file from supabase:\", e)\n          sentry_sdk.capture_exception(e)\n\n      # Delete from Supabase\n      return \"Success\"\n    except Exception as e:\n      err: str = f\"ERROR IN delete_data: Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def getAll(\n      self,\n      course_name: str,\n  ):\n    \"\"\"Get all course materials based on course name.\n    Args:\n        course_name (as uploaded on supabase)\n    Returns:\n        list of dictionaries with distinct s3 path, readable_filename and course_name, url, base_url.\n    \"\"\"\n\n    response = self.supabase_client.table(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).select(\n        'course_name, s3_path, readable_filename, url, base_url').eq('course_name', course_name).execute()\n\n    data = response.data\n    unique_combinations = set()\n    distinct_dicts = []\n\n    for item in data:\n      combination = (item['s3_path'], item['readable_filename'], item['course_name'], item['url'], item['base_url'])\n      if combination not in unique_combinations:\n        unique_combinations.add(combination)\n        distinct_dicts.append(item)\n\n    return distinct_dicts\n\n  def vector_search(self, search_query, course_name):\n    top_n = 80\n    # EMBED\n    openai_start_time = time.monotonic()\n    o = OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE)\n    user_query_embedding = o.embed_query(search_query)\n    openai_embedding_latency = time.monotonic() - openai_start_time\n\n    # SEARCH\n    myfilter = models.Filter(must=[\n        models.FieldCondition(key='course_name', match=models.MatchValue(value=course_name)),\n    ])\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='vector_search_invoked',\n                         properties={\n                             'user_query': search_query,\n                             'course_name': course_name,\n                         })\n    qdrant_start_time = time.monotonic()\n    search_results = self.qdrant_client.search(\n        collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n        query_filter=myfilter,\n        with_vectors=False,\n        query_vector=user_query_embedding,\n        limit=top_n,  # Return n closest points\n\n        # In a system with high disk latency, the re-scoring step may become a bottleneck: https://qdrant.tech/documentation/guides/quantization/\n        search_params=models.SearchParams(quantization=models.QuantizationSearchParams(rescore=False)))\n\n    found_docs: list[Document] = []\n    for d in search_results:\n      try:\n        metadata = d.payload\n        page_content = metadata['page_content']\n        del metadata['page_content']\n        if \"pagenumber\" not in metadata.keys() and \"pagenumber_or_timestamp\" in metadata.keys():  # type: ignore\n          # aiding in the database migration...\n          metadata[\"pagenumber\"] = metadata[\"pagenumber_or_timestamp\"]  # type: ignore\n\n        found_docs.append(Document(page_content=page_content, metadata=metadata))  # type: ignore\n      except Exception as e:\n        print(f\"Error in vector_search(), for course: `{course_name}`. Error: {e}\")\n        sentry_sdk.capture_exception(e)\n\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='vector_search_succeded',\n                         properties={\n                             'user_query': search_query,\n                             'course_name': course_name,\n                             'qdrant_latency_sec': time.monotonic() - qdrant_start_time,\n                             'openai_embedding_latency_sec': openai_embedding_latency\n                         })\n    # print(\"found_docs\", found_docs)\n    return found_docs\n\n  def getTopContexts(self, search_query: str, course_name: str, token_limit: int = 4_000) -&gt; Union[List[Dict], str]:\n    \"\"\"Here's a summary of the work.\n\n    /GET arguments\n      course name (optional) str: A json response with TBD fields.\n\n    Returns\n      JSON: A json response with TBD fields. See main.py:getTopContexts docs.\n      or\n      String: An error message with traceback.\n    \"\"\"\n    try:\n      start_time_overall = time.monotonic()\n\n      found_docs: list[Document] = self.vector_search(search_query=search_query, course_name=course_name)\n\n      pre_prompt = \"Please answer the following question. Use the context below, called your documents, only if it's helpful and don't use parts that are very irrelevant. It's good to quote from your documents directly, when you do always use Markdown footnotes for citations. Use react-markdown superscript to number the sources at the end of sentences (1, 2, 3...) and use react-markdown Footnotes to list the full document names for each number. Use ReactMarkdown aka 'react-markdown' formatting for super script citations, use semi-formal style. Feel free to say you don't know. \\nHere's a few passages of the high quality documents:\\n\"\n      # count tokens at start and end, then also count each context.\n      token_counter, _ = count_tokens_and_cost(pre_prompt + '\\n\\nNow please respond to my query: ' +\n                                               search_query)  # type: ignore\n\n      valid_docs = []\n      num_tokens = 0\n      for doc in found_docs:\n        doc_string = f\"Document: {doc.metadata['readable_filename']}{', page: ' + str(doc.metadata['pagenumber']) if doc.metadata['pagenumber'] else ''}\\n{str(doc.page_content)}\\n\"\n        num_tokens, prompt_cost = count_tokens_and_cost(doc_string)  # type: ignore\n\n        print(\n            f\"tokens used/limit: {token_counter}/{token_limit}, tokens in chunk: {num_tokens}, total prompt cost (of these contexts): {prompt_cost}. \ud83d\udcc4 File: {doc.metadata['readable_filename']}\"\n        )\n        if token_counter + num_tokens &lt;= token_limit:\n          token_counter += num_tokens\n          valid_docs.append(doc)\n        else:\n          # filled our token size, time to return\n          break\n\n      print(f\"Total tokens used: {token_counter}. Docs used: {len(valid_docs)} of {len(found_docs)} docs retrieved\")\n      print(f\"Course: {course_name} ||| search_query: {search_query}\")\n      print(f\"\u23f0 ^^ Runtime of getTopContexts: {(time.monotonic() - start_time_overall):.2f} seconds\")\n      if len(valid_docs) == 0:\n        return []\n\n      self.posthog.capture('distinct_id_of_the_user',\n                           event='success_get_top_contexts_OG',\n                           properties={\n                               'user_query': search_query,\n                               'course_name': course_name,\n                               'token_limit': token_limit,\n                               'total_tokens_used': token_counter,\n                               'total_contexts_used': len(valid_docs),\n                               'total_unique_docs_retrieved': len(found_docs),\n                               'getTopContext_total_latency_sec': time.monotonic() - start_time_overall,\n                           })\n\n      return self.format_for_json(valid_docs)\n    except Exception as e:\n      # return full traceback to front end\n      err: str = f\"ERROR: In /getTopContexts. Course: {course_name} ||| search_query: {search_query}\\nTraceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:\\n{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def batch_vector_search(self, search_queries: List[str], course_name: str, top_n: int = 50):\n    \"\"\"\n    Perform a similarity search for all the generated queries at once.\n    \"\"\"\n    start_time = time.monotonic()\n\n    from qdrant_client.http import models as rest\n    o = OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE)\n    # Prepare the filter for the course name\n    myfilter = rest.Filter(must=[\n        rest.FieldCondition(key='course_name', match=rest.MatchValue(value=course_name)),\n    ])\n\n    # Prepare the search requests\n    search_requests = []\n    for query in search_queries:\n      user_query_embedding = o.embed_query(query)\n      search_requests.append(\n          rest.SearchRequest(vector=user_query_embedding,\n                             filter=myfilter,\n                             limit=top_n,\n                             with_payload=True,\n                             params=models.SearchParams(quantization=models.QuantizationSearchParams(rescore=False))))\n\n    # Perform the batch search\n    search_results = self.qdrant_client.search_batch(\n        collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n        requests=search_requests,\n    )\n    # process search results\n    found_docs: list[list[Document]] = []\n    for result in search_results:\n      docs = []\n      for doc in result:\n        try:\n          metadata = doc.payload\n          page_content = metadata['page_content']\n          del metadata['page_content']\n\n          if \"pagenumber\" not in metadata.keys() and \"pagenumber_or_timestamp\" in metadata.keys():\n            metadata[\"pagenumber\"] = metadata[\"pagenumber_or_timestamp\"]\n\n          docs.append(Document(page_content=page_content, metadata=metadata))\n        except Exception:\n          print(traceback.print_exc())\n      found_docs.append(docs)\n\n    print(f\"\u23f0 Qdrant Batch Search runtime: {(time.monotonic() - start_time):.2f} seconds\")\n    return found_docs\n\n  def reciprocal_rank_fusion(self, results: list[list], k=60):\n    \"\"\"\n      Since we have multiple queries, and n documents returned per query, we need to go through all the results\n      and collect the documents with the highest overall score, as scored by qdrant similarity matching.\n      \"\"\"\n    fused_scores = {}\n    count = 0\n    unique_count = 0\n    for docs in results:\n      # Assumes the docs are returned in sorted order of relevance\n      count += len(docs)\n      for rank, doc in enumerate(docs):\n        doc_str = dumps(doc)\n        if doc_str not in fused_scores:\n          fused_scores[doc_str] = 0\n          unique_count += 1\n        fused_scores[doc_str] += 1 / (rank + k)\n        # Uncomment for debugging\n        # previous_score = fused_scores[doc_str]\n        #print(f\"Change score for doc: {doc_str}, previous score: {previous_score}, updated score: {fused_scores[doc_str]} \")\n    print(f\"Total number of documents in rank fusion: {count}\")\n    print(f\"Total number of unique documents in rank fusion: {unique_count}\")\n    reranked_results = [\n        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n    ]\n    return reranked_results\n\n  def getTopContextsWithMQR(self,\n                            search_query: str,\n                            course_name: str,\n                            token_limit: int = 4_000) -&gt; Union[List[Dict], str]:\n    \"\"\"\n    New info-retrieval pipeline that uses multi-query retrieval + filtering + reciprocal rank fusion + context padding.\n    1. Generate multiple queries based on the input search query.\n    2. Retrieve relevant docs for each query.\n    3. Filter the relevant docs based on the user query and pass them to the rank fusion step.\n    4. [CANCELED BEC POINTLESS] Rank the docs based on the relevance score.\n    5. Parent-doc-retrieval: Pad just the top 5 docs with expanded context from the original document.\n    \"\"\"\n    return 'fail'\n\n  #   try:\n  #     top_n_per_query = 40  # HARD CODE TO ENSURE WE HIT THE MAX TOKENS\n  #     start_time_overall = time.monotonic()\n  #     mq_start_time = time.monotonic()\n\n  #     # 1. GENERATE MULTIPLE QUERIES\n  #     generate_queries = (\n  #         MULTI_QUERY_PROMPT | self.llm | StrOutputParser() | (lambda x: x.split(\"\\n\")) |\n  #         (lambda x: list(filter(None, x)))  # filter out non-empty strings\n  #     )\n\n  #     generated_queries = generate_queries.invoke({\"original_query\": search_query})\n  #     print(\"generated_queries\", generated_queries)\n\n  #     # 2. VECTOR SEARCH FOR EACH QUERY\n  #     batch_found_docs_nested: list[list[Document]] = self.batch_vector_search(search_queries=generated_queries,\n  #                                                                              course_name=course_name,\n  #                                                                              top_n=top_n_per_query)\n\n  #     # 3. RANK REMAINING DOCUMENTS -- good for parent doc padding of top 5 at the end.\n  #     found_docs = self.reciprocal_rank_fusion(batch_found_docs_nested)\n  #     found_docs = [doc for doc, score in found_docs]\n  #     print(f\"Num docs after re-ranking: {len(found_docs)}\")\n  #     if len(found_docs) == 0:\n  #       return []\n  #     print(f\"\u23f0 Total multi-query processing runtime: {(time.monotonic() - mq_start_time):.2f} seconds\")\n\n  #     # 4. FILTER DOCS\n  #     filtered_docs = filter_top_contexts(contexts=found_docs, user_query=search_query, timeout=30, max_concurrency=180)\n  #     if len(filtered_docs) == 0:\n  #       return []\n\n  #     # 5. TOP DOC CONTEXT PADDING // parent document retriever\n  #     final_docs = context_parent_doc_padding(filtered_docs, search_query, course_name)\n  #     print(f\"Number of final docs after context padding: {len(final_docs)}\")\n\n  #     pre_prompt = \"Please answer the following question. Use the context below, called your documents, only if it's helpful and don't use parts that are very irrelevant. It's good to quote from your documents directly, when you do always use Markdown footnotes for citations. Use react-markdown superscript to number the sources at the end of sentences (1, 2, 3...) and use react-markdown Footnotes to list the full document names for each number. Use ReactMarkdown aka 'react-markdown' formatting for super script citations, use semi-formal style. Feel free to say you don't know. \\nHere's a few passages of the high quality documents:\\n\"\n  #     token_counter, _ = count_tokens_and_cost(pre_prompt + '\\n\\nNow please respond to my query: ' +\n  #                                              search_query)  # type: ignore\n\n  #     valid_docs = []\n  #     num_tokens = 0\n  #     for doc in final_docs:\n  #       doc_string = f\"Document: {doc['readable_filename']}{', page: ' + str(doc['pagenumber']) if doc['pagenumber'] else ''}\\n{str(doc['text'])}\\n\"\n  #       num_tokens, prompt_cost = count_tokens_and_cost(doc_string)  # type: ignore\n\n  #       print(f\"token_counter: {token_counter}, num_tokens: {num_tokens}, max_tokens: {token_limit}\")\n  #       if token_counter + num_tokens &lt;= token_limit:\n  #         token_counter += num_tokens\n  #         valid_docs.append(doc)\n  #       else:\n  #         # filled our token size, time to return\n  #         break\n\n  #     print(f\"Total tokens used: {token_counter} Used {len(valid_docs)} of total unique docs {len(found_docs)}.\")\n  #     print(f\"Course: {course_name} ||| search_query: {search_query}\")\n  #     print(f\"\u23f0 ^^ Runtime of getTopContextsWithMQR: {(time.monotonic() - start_time_overall):.2f} seconds\")\n\n  #     if len(valid_docs) == 0:\n  #       return []\n\n  #     self.posthog.capture('distinct_id_of_the_user',\n  #                          event='filter_top_contexts_succeeded',\n  #                          properties={\n  #                              'user_query': search_query,\n  #                              'course_name': course_name,\n  #                              'token_limit': token_limit,\n  #                              'total_tokens_used': token_counter,\n  #                              'total_contexts_used': len(valid_docs),\n  #                              'total_unique_docs_retrieved': len(found_docs),\n  #                          })\n\n  #     return self.format_for_json_mqr(valid_docs)\n  #   except Exception as e:\n  #     # return full traceback to front end\n  #     err: str = f\"ERROR: In /getTopContextsWithMQR. Course: {course_name} ||| search_query: {search_query}\\nTraceback: {traceback.format_exc()}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:\\n{e}\"  # type: ignore\n  #     print(err)\n  #     sentry_sdk.capture_exception(e)\n  #     return err\n\n  def format_for_json_mqr(self, found_docs) -&gt; List[Dict]:\n    \"\"\"\n    Same as format_for_json, but for the new MQR pipeline.\n    \"\"\"\n    for found_doc in found_docs:\n      if \"pagenumber\" not in found_doc.keys():\n        print(\"found no pagenumber\")\n        found_doc['pagenumber'] = found_doc['pagenumber_or_timestamp']\n\n    contexts = [\n        {\n            'text': doc['text'],\n            'readable_filename': doc['readable_filename'],\n            'course_name ': doc['course_name'],\n            's3_path': doc['s3_path'],\n            'pagenumber': doc['pagenumber'],\n            'url': doc['url'],  # wouldn't this error out?\n            'base_url': doc['base_url'],\n        } for doc in found_docs\n    ]\n\n    return contexts\n\n  def get_context_stuffed_prompt(self, user_question: str, course_name: str, top_n: int, top_k_to_search: int) -&gt; str:\n    \"\"\"\n    Get a stuffed prompt for a given user question and course name.\n    Args:\n      user_question (str)\n      course_name (str) : used for metadata filtering\n    Returns : str\n      a very long \"stuffed prompt\" with question + summaries of top_n most relevant documents.\n    \"\"\"\n    # MMR with metadata filtering based on course_name\n    vec_start_time = time.monotonic()\n    found_docs = self.vectorstore.max_marginal_relevance_search(user_question, k=top_n, fetch_k=top_k_to_search)\n    print(\n        f\"\u23f0 MMR Search runtime (top_n_to_keep: {top_n}, top_k_to_search: {top_k_to_search}): {(time.monotonic() - vec_start_time):.2f} seconds\"\n    )\n\n    requests = []\n    for doc in found_docs:\n      print(\"doc\", doc)\n      dictionary = {\n          \"model\": \"gpt-3.5-turbo\",\n          \"messages\": [{\n              \"role\":\n                  \"system\",\n              \"content\":\n                  \"You are a factual summarizer of partial documents. Stick to the facts (including partial info when necessary to avoid making up potentially incorrect details), and say I don't know when necessary.\"\n          }, {\n              \"role\":\n                  \"user\",\n              \"content\":\n                  f\"Provide a comprehensive summary of the given text, based on this question:\\n{doc.page_content}\\nQuestion: {user_question}\\nThe summary should cover all the key points that are relevant to the question, while also condensing the information into a concise format. The length of the summary should be as short as possible, without losing relevant information.\\nMake use of direct quotes from the text.\\nFeel free to include references, sentence fragments, keywords or anything that could help someone learn about it, only as it relates to the given question.\\nIf the text does not provide information to answer the question, please write 'None' and nothing else.\",\n          }],\n          \"n\": 1,\n          \"max_tokens\": 600,\n          \"metadata\": doc.metadata\n      }\n      requests.append(dictionary)\n\n    oai = OpenAIAPIProcessor(\n        input_prompts_list=requests,\n        request_url='https://api.openai.com/v1/chat/completions',\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        max_requests_per_minute=1500,\n        max_tokens_per_minute=90000,\n        token_encoding_name='cl100k_base',  # nosec -- reasonable bandit error suppression\n        max_attempts=5,\n        logging_level=20)\n\n    chain_start_time = time.monotonic()\n    asyncio.run(oai.process_api_requests_from_file())\n    results: list[str] = oai.results\n    print(f\"\u23f0 EXTREME context stuffing runtime: {(time.monotonic() - chain_start_time):.2f} seconds\")\n\n    print(f\"Cleaned results: {oai.cleaned_results}\")\n\n    all_texts = \"\"\n    separator = '---'  # between each context\n    token_counter = 0  #keeps track of tokens in each summarization\n    max_tokens = 7_500  #limit, will keep adding text to string until 8000 tokens reached.\n    for i, text in enumerate(oai.cleaned_results):\n      if text.lower().startswith('none') or text.lower().endswith('none.') or text.lower().endswith('none'):\n        # no useful text, it replied with a summary of \"None\"\n        continue\n      if text is not None:\n        if \"pagenumber\" not in results[i][-1].keys():  # type: ignore\n          results[i][-1]['pagenumber'] = results[i][-1].get('pagenumber_or_timestamp')  # type: ignore\n        num_tokens, prompt_cost = count_tokens_and_cost(text)  # type: ignore\n        if token_counter + num_tokens &gt; max_tokens:\n          print(f\"Total tokens yet in loop {i} is {num_tokens}\")\n          break  # Stop building the string if it exceeds the maximum number of tokens\n        token_counter += num_tokens\n        filename = str(results[i][-1].get('readable_filename', ''))  # type: ignore\n        pagenumber_or_timestamp = str(results[i][-1].get('pagenumber', ''))  # type: ignore\n        pagenumber = f\", page: {pagenumber_or_timestamp}\" if pagenumber_or_timestamp else ''\n        doc = f\"Document : filename: {filename}\" + pagenumber\n        summary = f\"\\nSummary: {text}\"\n        all_texts += doc + summary + '\\n' + separator + '\\n'\n\n    stuffed_prompt = \"\"\"Please answer the following question.\nUse the context below, called 'your documents', only if it's helpful and don't use parts that are very irrelevant.\nIt's good to quote 'your documents' directly using informal citations, like \"in document X it says Y\". Try to avoid giving false or misleading information. Feel free to say you don't know.\nTry to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable.\nThat said, be practical and really do your best, and don't let caution get too much in the way of being useful.\nTo help answer the question, here's a few passages of high quality documents:\\n{all_texts}\nNow please respond to my question: {user_question}\"\"\"\n\n    # \"Please answer the following question. It's good to quote 'your documents' directly, something like 'from ABS source it says XYZ' Feel free to say you don't know. \\nHere's a few passages of the high quality 'your documents':\\n\"\n\n    return stuffed_prompt\n\n  def get_stuffed_prompt(self, search_query: str, course_name: str, token_limit: int = 7_000) -&gt; str:\n    \"\"\"\n    Returns\n      String: A fully formatted prompt string.\n    \"\"\"\n    try:\n      top_n = 90\n      start_time_overall = time.monotonic()\n      o = OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE)\n      user_query_embedding = o.embed_documents(search_query)[0]  # type: ignore\n      myfilter = models.Filter(must=[\n          models.FieldCondition(key='course_name', match=models.MatchValue(value=course_name)),\n      ])\n\n      found_docs = self.qdrant_client.search(\n          collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n          query_filter=myfilter,\n          with_vectors=False,\n          query_vector=user_query_embedding,\n          limit=top_n  # Return 5 closest points\n      )\n      print(\"Search results: \", found_docs)\n      if len(found_docs) == 0:\n        return search_query\n\n      pre_prompt = \"Please answer the following question. Use the context below, called your documents, only if it's helpful and don't use parts that are very irrelevant. It's good to quote from your documents directly, when you do always use Markdown footnotes for citations. Use react-markdown superscript to number the sources at the end of sentences (1, 2, 3...) and use react-markdown Footnotes to list the full document names for each number. Use ReactMarkdown aka 'react-markdown' formatting for super script citations, use semi-formal style. Feel free to say you don't know. \\nHere's a few passages of the high quality documents:\\n\"\n\n      # count tokens at start and end, then also count each context.\n      token_counter, _ = count_tokens_and_cost(pre_prompt + '\\n\\nNow please respond to my query: ' +\n                                               search_query)  # type: ignore\n      valid_docs = []\n      for d in found_docs:\n        if d.payload is not None:\n          if \"pagenumber\" not in d.payload.keys():\n            d.payload[\"pagenumber\"] = d.payload[\"pagenumber_or_timestamp\"]\n\n          doc_string = f\"---\\nDocument: {d.payload['readable_filename']}{', page: ' + str(d.payload['pagenumber']) if d.payload['pagenumber'] else ''}\\n{d.payload.get('page_content')}\\n\"\n          num_tokens, prompt_cost = count_tokens_and_cost(doc_string)  # type: ignore\n\n          # print(f\"Page: {d.payload.get('page_content', ' '*100)[:100]}...\")\n          print(\n              f\"tokens used/limit: {token_counter}/{token_limit}, tokens in chunk: {num_tokens}, prompt cost of chunk: {prompt_cost}. \ud83d\udcc4 File: {d.payload.get('readable_filename', '')}\"\n          )\n          if token_counter + num_tokens &lt;= token_limit:\n            token_counter += num_tokens\n            valid_docs.append(\n                Document(page_content=d.payload.get('page_content', '&lt;Missing page content&gt;'), metadata=d.payload))\n          else:\n            continue\n\n      # Convert the valid_docs to full prompt\n      separator = '---\\n'  # between each context\n      context_text = separator.join(\n          f\"Document: {d.metadata['readable_filename']}{', page: ' + str(d.metadata['pagenumber']) if d.metadata['pagenumber'] else ''}\\n{d.page_content}\\n\"\n          for d in valid_docs)\n\n      # Create the stuffedPrompt\n      stuffedPrompt = (pre_prompt + context_text + '\\n\\nNow please respond to my query: ' + search_query)\n\n      TOTAL_num_tokens, prompt_cost = count_tokens_and_cost(stuffedPrompt, openai_model_name='gpt-4')  # type: ignore\n      print(f\"Total tokens: {TOTAL_num_tokens}, prompt_cost: {prompt_cost}\")\n      print(\"total docs: \", len(found_docs))\n      print(\"num docs used: \", len(valid_docs))\n\n      print(f\"\u23f0 ^^ Runtime of getTopContexts: {(time.monotonic() - start_time_overall):.2f} seconds\")\n      return stuffedPrompt\n    except Exception as e:\n      # return full traceback to front end\n      err: str = f\"Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n      print(err)\n      sentry_sdk.capture_exception(e)\n      return err\n\n  def format_for_json(self, found_docs: List[Document]) -&gt; List[Dict]:\n    \"\"\"Formatting only.\n      {'course_name': course_name, 'contexts': [{'source_name': 'Lumetta_notes', 'source_location': 'pg. 19', 'text': 'In FSM, we do this...'}, {'source_name': 'Lumetta_notes', 'source_location': 'pg. 20', 'text': 'In Assembly language, the code does that...'},]}\n\n    Args:\n        found_docs (List[Document]): _description_\n\n    Raises:\n        Exception: _description_\n\n    Returns:\n        List[Dict]: _description_\n    \"\"\"\n    for found_doc in found_docs:\n      if \"pagenumber\" not in found_doc.metadata.keys():\n        print(\"found no pagenumber\")\n        found_doc.metadata['pagenumber'] = found_doc.metadata['pagenumber_or_timestamp']\n\n    contexts = [\n        {\n            'text': doc.page_content,\n            'readable_filename': doc.metadata['readable_filename'],\n            'course_name ': doc.metadata['course_name'],\n            's3_path': doc.metadata['s3_path'],\n            'pagenumber': doc.metadata['pagenumber'],  # this because vector db schema is older...\n            # OPTIONAL PARAMS...\n            'url': doc.metadata.get('url'),  # wouldn't this error out?\n            'base_url': doc.metadata.get('base_url'),\n        } for doc in found_docs\n    ]\n\n    return contexts\n\n  def check_for_duplicates(self, texts: List[Dict], metadatas: List[Dict[str, Any]]) -&gt; bool:\n    \"\"\"\n    For given metadata, fetch docs from Supabase based on S3 path or URL.\n    If docs exists, concatenate the texts and compare with current texts, if same, return True.\n    \"\"\"\n    doc_table = os.getenv('NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE', '')\n    course_name = metadatas[0]['course_name']\n    incoming_s3_path = metadatas[0]['s3_path']\n    url = metadatas[0]['url']\n    original_filename = incoming_s3_path.split('/')[-1][37:]  # remove the 37-char uuid prefix\n\n    # check if uuid exists in s3_path -- not all s3_paths have uuids!\n    incoming_filename = incoming_s3_path.split('/')[-1]\n    pattern = re.compile(r'[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}',\n                         re.I)  # uuid V4 pattern, and v4 only.\n    if bool(pattern.search(incoming_filename)):\n      # uuid pattern exists -- remove the uuid and proceed with duplicate checking\n      original_filename = incoming_filename[37:]\n    else:\n      # do not remove anything and proceed with duplicate checking\n      original_filename = incoming_filename\n\n    if incoming_s3_path:\n      filename = incoming_s3_path\n      supabase_contents = self.supabase_client.table(doc_table).select('id', 'contexts', 's3_path').eq(\n          'course_name', course_name).like('s3_path', '%' + original_filename + '%').order('id', desc=True).execute()\n      supabase_contents = supabase_contents.data\n    elif url:\n      filename = url\n      supabase_contents = self.supabase_client.table(doc_table).select('id', 'contexts', 's3_path').eq(\n          'course_name', course_name).eq('url', url).order('id', desc=True).execute()\n      supabase_contents = supabase_contents.data\n    else:\n      filename = None\n      supabase_contents = []\n\n    supabase_whole_text = \"\"\n    if len(supabase_contents) &gt; 0:  # if a doc with same filename exists in Supabase\n      # concatenate texts\n      supabase_contexts = supabase_contents[0]\n      for text in supabase_contexts['contexts']:\n        supabase_whole_text += text['text']\n\n      current_whole_text = \"\"\n      for text in texts:\n        current_whole_text += text['input']\n\n      if supabase_whole_text == current_whole_text:  # matches the previous file\n        print(f\"Duplicate ingested! \ud83d\udcc4 s3_path: {filename}.\")\n        return True\n\n      else:  # the file is updated\n        print(f\"Updated file detected! Same filename, new contents. \ud83d\udcc4 s3_path: {filename}\")\n\n        # call the delete function on older docs\n        for content in supabase_contents:\n          print(\"older s3_path to be deleted: \", content['s3_path'])\n          delete_status = self.delete_data(course_name, content['s3_path'], '')\n          print(\"delete_status: \", delete_status)\n        return False\n\n    else:  # filename does not already exist in Supabase, so its a brand new file\n      print(f\"NOT a duplicate! \ud83d\udcc4s3_path: {filename}\")\n      return False\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.__init__","title":"<code>__init__()</code>","text":"<p>Initialize AWS S3, Qdrant, and Supabase.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def __init__(self):\n  \"\"\"\n  Initialize AWS S3, Qdrant, and Supabase.\n  \"\"\"\n  openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n  # vector DB\n  self.qdrant_client = QdrantClient(\n      url=os.getenv('QDRANT_URL'),\n      api_key=os.getenv('QDRANT_API_KEY'),\n  )\n\n  self.vectorstore = Qdrant(client=self.qdrant_client,\n                            collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n                            embeddings=OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE))\n\n  # S3\n  self.s3_client = boto3.client(\n      's3',\n      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n  )\n\n  # Create a Supabase client\n  self.supabase_client = supabase.create_client(  # type: ignore\n      supabase_url=os.environ['SUPABASE_URL'], supabase_key=os.environ['SUPABASE_API_KEY'])\n\n  self.llm = AzureChatOpenAI(\n      temperature=0,\n      deployment_name=os.getenv('AZURE_OPENAI_ENGINE'),  #type:ignore\n      openai_api_base=os.getenv('AZURE_OPENAI_ENDPOINT'),  #type:ignore\n      openai_api_key=os.getenv('AZURE_OPENAI_KEY'),  #type:ignore\n      openai_api_version=os.getenv('OPENAI_API_VERSION'),  #type:ignore\n      openai_api_type=OPENAI_API_TYPE)\n\n  self.posthog = Posthog(sync_mode=True,\n                         project_api_key=os.environ['POSTHOG_API_KEY'],\n                         host='https://app.posthog.com')\n\n  return None\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.batch_vector_search","title":"<code>batch_vector_search(search_queries, course_name, top_n=50)</code>","text":"<p>Perform a similarity search for all the generated queries at once.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def batch_vector_search(self, search_queries: List[str], course_name: str, top_n: int = 50):\n  \"\"\"\n  Perform a similarity search for all the generated queries at once.\n  \"\"\"\n  start_time = time.monotonic()\n\n  from qdrant_client.http import models as rest\n  o = OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE)\n  # Prepare the filter for the course name\n  myfilter = rest.Filter(must=[\n      rest.FieldCondition(key='course_name', match=rest.MatchValue(value=course_name)),\n  ])\n\n  # Prepare the search requests\n  search_requests = []\n  for query in search_queries:\n    user_query_embedding = o.embed_query(query)\n    search_requests.append(\n        rest.SearchRequest(vector=user_query_embedding,\n                           filter=myfilter,\n                           limit=top_n,\n                           with_payload=True,\n                           params=models.SearchParams(quantization=models.QuantizationSearchParams(rescore=False))))\n\n  # Perform the batch search\n  search_results = self.qdrant_client.search_batch(\n      collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n      requests=search_requests,\n  )\n  # process search results\n  found_docs: list[list[Document]] = []\n  for result in search_results:\n    docs = []\n    for doc in result:\n      try:\n        metadata = doc.payload\n        page_content = metadata['page_content']\n        del metadata['page_content']\n\n        if \"pagenumber\" not in metadata.keys() and \"pagenumber_or_timestamp\" in metadata.keys():\n          metadata[\"pagenumber\"] = metadata[\"pagenumber_or_timestamp\"]\n\n        docs.append(Document(page_content=page_content, metadata=metadata))\n      except Exception:\n        print(traceback.print_exc())\n    found_docs.append(docs)\n\n  print(f\"\u23f0 Qdrant Batch Search runtime: {(time.monotonic() - start_time):.2f} seconds\")\n  return found_docs\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.check_for_duplicates","title":"<code>check_for_duplicates(texts, metadatas)</code>","text":"<p>For given metadata, fetch docs from Supabase based on S3 path or URL. If docs exists, concatenate the texts and compare with current texts, if same, return True.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def check_for_duplicates(self, texts: List[Dict], metadatas: List[Dict[str, Any]]) -&gt; bool:\n  \"\"\"\n  For given metadata, fetch docs from Supabase based on S3 path or URL.\n  If docs exists, concatenate the texts and compare with current texts, if same, return True.\n  \"\"\"\n  doc_table = os.getenv('NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE', '')\n  course_name = metadatas[0]['course_name']\n  incoming_s3_path = metadatas[0]['s3_path']\n  url = metadatas[0]['url']\n  original_filename = incoming_s3_path.split('/')[-1][37:]  # remove the 37-char uuid prefix\n\n  # check if uuid exists in s3_path -- not all s3_paths have uuids!\n  incoming_filename = incoming_s3_path.split('/')[-1]\n  pattern = re.compile(r'[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}',\n                       re.I)  # uuid V4 pattern, and v4 only.\n  if bool(pattern.search(incoming_filename)):\n    # uuid pattern exists -- remove the uuid and proceed with duplicate checking\n    original_filename = incoming_filename[37:]\n  else:\n    # do not remove anything and proceed with duplicate checking\n    original_filename = incoming_filename\n\n  if incoming_s3_path:\n    filename = incoming_s3_path\n    supabase_contents = self.supabase_client.table(doc_table).select('id', 'contexts', 's3_path').eq(\n        'course_name', course_name).like('s3_path', '%' + original_filename + '%').order('id', desc=True).execute()\n    supabase_contents = supabase_contents.data\n  elif url:\n    filename = url\n    supabase_contents = self.supabase_client.table(doc_table).select('id', 'contexts', 's3_path').eq(\n        'course_name', course_name).eq('url', url).order('id', desc=True).execute()\n    supabase_contents = supabase_contents.data\n  else:\n    filename = None\n    supabase_contents = []\n\n  supabase_whole_text = \"\"\n  if len(supabase_contents) &gt; 0:  # if a doc with same filename exists in Supabase\n    # concatenate texts\n    supabase_contexts = supabase_contents[0]\n    for text in supabase_contexts['contexts']:\n      supabase_whole_text += text['text']\n\n    current_whole_text = \"\"\n    for text in texts:\n      current_whole_text += text['input']\n\n    if supabase_whole_text == current_whole_text:  # matches the previous file\n      print(f\"Duplicate ingested! \ud83d\udcc4 s3_path: {filename}.\")\n      return True\n\n    else:  # the file is updated\n      print(f\"Updated file detected! Same filename, new contents. \ud83d\udcc4 s3_path: {filename}\")\n\n      # call the delete function on older docs\n      for content in supabase_contents:\n        print(\"older s3_path to be deleted: \", content['s3_path'])\n        delete_status = self.delete_data(course_name, content['s3_path'], '')\n        print(\"delete_status: \", delete_status)\n      return False\n\n  else:  # filename does not already exist in Supabase, so its a brand new file\n    print(f\"NOT a duplicate! \ud83d\udcc4s3_path: {filename}\")\n    return False\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.delete_data","title":"<code>delete_data(course_name, s3_path, source_url)</code>","text":"<p>Delete file from S3, Qdrant, and Supabase.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def delete_data(self, course_name: str, s3_path: str, source_url: str):\n  \"\"\"Delete file from S3, Qdrant, and Supabase.\"\"\"\n  print(f\"Deleting {s3_path} from S3, Qdrant, and Supabase for course {course_name}\")\n  # add delete from doc map logic here\n  try:\n    # Delete file from S3\n    bucket_name = os.getenv('S3_BUCKET_NAME')\n\n    # Delete files by S3 path\n    if s3_path:\n      try:\n        self.s3_client.delete_object(Bucket=bucket_name, Key=s3_path)\n      except Exception as e:\n        print(\"Error in deleting file from s3:\", e)\n        sentry_sdk.capture_exception(e)\n      # Delete from Qdrant\n      # docs for nested keys: https://qdrant.tech/documentation/concepts/filtering/#nested-key\n      # Qdrant \"points\" look like this: Record(id='000295ca-bd28-ac4a-6f8d-c245f7377f90', payload={'metadata': {'course_name': 'zotero-extreme', 'pagenumber_or_timestamp': 15, 'readable_filename': 'Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf', 's3_path': 'courses/zotero-extreme/Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf'}, 'page_content': '18  \\nDunlosky et al.\\n3.3 Effects in representative educational contexts. Sev-\\neral of the large summarization-training studies have been \\nconducted in regular classrooms, indicating the feasibility of \\ndoing so. For example, the study by A. King (1992) took place \\nin the context of a remedial study-skills course for undergrad-\\nuates, and the study by Rinehart et al. (1986) took place in \\nsixth-grade classrooms, with the instruction led by students \\nregular teachers. In these and other cases, students benefited \\nfrom the classroom training. We suspect it may actually be \\nmore feasible to conduct these kinds of training  ...\n      try:\n        self.qdrant_client.delete(\n            collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n            points_selector=models.Filter(must=[\n                models.FieldCondition(\n                    key=\"s3_path\",\n                    match=models.MatchValue(value=s3_path),\n                ),\n            ]),\n        )\n      except Exception as e:\n        if \"timed out\" in str(e):\n          # Timed out is fine. Still deletes.\n          # https://github.com/qdrant/qdrant/issues/3654#issuecomment-1955074525\n          pass\n        else:\n          print(\"Error in deleting file from Qdrant:\", e)\n          sentry_sdk.capture_exception(e)\n      try:\n        # delete from Nomic\n        response = self.supabase_client.from_(\n            os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).select(\"id, s3_path, contexts\").eq(\n                's3_path', s3_path).eq('course_name', course_name).execute()\n        data = response.data[0]  #single record fetched\n        nomic_ids_to_delete = []\n        context_count = len(data['contexts'])\n        for i in range(1, context_count + 1):\n          nomic_ids_to_delete.append(str(data['id']) + \"_\" + str(i))\n\n        # delete from Nomic\n        res = delete_from_document_map(course_name, nomic_ids_to_delete)\n      except Exception as e:\n        print(\"Error in deleting file from Nomic:\", e)\n        sentry_sdk.capture_exception(e)\n\n      try:\n        self.supabase_client.from_(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).delete().eq(\n            's3_path', s3_path).eq('course_name', course_name).execute()\n      except Exception as e:\n        print(\"Error in deleting file from supabase:\", e)\n        sentry_sdk.capture_exception(e)\n\n    # Delete files by their URL identifier\n    elif source_url:\n      try:\n        # Delete from Qdrant\n        self.qdrant_client.delete(\n            collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n            points_selector=models.Filter(must=[\n                models.FieldCondition(\n                    key=\"url\",\n                    match=models.MatchValue(value=source_url),\n                ),\n            ]),\n        )\n      except Exception as e:\n        if \"timed out\" in str(e):\n          # Timed out is fine. Still deletes.\n          # https://github.com/qdrant/qdrant/issues/3654#issuecomment-1955074525\n          pass\n        else:\n          print(\"Error in deleting file from Qdrant:\", e)\n          sentry_sdk.capture_exception(e)\n      try:\n        # delete from Nomic\n        response = self.supabase_client.from_(\n            os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).select(\"id, url, contexts\").eq(\n                'url', source_url).eq('course_name', course_name).execute()\n        data = response.data[0]  #single record fetched\n        nomic_ids_to_delete = []\n        context_count = len(data['contexts'])\n        for i in range(1, context_count + 1):\n          nomic_ids_to_delete.append(str(data['id']) + \"_\" + str(i))\n\n        # delete from Nomic\n        res = delete_from_document_map(course_name, nomic_ids_to_delete)\n      except Exception as e:\n        print(\"Error in deleting file from Nomic:\", e)\n        sentry_sdk.capture_exception(e)\n\n      try:\n        # delete from Supabase\n        self.supabase_client.from_(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).delete().eq(\n            'url', source_url).eq('course_name', course_name).execute()\n      except Exception as e:\n        print(\"Error in deleting file from supabase:\", e)\n        sentry_sdk.capture_exception(e)\n\n    # Delete from Supabase\n    return \"Success\"\n  except Exception as e:\n    err: str = f\"ERROR IN delete_data: Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    return err\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.delete_entire_course","title":"<code>delete_entire_course(course_name)</code>","text":"<p>Delete entire course.</p> <p>Delete materials from S3, Supabase SQL, Vercel KV, and QDrant vector DB Args:     course_name (str): description</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def delete_entire_course(self, course_name: str):\n  \"\"\"Delete entire course.\n\n  Delete materials from S3, Supabase SQL, Vercel KV, and QDrant vector DB\n  Args:\n      course_name (str): _description_\n  \"\"\"\n  print(f\"Deleting entire course: {course_name}\")\n  try:\n    # Delete file from S3\n    print(\"Deleting from S3\")\n    objects_to_delete = self.s3_client.list_objects(Bucket=os.getenv('S3_BUCKET_NAME'),\n                                                    Prefix=f'courses/{course_name}/')\n    for object in objects_to_delete['Contents']:\n      self.s3_client.delete_object(Bucket=os.getenv('S3_BUCKET_NAME'), Key=object['Key'])\n  except Exception as e:\n    err: str = f\"ERROR IN delete_entire_course(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    pass\n\n  try:\n    # Delete from Qdrant\n    # docs for nested keys: https://qdrant.tech/documentation/concepts/filtering/#nested-key\n    # Qdrant \"points\" look like this: Record(id='000295ca-bd28-ac4a-6f8d-c245f7377f90', payload={'metadata': {'course_name': 'zotero-extreme', 'pagenumber_or_timestamp': 15, 'readable_filename': 'Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf', 's3_path': 'courses/zotero-extreme/Dunlosky et al. - 2013 - Improving Students\u2019 Learning With Effective Learni.pdf'}, 'page_content': '18  \\nDunlosky et al.\\n3.3 Effects in representative educational contexts. Sev-\\neral of the large summarization-training studies have been \\nconducted in regular classrooms, indicating the feasibility of \\ndoing so. For example, the study by A. King (1992) took place \\nin the context of a remedial study-skills course for undergrad-\\nuates, and the study by Rinehart et al. (1986) took place in \\nsixth-grade classrooms, with the instruction led by students \\nregular teachers. In these and other cases, students benefited \\nfrom the classroom training. We suspect it may actually be \\nmore feasible to conduct these kinds of training studies in \\nclassrooms than in the laboratory, given the nature of the time \\ncommitment for students. Even some of the studies that did \\nnot involve training were conducted outside the laboratory; for \\nexample, in the Bednall and Kehoe (2011) study on learning \\nabout logical fallacies from Web modules (see data in Table 3), \\nthe modules were actually completed as a homework assign-\\nment. Overall, benefits can be observed in classroom settings; \\nthe real constraint is whether students have the skill to suc-\\ncessfully summarize, not whether summarization occurs in the \\nlab or the classroom.\\n3.4 Issues for implementation. Summarization would be \\nfeasible for undergraduates or other learners who already \\nknow how to summarize. For these students, summarization \\nwould constitute an easy-to-implement technique that would \\nnot take a lot of time to complete or understand. The only \\nconcern would be whether these students might be better \\nserved by some other strategy, but certainly summarization \\nwould be better than the study strategies students typically \\nfavor, such as highlighting and rereading (as we discuss in the \\nsections on those strategies below). A trickier issue would \\nconcern implementing the strategy with students who are not \\nskilled summarizers. Relatively intensive training programs \\nare required for middle school students or learners with learn-\\ning disabilities to benefit from summarization. Such efforts \\nare not misplaced; training has been shown to benefit perfor-\\nmance on a range of measures, although the training proce-\\ndures do raise practical issues (e.g., Gajria &amp; Salvia, 1992: \\n6.511 hours of training used for sixth through ninth graders \\nwith learning disabilities; Malone &amp; Mastropieri, 1991: 2 \\ndays of training used for middle school students with learning \\ndisabilities; Rinehart et al., 1986: 4550 minutes of instruc-\\ntion per day for 5 days used for sixth graders). Of course, \\ninstructors may want students to summarize material because \\nsummarization itself is a goal, not because they plan to use \\nsummarization as a study technique, and that goal may merit \\nthe efforts of training.\\nHowever, if the goal is to use summarization as a study \\ntechnique, our question is whether training students would be \\nworth the amount of time it would take, both in terms of the \\ntime required on the part of the instructor and in terms of the \\ntime taken away from students other activities. For instance, \\nin terms of efficacy, summarization tends to fall in the middle \\nof the pack when compared to other techniques. In direct \\ncomparisons, it was sometimes more useful than rereading \\n(Rewey, Dansereau, &amp; Peel, 1991) and was as useful as note-\\ntaking (e.g., Bretzing &amp; Kulhavy, 1979) but was less powerful \\nthan generating explanations (e.g., Bednall &amp; Kehoe, 2011) or \\nself-questioning (A. King, 1992).\\n3.5 Summarization: Overall assessment. On the basis of the \\navailable evidence, we rate summarization as low utility. It can \\nbe an effective learning strategy for learners who are already \\nskilled at summarizing; however, many learners (including \\nchildren, high school students, and even some undergraduates) \\nwill require extensive training, which makes this strategy less \\nfeasible. Our enthusiasm is further dampened by mixed find-\\nings regarding which tasks summarization actually helps. \\nAlthough summarization has been examined with a wide \\nrange of text materials, many researchers have pointed to fac-\\ntors of these texts that seem likely to moderate the effects of \\nsummarization (e.g'}, vector=None),\n    print(\"deleting from qdrant\")\n    self.qdrant_client.delete(\n        collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n        points_selector=models.Filter(must=[\n            models.FieldCondition(\n                key=\"course_name\",\n                match=models.MatchValue(value=course_name),\n            ),\n        ]),\n    )\n  except Exception as e:\n    err: str = f\"ERROR IN delete_entire_course(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    pass\n\n  try:\n    # Delete from Supabase\n    print(\"deleting from supabase\")\n    response = self.supabase_client.from_(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).delete().eq(\n        'course_name', course_name).execute()\n    print(\"supabase response: \", response)\n    return \"Success\"\n  except Exception as e:\n    err: str = f\"ERROR IN delete_entire_course(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.format_for_json","title":"<code>format_for_json(found_docs)</code>","text":"<p>Formatting only.   {'course_name': course_name, 'contexts': [{'source_name': 'Lumetta_notes', 'source_location': 'pg. 19', 'text': 'In FSM, we do this...'}, {'source_name': 'Lumetta_notes', 'source_location': 'pg. 20', 'text': 'In Assembly language, the code does that...'},]}</p> <p>Parameters:</p> Name Type Description Default <code>found_docs</code> <code>List[Document]</code> <p>description</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>description</p> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: description</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def format_for_json(self, found_docs: List[Document]) -&gt; List[Dict]:\n  \"\"\"Formatting only.\n    {'course_name': course_name, 'contexts': [{'source_name': 'Lumetta_notes', 'source_location': 'pg. 19', 'text': 'In FSM, we do this...'}, {'source_name': 'Lumetta_notes', 'source_location': 'pg. 20', 'text': 'In Assembly language, the code does that...'},]}\n\n  Args:\n      found_docs (List[Document]): _description_\n\n  Raises:\n      Exception: _description_\n\n  Returns:\n      List[Dict]: _description_\n  \"\"\"\n  for found_doc in found_docs:\n    if \"pagenumber\" not in found_doc.metadata.keys():\n      print(\"found no pagenumber\")\n      found_doc.metadata['pagenumber'] = found_doc.metadata['pagenumber_or_timestamp']\n\n  contexts = [\n      {\n          'text': doc.page_content,\n          'readable_filename': doc.metadata['readable_filename'],\n          'course_name ': doc.metadata['course_name'],\n          's3_path': doc.metadata['s3_path'],\n          'pagenumber': doc.metadata['pagenumber'],  # this because vector db schema is older...\n          # OPTIONAL PARAMS...\n          'url': doc.metadata.get('url'),  # wouldn't this error out?\n          'base_url': doc.metadata.get('base_url'),\n      } for doc in found_docs\n  ]\n\n  return contexts\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.format_for_json_mqr","title":"<code>format_for_json_mqr(found_docs)</code>","text":"<p>Same as format_for_json, but for the new MQR pipeline.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def format_for_json_mqr(self, found_docs) -&gt; List[Dict]:\n  \"\"\"\n  Same as format_for_json, but for the new MQR pipeline.\n  \"\"\"\n  for found_doc in found_docs:\n    if \"pagenumber\" not in found_doc.keys():\n      print(\"found no pagenumber\")\n      found_doc['pagenumber'] = found_doc['pagenumber_or_timestamp']\n\n  contexts = [\n      {\n          'text': doc['text'],\n          'readable_filename': doc['readable_filename'],\n          'course_name ': doc['course_name'],\n          's3_path': doc['s3_path'],\n          'pagenumber': doc['pagenumber'],\n          'url': doc['url'],  # wouldn't this error out?\n          'base_url': doc['base_url'],\n      } for doc in found_docs\n  ]\n\n  return contexts\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.getAll","title":"<code>getAll(course_name)</code>","text":"<p>Get all course materials based on course name. Args:     course_name (as uploaded on supabase) Returns:     list of dictionaries with distinct s3 path, readable_filename and course_name, url, base_url.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def getAll(\n    self,\n    course_name: str,\n):\n  \"\"\"Get all course materials based on course name.\n  Args:\n      course_name (as uploaded on supabase)\n  Returns:\n      list of dictionaries with distinct s3 path, readable_filename and course_name, url, base_url.\n  \"\"\"\n\n  response = self.supabase_client.table(os.environ['NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE']).select(\n      'course_name, s3_path, readable_filename, url, base_url').eq('course_name', course_name).execute()\n\n  data = response.data\n  unique_combinations = set()\n  distinct_dicts = []\n\n  for item in data:\n    combination = (item['s3_path'], item['readable_filename'], item['course_name'], item['url'], item['base_url'])\n    if combination not in unique_combinations:\n      unique_combinations.add(combination)\n      distinct_dicts.append(item)\n\n  return distinct_dicts\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.getTopContexts","title":"<code>getTopContexts(search_query, course_name, token_limit=4000)</code>","text":"<p>Here's a summary of the work.</p> <p>/GET arguments   course name (optional) str: A json response with TBD fields.</p> <p>Returns   JSON: A json response with TBD fields. See main.py:getTopContexts docs.   or   String: An error message with traceback.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def getTopContexts(self, search_query: str, course_name: str, token_limit: int = 4_000) -&gt; Union[List[Dict], str]:\n  \"\"\"Here's a summary of the work.\n\n  /GET arguments\n    course name (optional) str: A json response with TBD fields.\n\n  Returns\n    JSON: A json response with TBD fields. See main.py:getTopContexts docs.\n    or\n    String: An error message with traceback.\n  \"\"\"\n  try:\n    start_time_overall = time.monotonic()\n\n    found_docs: list[Document] = self.vector_search(search_query=search_query, course_name=course_name)\n\n    pre_prompt = \"Please answer the following question. Use the context below, called your documents, only if it's helpful and don't use parts that are very irrelevant. It's good to quote from your documents directly, when you do always use Markdown footnotes for citations. Use react-markdown superscript to number the sources at the end of sentences (1, 2, 3...) and use react-markdown Footnotes to list the full document names for each number. Use ReactMarkdown aka 'react-markdown' formatting for super script citations, use semi-formal style. Feel free to say you don't know. \\nHere's a few passages of the high quality documents:\\n\"\n    # count tokens at start and end, then also count each context.\n    token_counter, _ = count_tokens_and_cost(pre_prompt + '\\n\\nNow please respond to my query: ' +\n                                             search_query)  # type: ignore\n\n    valid_docs = []\n    num_tokens = 0\n    for doc in found_docs:\n      doc_string = f\"Document: {doc.metadata['readable_filename']}{', page: ' + str(doc.metadata['pagenumber']) if doc.metadata['pagenumber'] else ''}\\n{str(doc.page_content)}\\n\"\n      num_tokens, prompt_cost = count_tokens_and_cost(doc_string)  # type: ignore\n\n      print(\n          f\"tokens used/limit: {token_counter}/{token_limit}, tokens in chunk: {num_tokens}, total prompt cost (of these contexts): {prompt_cost}. \ud83d\udcc4 File: {doc.metadata['readable_filename']}\"\n      )\n      if token_counter + num_tokens &lt;= token_limit:\n        token_counter += num_tokens\n        valid_docs.append(doc)\n      else:\n        # filled our token size, time to return\n        break\n\n    print(f\"Total tokens used: {token_counter}. Docs used: {len(valid_docs)} of {len(found_docs)} docs retrieved\")\n    print(f\"Course: {course_name} ||| search_query: {search_query}\")\n    print(f\"\u23f0 ^^ Runtime of getTopContexts: {(time.monotonic() - start_time_overall):.2f} seconds\")\n    if len(valid_docs) == 0:\n      return []\n\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='success_get_top_contexts_OG',\n                         properties={\n                             'user_query': search_query,\n                             'course_name': course_name,\n                             'token_limit': token_limit,\n                             'total_tokens_used': token_counter,\n                             'total_contexts_used': len(valid_docs),\n                             'total_unique_docs_retrieved': len(found_docs),\n                             'getTopContext_total_latency_sec': time.monotonic() - start_time_overall,\n                         })\n\n    return self.format_for_json(valid_docs)\n  except Exception as e:\n    # return full traceback to front end\n    err: str = f\"ERROR: In /getTopContexts. Course: {course_name} ||| search_query: {search_query}\\nTraceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:\\n{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    return err\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.getTopContextsWithMQR","title":"<code>getTopContextsWithMQR(search_query, course_name, token_limit=4000)</code>","text":"<p>New info-retrieval pipeline that uses multi-query retrieval + filtering + reciprocal rank fusion + context padding. 1. Generate multiple queries based on the input search query. 2. Retrieve relevant docs for each query. 3. Filter the relevant docs based on the user query and pass them to the rank fusion step. 4. [CANCELED BEC POINTLESS] Rank the docs based on the relevance score. 5. Parent-doc-retrieval: Pad just the top 5 docs with expanded context from the original document.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def getTopContextsWithMQR(self,\n                          search_query: str,\n                          course_name: str,\n                          token_limit: int = 4_000) -&gt; Union[List[Dict], str]:\n  \"\"\"\n  New info-retrieval pipeline that uses multi-query retrieval + filtering + reciprocal rank fusion + context padding.\n  1. Generate multiple queries based on the input search query.\n  2. Retrieve relevant docs for each query.\n  3. Filter the relevant docs based on the user query and pass them to the rank fusion step.\n  4. [CANCELED BEC POINTLESS] Rank the docs based on the relevance score.\n  5. Parent-doc-retrieval: Pad just the top 5 docs with expanded context from the original document.\n  \"\"\"\n  return 'fail'\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.get_context_stuffed_prompt","title":"<code>get_context_stuffed_prompt(user_question, course_name, top_n, top_k_to_search)</code>","text":"<p>Get a stuffed prompt for a given user question and course name. Args:   user_question (str)   course_name (str) : used for metadata filtering Returns : str   a very long \"stuffed prompt\" with question + summaries of top_n most relevant documents.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>  def get_context_stuffed_prompt(self, user_question: str, course_name: str, top_n: int, top_k_to_search: int) -&gt; str:\n    \"\"\"\n    Get a stuffed prompt for a given user question and course name.\n    Args:\n      user_question (str)\n      course_name (str) : used for metadata filtering\n    Returns : str\n      a very long \"stuffed prompt\" with question + summaries of top_n most relevant documents.\n    \"\"\"\n    # MMR with metadata filtering based on course_name\n    vec_start_time = time.monotonic()\n    found_docs = self.vectorstore.max_marginal_relevance_search(user_question, k=top_n, fetch_k=top_k_to_search)\n    print(\n        f\"\u23f0 MMR Search runtime (top_n_to_keep: {top_n}, top_k_to_search: {top_k_to_search}): {(time.monotonic() - vec_start_time):.2f} seconds\"\n    )\n\n    requests = []\n    for doc in found_docs:\n      print(\"doc\", doc)\n      dictionary = {\n          \"model\": \"gpt-3.5-turbo\",\n          \"messages\": [{\n              \"role\":\n                  \"system\",\n              \"content\":\n                  \"You are a factual summarizer of partial documents. Stick to the facts (including partial info when necessary to avoid making up potentially incorrect details), and say I don't know when necessary.\"\n          }, {\n              \"role\":\n                  \"user\",\n              \"content\":\n                  f\"Provide a comprehensive summary of the given text, based on this question:\\n{doc.page_content}\\nQuestion: {user_question}\\nThe summary should cover all the key points that are relevant to the question, while also condensing the information into a concise format. The length of the summary should be as short as possible, without losing relevant information.\\nMake use of direct quotes from the text.\\nFeel free to include references, sentence fragments, keywords or anything that could help someone learn about it, only as it relates to the given question.\\nIf the text does not provide information to answer the question, please write 'None' and nothing else.\",\n          }],\n          \"n\": 1,\n          \"max_tokens\": 600,\n          \"metadata\": doc.metadata\n      }\n      requests.append(dictionary)\n\n    oai = OpenAIAPIProcessor(\n        input_prompts_list=requests,\n        request_url='https://api.openai.com/v1/chat/completions',\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        max_requests_per_minute=1500,\n        max_tokens_per_minute=90000,\n        token_encoding_name='cl100k_base',  # nosec -- reasonable bandit error suppression\n        max_attempts=5,\n        logging_level=20)\n\n    chain_start_time = time.monotonic()\n    asyncio.run(oai.process_api_requests_from_file())\n    results: list[str] = oai.results\n    print(f\"\u23f0 EXTREME context stuffing runtime: {(time.monotonic() - chain_start_time):.2f} seconds\")\n\n    print(f\"Cleaned results: {oai.cleaned_results}\")\n\n    all_texts = \"\"\n    separator = '---'  # between each context\n    token_counter = 0  #keeps track of tokens in each summarization\n    max_tokens = 7_500  #limit, will keep adding text to string until 8000 tokens reached.\n    for i, text in enumerate(oai.cleaned_results):\n      if text.lower().startswith('none') or text.lower().endswith('none.') or text.lower().endswith('none'):\n        # no useful text, it replied with a summary of \"None\"\n        continue\n      if text is not None:\n        if \"pagenumber\" not in results[i][-1].keys():  # type: ignore\n          results[i][-1]['pagenumber'] = results[i][-1].get('pagenumber_or_timestamp')  # type: ignore\n        num_tokens, prompt_cost = count_tokens_and_cost(text)  # type: ignore\n        if token_counter + num_tokens &gt; max_tokens:\n          print(f\"Total tokens yet in loop {i} is {num_tokens}\")\n          break  # Stop building the string if it exceeds the maximum number of tokens\n        token_counter += num_tokens\n        filename = str(results[i][-1].get('readable_filename', ''))  # type: ignore\n        pagenumber_or_timestamp = str(results[i][-1].get('pagenumber', ''))  # type: ignore\n        pagenumber = f\", page: {pagenumber_or_timestamp}\" if pagenumber_or_timestamp else ''\n        doc = f\"Document : filename: {filename}\" + pagenumber\n        summary = f\"\\nSummary: {text}\"\n        all_texts += doc + summary + '\\n' + separator + '\\n'\n\n    stuffed_prompt = \"\"\"Please answer the following question.\nUse the context below, called 'your documents', only if it's helpful and don't use parts that are very irrelevant.\nIt's good to quote 'your documents' directly using informal citations, like \"in document X it says Y\". Try to avoid giving false or misleading information. Feel free to say you don't know.\nTry to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable.\nThat said, be practical and really do your best, and don't let caution get too much in the way of being useful.\nTo help answer the question, here's a few passages of high quality documents:\\n{all_texts}\nNow please respond to my question: {user_question}\"\"\"\n\n    # \"Please answer the following question. It's good to quote 'your documents' directly, something like 'from ABS source it says XYZ' Feel free to say you don't know. \\nHere's a few passages of the high quality 'your documents':\\n\"\n\n    return stuffed_prompt\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.get_stuffed_prompt","title":"<code>get_stuffed_prompt(search_query, course_name, token_limit=7000)</code>","text":"<p>Returns   String: A fully formatted prompt string.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def get_stuffed_prompt(self, search_query: str, course_name: str, token_limit: int = 7_000) -&gt; str:\n  \"\"\"\n  Returns\n    String: A fully formatted prompt string.\n  \"\"\"\n  try:\n    top_n = 90\n    start_time_overall = time.monotonic()\n    o = OpenAIEmbeddings(openai_api_type=OPENAI_API_TYPE)\n    user_query_embedding = o.embed_documents(search_query)[0]  # type: ignore\n    myfilter = models.Filter(must=[\n        models.FieldCondition(key='course_name', match=models.MatchValue(value=course_name)),\n    ])\n\n    found_docs = self.qdrant_client.search(\n        collection_name=os.environ['QDRANT_COLLECTION_NAME'],\n        query_filter=myfilter,\n        with_vectors=False,\n        query_vector=user_query_embedding,\n        limit=top_n  # Return 5 closest points\n    )\n    print(\"Search results: \", found_docs)\n    if len(found_docs) == 0:\n      return search_query\n\n    pre_prompt = \"Please answer the following question. Use the context below, called your documents, only if it's helpful and don't use parts that are very irrelevant. It's good to quote from your documents directly, when you do always use Markdown footnotes for citations. Use react-markdown superscript to number the sources at the end of sentences (1, 2, 3...) and use react-markdown Footnotes to list the full document names for each number. Use ReactMarkdown aka 'react-markdown' formatting for super script citations, use semi-formal style. Feel free to say you don't know. \\nHere's a few passages of the high quality documents:\\n\"\n\n    # count tokens at start and end, then also count each context.\n    token_counter, _ = count_tokens_and_cost(pre_prompt + '\\n\\nNow please respond to my query: ' +\n                                             search_query)  # type: ignore\n    valid_docs = []\n    for d in found_docs:\n      if d.payload is not None:\n        if \"pagenumber\" not in d.payload.keys():\n          d.payload[\"pagenumber\"] = d.payload[\"pagenumber_or_timestamp\"]\n\n        doc_string = f\"---\\nDocument: {d.payload['readable_filename']}{', page: ' + str(d.payload['pagenumber']) if d.payload['pagenumber'] else ''}\\n{d.payload.get('page_content')}\\n\"\n        num_tokens, prompt_cost = count_tokens_and_cost(doc_string)  # type: ignore\n\n        # print(f\"Page: {d.payload.get('page_content', ' '*100)[:100]}...\")\n        print(\n            f\"tokens used/limit: {token_counter}/{token_limit}, tokens in chunk: {num_tokens}, prompt cost of chunk: {prompt_cost}. \ud83d\udcc4 File: {d.payload.get('readable_filename', '')}\"\n        )\n        if token_counter + num_tokens &lt;= token_limit:\n          token_counter += num_tokens\n          valid_docs.append(\n              Document(page_content=d.payload.get('page_content', '&lt;Missing page content&gt;'), metadata=d.payload))\n        else:\n          continue\n\n    # Convert the valid_docs to full prompt\n    separator = '---\\n'  # between each context\n    context_text = separator.join(\n        f\"Document: {d.metadata['readable_filename']}{', page: ' + str(d.metadata['pagenumber']) if d.metadata['pagenumber'] else ''}\\n{d.page_content}\\n\"\n        for d in valid_docs)\n\n    # Create the stuffedPrompt\n    stuffedPrompt = (pre_prompt + context_text + '\\n\\nNow please respond to my query: ' + search_query)\n\n    TOTAL_num_tokens, prompt_cost = count_tokens_and_cost(stuffedPrompt, openai_model_name='gpt-4')  # type: ignore\n    print(f\"Total tokens: {TOTAL_num_tokens}, prompt_cost: {prompt_cost}\")\n    print(\"total docs: \", len(found_docs))\n    print(\"num docs used: \", len(valid_docs))\n\n    print(f\"\u23f0 ^^ Runtime of getTopContexts: {(time.monotonic() - start_time_overall):.2f} seconds\")\n    return stuffedPrompt\n  except Exception as e:\n    # return full traceback to front end\n    err: str = f\"Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    return err\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.ingest_coursera","title":"<code>ingest_coursera(coursera_course_name, course_name)</code>","text":"<p>Download all the files from a coursera course and ingest them.</p> <ol> <li>Download the coursera content.</li> <li>Upload to S3 (so users can view it)</li> <li>Run everything through the ingest_bulk method.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>coursera_course_name</code> <code>str</code> <p>The name of the coursera course.</p> required <code>course_name</code> <code>str</code> <p>The name of the course in our system.</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>str</code> <p>Success or error message.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def ingest_coursera(self, coursera_course_name: str, course_name: str) -&gt; str:\n  \"\"\" Download all the files from a coursera course and ingest them.\n\n  1. Download the coursera content.\n  2. Upload to S3 (so users can view it)\n  3. Run everything through the ingest_bulk method.\n\n  Args:\n      coursera_course_name (str): The name of the coursera course.\n      course_name (str): The name of the course in our system.\n\n  Returns:\n      _type_: Success or error message.\n  \"\"\"\n  certificate = \"-ca 'FVhVoDp5cb-ZaoRr5nNJLYbyjCLz8cGvaXzizqNlQEBsG5wSq7AHScZGAGfC1nI0ehXFvWy1NG8dyuIBF7DLMA.X3cXsDvHcOmSdo3Fyvg27Q.qyGfoo0GOHosTVoSMFy-gc24B-_BIxJtqblTzN5xQWT3hSntTR1DMPgPQKQmfZh_40UaV8oZKKiF15HtZBaLHWLbpEpAgTg3KiTiU1WSdUWueo92tnhz-lcLeLmCQE2y3XpijaN6G4mmgznLGVsVLXb-P3Cibzz0aVeT_lWIJNrCsXrTFh2HzFEhC4FxfTVqS6cRsKVskPpSu8D9EuCQUwJoOJHP_GvcME9-RISBhi46p-Z1IQZAC4qHPDhthIJG4bJqpq8-ZClRL3DFGqOfaiu5y415LJcH--PRRKTBnP7fNWPKhcEK2xoYQLr9RxBVL3pzVPEFyTYtGg6hFIdJcjKOU11AXAnQ-Kw-Gb_wXiHmu63veM6T8N2dEkdqygMre_xMDT5NVaP3xrPbA4eAQjl9yov4tyX4AQWMaCS5OCbGTpMTq2Y4L0Mbz93MHrblM2JL_cBYa59bq7DFK1IgzmOjFhNG266mQlC9juNcEhc'\"\n  always_use_flags = \"-u kastanvday@gmail.com -p hSBsLaF5YM469# --ignore-formats mp4 --subtitle-language en --path ./coursera-dl\"\n\n  try:\n    subprocess.run(\n        f\"coursera-dl {always_use_flags} {certificate} {coursera_course_name}\",\n        check=True,\n        shell=True,  # nosec -- reasonable bandit error suppression\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE)  # capture_output=True,\n    dl_results_path = os.path.join('coursera-dl', coursera_course_name)\n    s3_paths: Union[List, None] = upload_data_files_to_s3(course_name, dl_results_path)\n\n    if s3_paths is None:\n      return \"Error: No files found in the coursera-dl directory\"\n\n    print(\"starting bulk ingest\")\n    start_time = time.monotonic()\n    self.bulk_ingest(s3_paths, course_name)\n    print(\"completed bulk ingest\")\n    print(f\"\u23f0 Runtime: {(time.monotonic() - start_time):.2f} seconds\")\n\n    # Cleanup the coursera downloads\n    shutil.rmtree(dl_results_path)\n\n    return \"Success\"\n  except Exception as e:\n    err: str = f\"Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    return err\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.ingest_github","title":"<code>ingest_github(github_url, course_name)</code>","text":"<p>Clones the given GitHub URL and uses Langchain to load data. 1. Clone the repo 2. Use Langchain to load the data 3. Pass to split_and_upload() Args:     github_url (str): The Github Repo URL to be ingested.     course_name (str): The name of the course in our system.</p> <p>Returns:</p> Name Type Description <code>_type_</code> <code>str</code> <p>Success or error message.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def ingest_github(self, github_url: str, course_name: str) -&gt; str:\n  \"\"\"\n  Clones the given GitHub URL and uses Langchain to load data.\n  1. Clone the repo\n  2. Use Langchain to load the data\n  3. Pass to split_and_upload()\n  Args:\n      github_url (str): The Github Repo URL to be ingested.\n      course_name (str): The name of the course in our system.\n\n  Returns:\n      _type_: Success or error message.\n  \"\"\"\n  try:\n    repo_path = \"media/cloned_repo\"\n    repo = Repo.clone_from(github_url, to_path=repo_path, depth=1, clone_submodules=False)\n    branch = repo.head.reference\n\n    loader = GitLoader(repo_path=\"media/cloned_repo\", branch=str(branch))\n    data = loader.load()\n    shutil.rmtree(\"media/cloned_repo\")\n    # create metadata for each file in data\n\n    for doc in data:\n      texts = doc.page_content\n      metadatas: Dict[str, Any] = {\n          'course_name': course_name,\n          's3_path': '',\n          'readable_filename': doc.metadata['file_name'],\n          'url': f\"{github_url}/blob/main/{doc.metadata['file_path']}\",\n          'pagenumber': '',\n          'timestamp': '',\n      }\n      self.split_and_upload(texts=[texts], metadatas=[metadatas])\n    return \"Success\"\n  except Exception as e:\n    err = f\"\u274c\u274c Error in (GITHUB ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n{traceback.format_exc()}\"\n    print(err)\n    sentry_sdk.capture_exception(e)\n    return err\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.ingest_single_web_text","title":"<code>ingest_single_web_text(course_name, base_url, url, content, title)</code>","text":"<p>Crawlee integration</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def ingest_single_web_text(self, course_name: str, base_url: str, url: str, content: str, title: str):\n  \"\"\"Crawlee integration\n  \"\"\"\n  self.posthog.capture('distinct_id_of_the_user',\n                       event='ingest_single_web_text_invoked',\n                       properties={\n                           'course_name': course_name,\n                           'base_url': base_url,\n                           'url': url,\n                           'content': content,\n                           'title': title\n                       })\n  try:\n    # if not, ingest the text\n    text = [content]\n    metadatas: List[Dict[str, Any]] = [{\n        'course_name': course_name,\n        's3_path': '',\n        'readable_filename': title,\n        'pagenumber': '',\n        'timestamp': '',\n        'url': url,\n        'base_url': base_url,\n    }]\n    self.split_and_upload(texts=text, metadatas=metadatas)\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='ingest_single_web_text_succeeded',\n                         properties={\n                             'course_name': course_name,\n                             'base_url': base_url,\n                             'url': url,\n                             'title': title\n                         })\n\n    return \"Success\"\n  except Exception as e:\n    err = f\"\u274c\u274c Error in (web text ingest): `{inspect.currentframe().f_code.co_name}`: {e}\\nTraceback:\\n\", traceback.format_exc(\n    )  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    return err\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.reciprocal_rank_fusion","title":"<code>reciprocal_rank_fusion(results, k=60)</code>","text":"<p>Since we have multiple queries, and n documents returned per query, we need to go through all the results and collect the documents with the highest overall score, as scored by qdrant similarity matching.</p> Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def reciprocal_rank_fusion(self, results: list[list], k=60):\n  \"\"\"\n    Since we have multiple queries, and n documents returned per query, we need to go through all the results\n    and collect the documents with the highest overall score, as scored by qdrant similarity matching.\n    \"\"\"\n  fused_scores = {}\n  count = 0\n  unique_count = 0\n  for docs in results:\n    # Assumes the docs are returned in sorted order of relevance\n    count += len(docs)\n    for rank, doc in enumerate(docs):\n      doc_str = dumps(doc)\n      if doc_str not in fused_scores:\n        fused_scores[doc_str] = 0\n        unique_count += 1\n      fused_scores[doc_str] += 1 / (rank + k)\n      # Uncomment for debugging\n      # previous_score = fused_scores[doc_str]\n      #print(f\"Change score for doc: {doc_str}, previous score: {previous_score}, updated score: {fused_scores[doc_str]} \")\n  print(f\"Total number of documents in rank fusion: {count}\")\n  print(f\"Total number of unique documents in rank fusion: {unique_count}\")\n  reranked_results = [\n      (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n  ]\n  return reranked_results\n</code></pre>"},{"location":"api_reference/#ai_ta_backend.vector_database.Ingest.split_and_upload","title":"<code>split_and_upload(texts, metadatas)</code>","text":"<p>This is usually the last step of document ingest. Chunk &amp; upload to Qdrant (and Supabase.. todo). Takes in Text and Metadata (from Langchain doc loaders) and splits / uploads to Qdrant.</p> <p>good examples here: https://langchain.readthedocs.io/en/latest/modules/utils/combine_docs_examples/textsplitter.html</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>description</p> required <code>metadatas</code> <code>List[Dict[str, Any]]</code> <p>description</p> required Source code in <code>ai_ta_backend/vector_database.py</code> <pre><code>def split_and_upload(self, texts: List[str], metadatas: List[Dict[str, Any]]):\n  \"\"\" This is usually the last step of document ingest. Chunk &amp; upload to Qdrant (and Supabase.. todo).\n  Takes in Text and Metadata (from Langchain doc loaders) and splits / uploads to Qdrant.\n\n  good examples here: https://langchain.readthedocs.io/en/latest/modules/utils/combine_docs_examples/textsplitter.html\n\n  Args:\n      texts (List[str]): _description_\n      metadatas (List[Dict[str, Any]]): _description_\n  \"\"\"\n  self.posthog.capture('distinct_id_of_the_user',\n                       event='split_and_upload_invoked',\n                       properties={\n                           'course_name': metadatas[0].get('course_name', None),\n                           's3_path': metadatas[0].get('s3_path', None),\n                           'readable_filename': metadatas[0].get('readable_filename', None),\n                           'url': metadatas[0].get('url', None),\n                           'base_url': metadatas[0].get('base_url', None),\n                       })\n\n  print(\"In split and upload\")\n  print(f\"metadatas: {metadatas}\")\n  print(f\"Texts: {texts}\")\n  assert len(texts) == len(\n      metadatas\n  ), f'must have equal number of text strings and metadata dicts. len(texts) is {len(texts)}. len(metadatas) is {len(metadatas)}'\n\n  try:\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=1000,\n        chunk_overlap=150,\n        separators=[\n            \"\\n\\n\", \"\\n\", \". \", \" \", \"\"\n        ]  # try to split on paragraphs... fallback to sentences, then chars, ensure we always fit in context window\n    )\n    contexts: List[Document] = text_splitter.create_documents(texts=texts, metadatas=metadatas)\n    input_texts = [{'input': context.page_content, 'model': 'text-embedding-ada-002'} for context in contexts]\n\n    # check for duplicates\n    is_duplicate = self.check_for_duplicates(input_texts, metadatas)\n    if is_duplicate:\n      self.posthog.capture('distinct_id_of_the_user',\n                           event='split_and_upload_succeeded',\n                           properties={\n                               'course_name': metadatas[0].get('course_name', None),\n                               's3_path': metadatas[0].get('s3_path', None),\n                               'readable_filename': metadatas[0].get('readable_filename', None),\n                               'url': metadatas[0].get('url', None),\n                               'base_url': metadatas[0].get('base_url', None),\n                               'is_duplicate': True,\n                           })\n      return \"Success\"\n\n    # adding chunk index to metadata for parent doc retrieval\n    for i, context in enumerate(contexts):\n      context.metadata['chunk_index'] = i\n\n    oai = OpenAIAPIProcessor(\n        input_prompts_list=input_texts,\n        request_url='https://api.openai.com/v1/embeddings',\n        api_key=os.getenv('VLADS_OPENAI_KEY'),\n        # request_url='https://uiuc-chat-canada-east.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15',\n        # api_key=os.getenv('AZURE_OPENAI_KEY'),\n        max_requests_per_minute=5_000,\n        max_tokens_per_minute=300_000,\n        max_attempts=20,\n        logging_level=logging.INFO,\n        token_encoding_name='cl100k_base')  # nosec -- reasonable bandit error suppression\n    asyncio.run(oai.process_api_requests_from_file())\n    # parse results into dict of shape page_content -&gt; embedding\n    embeddings_dict: dict[str, List[float]] = {\n        item[0]['input']: item[1]['data'][0]['embedding'] for item in oai.results\n    }\n\n    ### BULK upload to Qdrant ###\n    vectors: list[PointStruct] = []\n    for context in contexts:\n      # !DONE: Updated the payload so each key is top level (no more payload.metadata.course_name. Instead, use payload.course_name), great for creating indexes.\n      upload_metadata = {**context.metadata, \"page_content\": context.page_content}\n      vectors.append(\n          PointStruct(id=str(uuid.uuid4()), vector=embeddings_dict[context.page_content], payload=upload_metadata))\n\n    self.qdrant_client.upsert(\n        collection_name=os.environ['QDRANT_COLLECTION_NAME'],  # type: ignore\n        points=vectors  # type: ignore\n    )\n    ### Supabase SQL ###\n    contexts_for_supa = [{\n        \"text\": context.page_content,\n        \"pagenumber\": context.metadata.get('pagenumber'),\n        \"timestamp\": context.metadata.get('timestamp'),\n        \"chunk_index\": context.metadata.get('chunk_index'),\n        \"embedding\": embeddings_dict[context.page_content]\n    } for context in contexts]\n\n    document = {\n        \"course_name\": contexts[0].metadata.get('course_name'),\n        \"s3_path\": contexts[0].metadata.get('s3_path'),\n        \"readable_filename\": contexts[0].metadata.get('readable_filename'),\n        \"url\": contexts[0].metadata.get('url'),\n        \"base_url\": contexts[0].metadata.get('base_url'),\n        \"contexts\": contexts_for_supa,\n    }\n\n    response = self.supabase_client.table(\n        os.getenv('NEW_NEW_NEWNEW_MATERIALS_SUPABASE_TABLE')).insert(document).execute()  # type: ignore\n\n    # add to Nomic document map\n    if len(response.data) &gt; 0:\n      inserted_data = response.data[0]\n      res = log_to_document_map(inserted_data)\n\n    self.posthog.capture('distinct_id_of_the_user',\n                         event='split_and_upload_succeeded',\n                         properties={\n                             'course_name': metadatas[0].get('course_name', None),\n                             's3_path': metadatas[0].get('s3_path', None),\n                             'readable_filename': metadatas[0].get('readable_filename', None),\n                             'url': metadatas[0].get('url', None),\n                             'base_url': metadatas[0].get('base_url', None),\n                         })\n    print(\"successful END OF split_and_upload\")\n    return \"Success\"\n  except Exception as e:\n    err: str = f\"ERROR IN split_and_upload(): Traceback: {traceback.extract_tb(e.__traceback__)}\u274c\u274c Error in {inspect.currentframe().f_code.co_name}:{e}\"  # type: ignore\n    print(err)\n    sentry_sdk.capture_exception(e)\n    return err\n</code></pre>"},{"location":"api_reference/#aws-endpoints","title":"AWS endpoints","text":""},{"location":"api_reference/#ai_ta_backend.aws.upload_data_files_to_s3","title":"<code>upload_data_files_to_s3(course_name, localdir)</code>","text":"<p>Uploads all files in localdir to S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>course_name</code> <code>str</code> <p>Official course name on our website.</p> required <code>localdir</code> <code>str</code> <p>Local directory to upload from, coursera-dl downloads to this directory.</p> required <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>Optional[List[str]]: A list of S3 paths, the final resting place of uploads, or None if no files were uploaded.</p> Source code in <code>ai_ta_backend/aws.py</code> <pre><code>def upload_data_files_to_s3(course_name: str, localdir: str) -&gt; Optional[List[str]]:\n  \"\"\"Uploads all files in localdir to S3 bucket.\n\n  Args:\n    course_name (str): Official course name on our website.\n    localdir (str): Local directory to upload from, coursera-dl downloads to this directory.\n\n  Returns:\n    Optional[List[str]]: A list of S3 paths, the final resting place of uploads, or None if no files were uploaded.\n  \"\"\"\n  s3 = boto3.client(\n      's3',\n      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n  )\n\n  filenames = []\n  for root, _subdirs, files in os.walk(localdir):\n    for filename in files:\n      filenames.append(os.path.join(root, filename))\n\n  if not filenames:\n    print(f\"No files to upload. Not found in: {localdir}\")\n    return None\n\n  print(f\"Files to upload: {filenames}\")\n  print(\"About to upload...\")\n\n  s3_paths = []\n  s3_paths_lock = Lock()\n\n  def upload(myfile):\n    # get the last part of the path and append unique ID before it\n    directory, old_filename = os.path.split(myfile)\n    new_filename = str(uuid.uuid4()) + '-' + old_filename\n    new_filepath = os.path.join(directory, new_filename)\n\n    s3_file = f\"courses/{course_name}/{os.path.basename(new_filepath)}\"\n    s3.upload_file(myfile, os.getenv('S3_BUCKET_NAME'), s3_file)\n    with s3_paths_lock:\n      s3_paths.append(s3_file)\n\n  # only 2 parallel uploads because we're getting rate limited with min_p=6... 503 errors.\n  min_p = 2\n  max_p = cpu_count()\n  num_procs = max(min(len(filenames), max_p), min_p)\n  pool = ThreadPool(processes=num_procs)\n  pool.map(upload, filenames)\n\n  print(\"All data files uploaded to S3 successfully.\")\n  return s3_paths\n</code></pre>"},{"location":"vector_search/","title":"Here's how to do vector search","text":"<p>TBD. See References for more info.</p>"}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentMetadata['additional_fields'] {'additional_field1': 'Additional Value 1', 'additional_field2': 'Additional Value 2'}\n",
      "Column additional_field1 added to documents.\n",
      "Column additional_field2 added to documents.\n",
      "Error in SQL upload: Error binding parameter 12 - probably unsupported type.\n",
      "SQLite connection is closed\n",
      "{'authors': ['Author One', 'Author Two'], 'journal_name': 'Journal Name', 'publication_date': datetime.date(2024, 4, 20), 'keywords': ['keyword1', 'keyword2'], 'doi': '12345', 'title': 'Title', 'subtitle': 'Subtitle', 'visible_urls': [], 'field_of_science': 'Field of Science', 'concise_summary': 'Summary', 'specific_questions_document_can_answer': ['Question 1', 'Question 2'], 'new_questions_document_can_answer': ['Question 3'], 'additional_fields': {'additional_field1': 'Additional Value 1', 'additional_field2': 'Additional Value 2'}}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from ai_ta_backend.utils.types import DocumentMetadata\n",
    "\n",
    "doc = DocumentMetadata(\n",
    "    authors=['Author One', 'Author Two'],\n",
    "    journal_name='Journal Name',\n",
    "    publication_date=datetime.date.today(),\n",
    "    keywords=['keyword1', 'keyword2'],\n",
    "    doi='12345',\n",
    "    title='Title',\n",
    "    subtitle='Subtitle',\n",
    "    visible_urls=[],\n",
    "    field_of_science='Field of Science',\n",
    "    concise_summary='Summary',\n",
    "    specific_questions_document_can_answer=['Question 1', 'Question 2'],\n",
    "\n",
    "    # ! TODO: Put all dynamic fileds here. Like section titles. \n",
    "    additional_fields={\n",
    "        'additional_field1': 'Additional Value 1',  # Arbitrary additional field\n",
    "        'additional_field2': 'Additional Value 2',  # Another arbitrary additional field\n",
    "    }\n",
    ")\n",
    "\n",
    "from SQLite import insert_doc\n",
    "insert_doc(doc, commit_on_change=True)\n",
    "\n",
    "print(doc.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = list(DocumentMetadata.schema()[\"properties\"].keys())\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marvin test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from ai_ta_backend.utils.types import DocumentMetadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random test paper copy/pasted from from ArXiv: https://arxiv.org/html/2404.09995v1\n",
    "raw_text = \"\"\"License: CC BY-NC-ND 4.0\n",
    "arXiv:2404.09995v1 [cs.CV] 15 Apr 2024\n",
    "(eccv) Package eccv Warning: Package 'hyperref' is loaded with option 'pagebackref', which is *not* recommended for camera-ready version\n",
    "\n",
    "1\n",
    "Taming Latent Diffusion Model for Neural Radiance Field Inpainting\n",
    "Chieh Hubert Lin\n",
    "1122\n",
    "Changil Kim\n",
    "11\n",
    "Jia-Bin Huang\n",
    "1133\n",
    "Qinbo Li\n",
    "11\n",
    "Chih Yao Ma\n",
    "11\n",
    "Johannes Kopf\n",
    "11\n",
    "Ming-Hsuan Yang\n",
    "22\n",
    "Hung-Yu Tseng\n",
    "11\n",
    "Abstract\n",
    "Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes.\n",
    "\n",
    "Refer to caption\n",
    "Figure 1:NeRF inpainting. Given a set of posed images associated with inpainting masks, the proposed framework estimates a NeRF that renders high-quality novel views, where the inpainting region is realistic and contains high-frequency details.\n",
    "1Introduction\n",
    "The recent advancements in neural radiance fields (NeRF) [24, 27, 3] have achieved high-quality 3D reconstruction and novel-view synthesis of scenes captured with a collection of images. The success intrigues an increasing attention on manipulating NeRFs such as 3D scene stylization [38, 8] and NeRF editing [13]. In this work, we focus on the NeRF inpainting problem. As shown in Figure 1, given a set of images of a scene with the inpainting masks, our goal is to estimate a completed NeRF that renders high-quality images at novel viewpoints. The NeRF inpainting task enables a variety of 3D content creation applications such as removing objects from a scene [26, 39], completing non-observed part of the scene, and hallucinating contents in the designated regions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marvin\n",
    "res = marvin.extract(raw_text, target=DocumentMetadata)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = res[0]\n",
    "from SQLite import insert_doc\n",
    "insert_doc(doc, commit_on_change=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Parsing with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM PARSING\n",
    "from openai import OpenAI # pip install openai>=1.0\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"irrelevant\", # any non-empty string\n",
    "    base_url = \"https://api.ncsa.ai/llm/v1\" ## üëà ONLY CODE CHANGE ##\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\", # way better than mistral instruct v0.2. Works great! As good as GPT-4 so far.\n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You are an expert at categorizing scientific papers. Please categorize the following paper.\"},\n",
    "        {\"role\": \"user\", \"content\": \"You are an expert at categorizing scientific papers. Please categorize the following paper.\\n\" + raw_text},\n",
    "    ],\n",
    "    extra_body={\"guided_json\": DocumentMetadata.schema()},\n",
    "    temperature=0.2,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# ‚ö°Ô∏è‚ö°Ô∏è streaming \n",
    "final = \"\"\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    final += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = DocumentMetadata.parse_raw(final)\n",
    "doc.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SQLite import insert_doc\n",
    "insert_doc(doc, commit_on_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the JSON data\n",
    "filepath = Path(\"/Users/kastanday/code/ncsa/ai-ta/ai-experiments/s2orc-doc2json/output_dir/N18-3011.json\")\n",
    "with open(filepath) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the grouped text\n",
    "grouped_text = {}\n",
    "\n",
    "# Iterate over each item in the \"body_text\" array\n",
    "# for entry in data\n",
    "for item in data[\"pdf_parse\"][\"body_text\"]:\n",
    "    text = item['text']\n",
    "    try: \n",
    "        sec_num = item['sec_num'].split('.')[0]  # Extract the major section number\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    # Append the text to the corresponding major section\n",
    "    if sec_num in grouped_text:\n",
    "        grouped_text[sec_num] += ' ' + text\n",
    "    else:\n",
    "        grouped_text[sec_num] = text\n",
    "\n",
    "# Create the output JSON object\n",
    "output = []\n",
    "for sec_num, all_text in grouped_text.items():\n",
    "    output.append({\n",
    "        'all_text': all_text,\n",
    "        'major_sec_num': sec_num\n",
    "    })\n",
    "\n",
    "# Save the output JSON object to a file\n",
    "with open('output.json', 'w') as file:\n",
    "    json.dump(output, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73080044-b2f7-49e2-a315-57753d871f8f",
   "metadata": {},
   "source": [
    "Langgraph LATS testing\n",
    "\n",
    "Implement LATS for each of the step.\n",
    "Have a generic LATS class.\n",
    "First, have LATS for planning step - the system prompt should be rna_seq planner prompt.\n",
    "Once the plan is solved, have LATS for each of the step.\n",
    "The plan generated in the first LATS will be given to a parser. Parse each of the planning steps\n",
    "Then the lats get steps one by one and the plan and each steps response will be given to the next lats class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc5a747b-c728-4fc8-af3d-f6f557bcfa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "946388e9-f23b-4de3-8a2c-4211f42a19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define language model\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "        azure_deployment=\"gpt-4-128k\",\n",
    "        openai_api_version=os.getenv(\"AZURE_0125_MODEL_VERSION\"),\n",
    "        temperature=0,\n",
    "        azure_endpoint=os.getenv(\"AZURE_0125_MODEL_ENDPOINT\"),\n",
    "        openai_api_key=os.getenv(\"AZURE_0125_MODEL_API_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abe3d21a-4dea-48e2-9d09-f2b1093fc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tools \n",
    "from langchain.agents import load_tools\n",
    "from langchain_community.tools import VectorStoreQATool\n",
    "from langchain.tools import (BaseTool, StructuredTool)\n",
    "\n",
    "from code_intrepreter_sanbox import E2B_class\n",
    "from vector_db import get_vectorstore_retriever_tool\n",
    "\n",
    "\n",
    "def get_tools(langsmith_run_id: str, sync=True):\n",
    "      search = load_tools([\"serpapi\"])\n",
    "      code_execution_class = E2B_class(langsmith_run_id=langsmith_run_id)\n",
    "      e2b_code_execution_tool = StructuredTool.from_function(\n",
    "          func=code_execution_class.run_python_code,\n",
    "          name=\"Python-Code-Execution\",\n",
    "          description=\"Executes Python3 code in an safe Docker container.\",\n",
    "      )\n",
    "      e2b_shell_tool = StructuredTool.from_function(\n",
    "          func=code_execution_class.run_shell,\n",
    "          name=\"Shell-commands-except-for-git\",\n",
    "          description=\n",
    "          \"Run shell commands to, for example, execute shell scripts or R scripts. It is in the same environment as the Code Execution tool.\",\n",
    "      )\n",
    "      docs_tools: List[VectorStoreQATool] = [\n",
    "          get_vectorstore_retriever_tool(\n",
    "              course_name='langchain-docs',\n",
    "              name='Langchain-docs',\n",
    "              description=\n",
    "              \"Build context-aware, reasoning applications with LangChain's flexible abstractions and AI-first toolkit.\"),\n",
    "          get_vectorstore_retriever_tool(\n",
    "              course_name='ml4bio-star',\n",
    "              name='STAR-docs',\n",
    "              description=\n",
    "              'Basic STAR workflow consists of 2 steps: (1) Generating genome indexes files and (2) Mapping reads to the genome'\n",
    "          ),\n",
    "          get_vectorstore_retriever_tool(\n",
    "              course_name='ml4bio-fastqc',\n",
    "              name='FastQC-docs',\n",
    "              description=\n",
    "              'FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. It works with data from BAM, SAM or FastQ files'\n",
    "          ),\n",
    "          get_vectorstore_retriever_tool(\n",
    "              course_name='ml4bio-multiqc',\n",
    "              name='MultiQC-docs',\n",
    "              description=\n",
    "              \"MultiQC is a reporting tool that parses results and statistics from bioinformatics tool outputs, such as log files and console outputs. It helps to summarize experiments containing multiple samples and multiple analysis steps. It's designed to be placed at the end of pipelines or to be run manually when you've finished running your tools.\"\n",
    "          ),\n",
    "          get_vectorstore_retriever_tool(\n",
    "              course_name='ml4bio-bioconductor',\n",
    "              name='Bioconductor-docs',\n",
    "              description=\n",
    "              \"Bioconductor is a project that contains hundreds of individual R packages. They're all high quality libraries that provide widespread access to a broad range of powerful statistical and graphical methods for the analysis of genomic data. Some of them also facilitate the inclusion of biological metadata in the analysis of genomic data, e.g. literature data from PubMed, annotation data from Entrez genes.\"\n",
    "          ),\n",
    "      ]\n",
    "\n",
    "      tools: list[BaseTool] =  search + docs_tools + [e2b_code_execution_tool, e2b_shell_tool] \n",
    "      return tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb6c5732-ae20-4b86-8e55-42a877fbcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:116: UserWarning: Api key is used with unsecure connection.\n",
      "  warnings.warn(\"Api key is used with unsecure connection.\")\n",
      "/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:116: UserWarning: Api key is used with unsecure connection.\n",
      "  warnings.warn(\"Api key is used with unsecure connection.\")\n",
      "/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:116: UserWarning: Api key is used with unsecure connection.\n",
      "  warnings.warn(\"Api key is used with unsecure connection.\")\n",
      "/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:116: UserWarning: Api key is used with unsecure connection.\n",
      "  warnings.warn(\"Api key is used with unsecure connection.\")\n",
      "/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:116: UserWarning: Api key is used with unsecure connection.\n",
      "  warnings.warn(\"Api key is used with unsecure connection.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='edf00a75c49d95767f0f7b99cddb763bea1145f2f94cf9f88879bbcab19c9a8f', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='edf00a75c49d95767f0f7b99cddb763bea1145f2f94cf9f88879bbcab19c9a8f', aiosession=None)>),\n",
       " VectorStoreQATool(name='Langchain-docs', description=\"Build context-aware, reasoning applications with LangChain's flexible abstractions and AI-first toolkit.\", vectorstore=<langchain_community.vectorstores.qdrant.Qdrant object at 0x14037e860>, llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x140174bb0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x136718610>, model_name='gpt-4-0613', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=180.0, max_retries=3, azure_endpoint='https://uiuc-chat-canada-east.openai.azure.com/', deployment_name='gpt-4-from-canada-east', openai_api_version='2023-05-15', openai_api_type='azure')),\n",
       " VectorStoreQATool(name='STAR-docs', description='Basic STAR workflow consists of 2 steps: (1) Generating genome indexes files and (2) Mapping reads to the genome', vectorstore=<langchain_community.vectorstores.qdrant.Qdrant object at 0x136731f30>, llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1367301c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1453ff730>, model_name='gpt-4-0613', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=180.0, max_retries=3, azure_endpoint='https://uiuc-chat-canada-east.openai.azure.com/', deployment_name='gpt-4-from-canada-east', openai_api_version='2023-05-15', openai_api_type='azure')),\n",
       " VectorStoreQATool(name='FastQC-docs', description='FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. It works with data from BAM, SAM or FastQ files', vectorstore=<langchain_community.vectorstores.qdrant.Qdrant object at 0x1453fc550>, llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1453f0dc0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1453f2260>, model_name='gpt-4-0613', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=180.0, max_retries=3, azure_endpoint='https://uiuc-chat-canada-east.openai.azure.com/', deployment_name='gpt-4-from-canada-east', openai_api_version='2023-05-15', openai_api_type='azure')),\n",
       " VectorStoreQATool(name='MultiQC-docs', description=\"MultiQC is a reporting tool that parses results and statistics from bioinformatics tool outputs, such as log files and console outputs. It helps to summarize experiments containing multiple samples and multiple analysis steps. It's designed to be placed at the end of pipelines or to be run manually when you've finished running your tools.\", vectorstore=<langchain_community.vectorstores.qdrant.Qdrant object at 0x1453deb90>, llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1453dd630>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1453dc190>, model_name='gpt-4-0613', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=180.0, max_retries=3, azure_endpoint='https://uiuc-chat-canada-east.openai.azure.com/', deployment_name='gpt-4-from-canada-east', openai_api_version='2023-05-15', openai_api_type='azure')),\n",
       " VectorStoreQATool(name='Bioconductor-docs', description=\"Bioconductor is a project that contains hundreds of individual R packages. They're all high quality libraries that provide widespread access to a broad range of powerful statistical and graphical methods for the analysis of genomic data. Some of them also facilitate the inclusion of biological metadata in the analysis of genomic data, e.g. literature data from PubMed, annotation data from Entrez genes.\", vectorstore=<langchain_community.vectorstores.qdrant.Qdrant object at 0x1453c8f70>, llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1453bb9d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1453ba530>, model_name='gpt-4-0613', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', request_timeout=180.0, max_retries=3, azure_endpoint='https://uiuc-chat-canada-east.openai.azure.com/', deployment_name='gpt-4-from-canada-east', openai_api_version='2023-05-15', openai_api_type='azure')),\n",
       " StructuredTool(name='Python-Code-Execution', description='Python-Code-Execution(code: str) - Executes Python3 code in an safe Docker container.', args_schema=<class 'pydantic.main.Python-Code-ExecutionSchema'>, func=<bound method E2B_class.run_python_code of <code_intrepreter_sanbox.E2B_class object at 0x13679d7e0>>),\n",
       " StructuredTool(name='Shell-commands-except-for-git', description='Shell-commands-except-for-git(shell_command: str) - Run shell commands to, for example, execute shell scripts or R scripts. It is in the same environment as the Code Execution tool.', args_schema=<class 'pydantic.main.Shell-commands-except-for-gitSchema'>, func=<bound method E2B_class.run_shell of <code_intrepreter_sanbox.E2B_class object at 0x13679d7e0>>)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread e2b-sandbox-refresh:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/sandbox/sandbox_connection.py\", line 431, in _refresh\n",
      "    api.sandboxes_sandbox_id_refreshes_post(\n",
      "  File \"pydantic/decorator.py\", line 40, in pydantic.decorator.validate_arguments.validate.wrapper_function\n",
      "    from contextlib import _GeneratorContextManager\n",
      "  File \"pydantic/decorator.py\", line 134, in pydantic.decorator.ValidatedFunction.call\n",
      "    \n",
      "  File \"pydantic/decorator.py\", line 206, in pydantic.decorator.ValidatedFunction.execute\n",
      "    \n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/api/sandboxes_api.py\", line 379, in sandboxes_sandbox_id_refreshes_post\n",
      "    return self.sandboxes_sandbox_id_refreshes_post_with_http_info(\n",
      "  File \"pydantic/decorator.py\", line 40, in pydantic.decorator.validate_arguments.validate.wrapper_function\n",
      "    from contextlib import _GeneratorContextManager\n",
      "  File \"pydantic/decorator.py\", line 134, in pydantic.decorator.ValidatedFunction.call\n",
      "    \n",
      "  File \"pydantic/decorator.py\", line 206, in pydantic.decorator.ValidatedFunction.execute\n",
      "    \n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/api/sandboxes_api.py\", line 492, in sandboxes_sandbox_id_refreshes_post_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/api_client.py\", line 466, in call_api\n",
      "    return self.__call_api(*args)\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/api_client.py\", line 242, in __call_api\n",
      "    raise e\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/api_client.py\", line 229, in __call_api\n",
      "    response_data = self.request(\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/api_client.py\", line 507, in request\n",
      "    return self.rest_client.post_request(\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/rest.py\", line 352, in post_request\n",
      "    return self.request(\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/api/v1/client/rest.py\", line 259, in request\n",
      "    raise NotFoundException(http_resp=r)\n",
      "e2b.api.v1.client.exceptions.NotFoundException: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'content-type': 'application/json; charset=utf-8', 'date': 'Mon, 11 Mar 2024 19:38:54 GMT', 'Content-Length': '97', 'via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"code\":404,\"message\":\"Error refreshing sandbox - sandbox 'idelq6lkojtjj5btki6qq' was not found\"}\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/minum/Documents/NCSA/UIUC-Chatbot/ai-ta-backend/.venv310/lib/python3.10/site-packages/e2b/sandbox/sandbox_connection.py\", line 438, in _refresh\n",
      "    raise SandboxException(\n",
      "e2b.sandbox.exception.SandboxException: Sandbox idelq6lkojtjj5btki6qq-22b47deb failed because it cannot be found\n"
     ]
    }
   ],
   "source": [
    "# define tools for the executor agent\n",
    "import uuid\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "\n",
    "langsmith_run_id = str(uuid.uuid4())  # for Langsmith\n",
    "tools = get_tools(langsmith_run_id)\n",
    "tool_executor = ToolExecutor(tools=tools)\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5372d8fb-4580-40d8-b84c-fbf13420219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate planner - first candidate - first lats\n",
    "planner_system_prompt = \"\"\"\n",
    "You are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\n",
    "\n",
    "First, write a step-by-step plan for the task. The plan should be descriptive and well-explained. \n",
    "\n",
    "The main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go.\n",
    "\n",
    "You have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\n",
    "\n",
    "When you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\n",
    "\n",
    "When referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\n",
    "\n",
    "Communicate with the user in Markdown.\n",
    "\n",
    "Conclude your plan with '<END_OF_PLAN>'.\n",
    "\n",
    "Verify the solution and provide evidence where possible.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "planner_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", planner_system_prompt),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "initial_planner_chain = planner_prompt_template | llm.bind_tools(tools=tools).with_config(\n",
    "    run_name=\"GenerateInitialPlannerCandidate\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dc88470-cb00-4453-9cc5-2facbac4f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_system_prompt = \"\"\"\n",
    "You are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\n",
    "\n",
    "Write code to achieve the given task. You have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\n",
    "\n",
    "When you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\n",
    "\n",
    "Before any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\n",
    "\n",
    "For package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\n",
    "\n",
    "When referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\n",
    "\n",
    "Prefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\n",
    "\n",
    "Include steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\n",
    "\n",
    "For code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\n",
    "\n",
    "Execute your code and provide results. If an error arises, correct it and provide the updated code. \n",
    "\n",
    "If a solution isn't reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\n",
    "\n",
    "Verify the solution and provide evidence where possible.\n",
    "\n",
    "End the interaction with \"TERMINATE\" once the task is completed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "executor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", executor_system_prompt),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "initial_executor_chain = executor_prompt_template | llm.bind_tools(tools=tools).with_config(\n",
    "    run_name=\"GenerateInitialExecutorCandidate\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca27d0bb-2387-466f-a8cc-44cef35467b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = JsonOutputToolsParser(return_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a348cc90-6e98-4063-82a9-e23440f28013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- MIGHT TRUNCATE MESSAGES ------- \n",
      "message content='\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a plan. Recap the plan between each code block. This recapping is necessary to maintain the context due to the short-term memory constraints.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nCommunicate with the user in Markdown.\\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go. Regularly commit to GitHub, aiming to push a comprehensive verbose notebook upon completion.\\n\\nConclude your plan with \\'<END_OF_PLAN>\\'.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nIf an error arises, correct it and provide the updated code. If a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESEQ2.'\n",
      "HumanMessage\n",
      "/\\/\\/\\/\\/\\ num_tokens_in_messages 566\n",
      "/\\/\\/\\/\\/\\ Hard coded context window size of: 120000\n",
      "message content='\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a plan. Recap the plan between each code block. This recapping is necessary to maintain the context due to the short-term memory constraints.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nCommunicate with the user in Markdown.\\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go. Regularly commit to GitHub, aiming to push a comprehensive verbose notebook upon completion.\\n\\nConclude your plan with \\'<END_OF_PLAN>\\'.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nIf an error arises, correct it and provide the updated code. If a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESEQ2.'\n",
      "HumanMessage\n",
      "message content='\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a plan. Recap the plan between each code block. This recapping is necessary to maintain the context due to the short-term memory constraints.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nCommunicate with the user in Markdown.\\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go. Regularly commit to GitHub, aiming to push a comprehensive verbose notebook upon completion.\\n\\nConclude your plan with \\'<END_OF_PLAN>\\'.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nIf an error arises, correct it and provide the updated code. If a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESEQ2.'\n",
      "HumanMessage\n",
      "message_dicts [{'role': 'system', 'content': '\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a plan. Recap the plan between each code block. This recapping is necessary to maintain the context due to the short-term memory constraints.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nCommunicate with the user in Markdown.\\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go. Regularly commit to GitHub, aiming to push a comprehensive verbose notebook upon completion.\\n\\nConclude your plan with \\'<END_OF_PLAN>\\'.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nIf an error arises, correct it and provide the updated code. If a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'}, {'role': 'user', 'content': 'Implement an RNA-Sequence Analysis Workflow using DESEQ2.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To implement an RNA-Sequence Analysis Workflow using DESeq2, we will follow a structured approach. DESeq2 is a popular R package used for analyzing count data from RNA sequencing experiments to find differentially expressed genes. The workflow will involve several steps, including quality control, alignment, counting, and differential expression analysis. Here\\'s a detailed plan:\\n\\n1. **Environment Setup**: Ensure R and necessary packages (DESeq2, Bioconductor) are installed.\\n2. **Quality Control**: Use FastQC to assess the quality of the raw sequencing data.\\n3. **Alignment**: Align the reads to a reference genome using a tool like STAR or HISAT2.\\n4. **Counting**: Count the number of reads that map to each gene using featureCounts or a similar tool.\\n5. **Differential Expression Analysis**: Use DESeq2 to perform differential expression analysis on the count data.\\n6. **Results Interpretation**: Interpret the results, focusing on significantly differentially expressed genes.\\n\\n### Step 1: Environment Setup\\n\\nBefore we start, we need to ensure that R is installed and set up correctly, along with the DESeq2 package from Bioconductor. This step will be executed in the Docker environment.\\n\\n```R\\n# filename: setup_environment.R\\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\\n    install.packages(\"BiocManager\")\\nBiocManager::install(\"DESeq2\")\\n```\\n\\n### Step 2: Quality Control with FastQC\\n\\nWe will use FastQC to perform quality control checks on the raw sequencing data. This step ensures that the data is of high quality before proceeding with further analysis.\\n\\n```shell\\n# This is a shell command to be executed in the Docker environment.\\nfastqc data/*.fastq.gz -o fastqc_results/\\n```\\n\\n### Step 3: Alignment with STAR\\n\\nNext, we align the reads to a reference genome using STAR. This step requires a pre-built genome index, which can be generated using STAR if not already available.\\n\\n```shell\\n# This is a shell command to be executed in the Docker environment.\\nSTAR --genomeDir /path/to/genomeIndex --readFilesIn sample1.fastq.gz sample2.fastq.gz --readFilesCommand zcat --outFileNamePrefix aligned/\\n```\\n\\n### Step 4: Counting with featureCounts\\n\\nAfter alignment, we count the number of reads mapping to each gene using featureCounts.\\n\\n```shell\\n# This is a shell command to be executed in the Docker environment.\\nfeatureCounts -a /path/to/annotation.gtf -o counts.txt aligned/Aligned.out.sam\\n```\\n\\n### Step 5: Differential Expression Analysis with DESeq2\\n\\nWe will then use DESeq2 for differential expression analysis on the count data.\\n\\n```R\\n# filename: deseq2_analysis.R\\nlibrary(DESeq2)\\ncountData <- read.csv(\"counts.txt\", sep=\"\\\\t\", header=TRUE, row.names=1)\\ncolData <- read.csv(\"colData.csv\", header=TRUE, row.names=1)\\ndds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ condition)\\ndds <- DESeq(dds)\\nres <- results(dds)\\nwrite.csv(as.data.frame(res), file=\"DESeq2_results.csv\")\\n```\\n\\n### Step 6: Results Interpretation\\n\\nFinally, we interpret the results from DESeq2, focusing on genes that are significantly differentially expressed.\\n\\nThis plan outlines the steps and tools required to implement an RNA-Sequence Analysis Workflow using DESeq2. Each step will be executed sequentially, ensuring the environment is correctly set up and all necessary packages and tools are installed.\\n\\n<END_OF_PLAN>', additional_kwargs={'tool_calls': [{'id': 'call_sbIlTEqjnrZSWyyjEExyhm6z', 'function': {'arguments': '{\"shell_command\":\"Rscript setup_environment.R\"}', 'name': 'Shell-commands-except-for-git'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_response = initial_answer_chain.invoke(\n",
    "    {\"input\": \"Implement an RNA-Sequence Analysis Workflow using DESEQ2.\"}\n",
    ")\n",
    "initial_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f06e2aa-f2b3-4946-bbbd-2d330840be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from collections import deque\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        reflection: Reflection,\n",
    "        parent: Optional[Node] = None,\n",
    "    ):\n",
    "        self.messages = messages\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "        self.visits = 0\n",
    "        self.reflection = reflection\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "        if self._is_solved:\n",
    "            self._mark_tree_as_solved()\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"<Node value={self.value}, visits={self.visits},\"\n",
    "            f\" solution={self.messages} reflection={self.reflection}/>\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def is_solved(self):\n",
    "        \"\"\"If any solutions exist, we can end the search.\"\"\"\n",
    "        return self._is_solved\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not self.children\n",
    "\n",
    "    @property\n",
    "    def best_child(self):\n",
    "        \"\"\"Select the child with the highest UCT to search next.\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: child.upper_confidence_bound())\n",
    "\n",
    "    @property\n",
    "    def best_child_score(self):\n",
    "        \"\"\"Return the child with the highest value.\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
    "\n",
    "    @property\n",
    "    def height(self) -> int:\n",
    "        \"\"\"Check for how far we've rolled out the tree.\"\"\"\n",
    "        if self.children:\n",
    "            return 1 + max([child.height for child in self.children])\n",
    "        return 1\n",
    "\n",
    "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
    "        \"\"\"Return the UCT score. This helps balance exploration vs. exploitation of a branch.\"\"\"\n",
    "        if self.parent is None:\n",
    "            raise ValueError(\"Cannot obtain UCT from root node\")\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "        # Encourages exploitation of high-value trajectories\n",
    "        average_reward = self.value / self.visits\n",
    "        # Encourages exploration of less-visited trajectories\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return average_reward + exploration_weight * exploration_term\n",
    "\n",
    "    def backpropagate(self, reward: float):\n",
    "        \"\"\"Update the score of this node and its parents.\"\"\"\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def get_messages(self, include_reflections: bool = True):\n",
    "        if include_reflections:\n",
    "            return self.messages + [self.reflection.as_message()]\n",
    "        return self.messages\n",
    "\n",
    "    def get_trajectory(self, include_reflections: bool = True) -> List[BaseMessage]:\n",
    "        \"\"\"Get messages representing this search branch.\"\"\"\n",
    "        messages = []\n",
    "        node = self\n",
    "        while node:\n",
    "            messages.extend(\n",
    "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
    "            )\n",
    "            node = node.parent\n",
    "        # Reverse the final back-tracked trajectory to return in the correct order\n",
    "        return messages[::-1]  # root solution, reflection, child 1, ...\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"Return the best solution from within the current sub-tree.\"\"\"\n",
    "        all_nodes = [self]\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "        best_node = max(\n",
    "            all_nodes,\n",
    "            # We filter out all non-terminal, non-solution trajectories\n",
    "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
    "        )\n",
    "        return best_node\n",
    "    \n",
    "    def _get_all_children(self):\n",
    "        all_nodes = []\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "        return all_nodes\n",
    "\n",
    "    def _mark_tree_as_solved(self):\n",
    "        parent = self.parent\n",
    "        while parent:\n",
    "            parent._is_solved = True\n",
    "            parent = parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7210358-b13e-458f-b8b1-6ccfa49ffc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph state\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class PlannerTreeState(TypedDict):\n",
    "    # The full tree\n",
    "    root: Node\n",
    "    # The original input\n",
    "    input: str\n",
    "\n",
    "\n",
    "class ExecutorTreeState(TypedDict):\n",
    "    # The full tree\n",
    "    root: Node\n",
    "    # The plan input - one step at a time\n",
    "    input: str\n",
    "    previous_steps: Annotated[List[str], operator.add] = []\n",
    "    previous_results: Annotated[List[str], operator.add] = []\n",
    "\n",
    "\n",
    "# after each plan execution, clean the tree\n",
    "# executortree.clear()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d61cef7-a56d-427a-95dc-3e28ef388d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the planner node we will add to the graph\n",
    "def generate_initial_planner_response(state: PlannerTreeState) -> dict:\n",
    "    \"\"\"Generate the initial candidate response.\"\"\"\n",
    "    res = initial_planner_chain.invoke({\"input\": state[\"input\"]})\n",
    "    parsed = parser.invoke(res)\n",
    "    tool_responses = tool_executor.batch(\n",
    "        [ToolInvocation(tool=r[\"type\"], tool_input=r[\"args\"]) for r in parsed]\n",
    "    )\n",
    "    output_messages = [res] + [\n",
    "        ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
    "        for resp, tool_call in zip(tool_responses, parsed)\n",
    "    ]\n",
    "    reflection = reflection_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
    "    )\n",
    "    root = Node(output_messages, reflection=reflection)\n",
    "    return {\n",
    "        **state,\n",
    "        \"root\": root,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11564697-80bd-433d-94c3-6876b259c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the executor node we will add to the graph\n",
    "def generate_initial_executor_response(state: ExecutorTreeState) -> dict:\n",
    "    \"\"\"Generate the initial candidate response.\"\"\"\n",
    "    res = initial_executor_chain.invoke({\"input\": state[\"input\"]})\n",
    "    state.previous_steps.append(state[\"input\"])\n",
    "    parsed = parser.invoke(res)\n",
    "    tool_responses = tool_executor.batch(\n",
    "        [ToolInvocation(tool=r[\"type\"], tool_input=r[\"args\"]) for r in parsed]\n",
    "    )\n",
    "    output_messages = [res] + [\n",
    "        ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
    "        for resp, tool_call in zip(tool_responses, parsed)\n",
    "    ]\n",
    "    reflection = reflection_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
    "    )\n",
    "    root = Node(output_messages, reflection=reflection)\n",
    "    return {\n",
    "        **state,\n",
    "        \"root\": root,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85f9d9a3-2cf4-452b-bd34-bea35cb4f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates N candidate values\n",
    "# for a single input to sample actions from the environment\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
    "    n = config[\"configurable\"].get(\"N\", 5)\n",
    "    bound_kwargs = llm.bind_tools(tools=tools).kwargs\n",
    "    print(\"bound kwargs\")\n",
    "    print(bound_kwargs)\n",
    "    chat_result = llm.generate(\n",
    "        [messages.to_messages()],\n",
    "        n=n,\n",
    "        callbacks=config[\"callbacks\"],\n",
    "        run_name=\"GenerateCandidates\",\n",
    "        **bound_kwargs\n",
    "    )\n",
    "    print(\"chat result : \", chat_result)\n",
    "    return [gen.message for gen in chat_result.generations[0]]\n",
    "\n",
    "\n",
    "expansion_chain = prompt_template | generate_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fd12716-adbd-4e92-8cd7-c276ee6daea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generation node\n",
    "# We will package the candidate generation and reflection steps in the following \"expand\" node. \n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "def plannerexpand(state: PlannerTreeState, config: RunnableConfig) -> dict:\n",
    "    \"\"\"Starting from the \"best\" node in the tree, generate N candidates for the next step.\"\"\"\n",
    "    print(\"RunnableConfig :\", config)\n",
    "    print(\"RunnableConfig configurable:\", config[\"configurable\"])\n",
    "    root = state[\"root\"]\n",
    "    best_candidate: Node = root.best_child if root.children else root\n",
    "    messages = best_candidate.get_trajectory()\n",
    "    print(\"messages :\")\n",
    "    print(messages)\n",
    "    # Generate N candidates from the single child candidate\n",
    "    new_candidates = expansion_chain.invoke(\n",
    "        {\"input\": state[\"input\"], \"messages\": messages}, config\n",
    "    )\n",
    "    parsed = parser.batch(new_candidates)\n",
    "    flattened = [\n",
    "        (i, tool_call)\n",
    "        for i, tool_calls in enumerate(parsed)\n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "    tool_responses = tool_executor.batch(\n",
    "        [\n",
    "            ToolInvocation(tool=tool_call[\"type\"], tool_input=tool_call[\"args\"])\n",
    "            for _, tool_call in flattened\n",
    "        ]\n",
    "    )\n",
    "    collected_responses = defaultdict(list)\n",
    "    for (i, tool_call), resp in zip(flattened, tool_responses):\n",
    "        collected_responses[i].append(\n",
    "            ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
    "        )\n",
    "    output_messages = []\n",
    "    for i, candidate in enumerate(new_candidates):\n",
    "        output_messages.append([candidate] + collected_responses[i])\n",
    "    print(\"candidate output messages\")\n",
    "    print(output_messages)\n",
    "    # Reflect on each candidate\n",
    "    # For tasks with external validation, you'd add that here.\n",
    "    reflections = reflection_chain.batch(\n",
    "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
    "        config,\n",
    "    )\n",
    "    # Grow tree\n",
    "    child_nodes = [\n",
    "        Node(cand, parent=best_candidate, reflection=reflection)\n",
    "        for cand, reflection in zip(output_messages, reflections)\n",
    "    ]\n",
    "    print(\"child nodes\")\n",
    "    print(child_nodes)\n",
    "    best_candidate.children.extend(child_nodes)\n",
    "    # We have already extended the tree directly, so we just return the state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5132b75c-98ef-4adf-bc47-b76afc8126e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generation node\n",
    "# We will package the candidate generation and reflection steps in the following \"expand\" node. \n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "def executorexpand(state: ExecutorTreeState, config: RunnableConfig) -> dict:\n",
    "    \"\"\"Starting from the \"best\" node in the tree, generate N candidates for the next step.\"\"\"\n",
    "    print(\"RunnableConfig :\", config)\n",
    "    print(\"RunnableConfig configurable:\", config[\"configurable\"])\n",
    "    root = state[\"root\"]\n",
    "    best_candidate: Node = root.best_child if root.children else root\n",
    "    messages = best_candidate.get_trajectory()\n",
    "    print(\"messages :\")\n",
    "    print(messages)\n",
    "    # Generate N candidates from the single child candidate\n",
    "    new_candidates = expansion_chain.invoke(\n",
    "        {\"input\": state[\"current_step\"], \"messages\": messages}, config\n",
    "    )\n",
    "    parsed = parser.batch(new_candidates)\n",
    "    flattened = [\n",
    "        (i, tool_call)\n",
    "        for i, tool_calls in enumerate(parsed)\n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "    tool_responses = tool_executor.batch(\n",
    "        [\n",
    "            ToolInvocation(tool=tool_call[\"type\"], tool_input=tool_call[\"args\"])\n",
    "            for _, tool_call in flattened\n",
    "        ]\n",
    "    )\n",
    "    collected_responses = defaultdict(list)\n",
    "    for (i, tool_call), resp in zip(flattened, tool_responses):\n",
    "        collected_responses[i].append(\n",
    "            ToolMessage(content=json.dumps(resp), tool_call_id=tool_call[\"id\"])\n",
    "        )\n",
    "    output_messages = []\n",
    "    for i, candidate in enumerate(new_candidates):\n",
    "        output_messages.append([candidate] + collected_responses[i])\n",
    "    print(\"candidate output messages\")\n",
    "    print(output_messages)\n",
    "    # Reflect on each candidate\n",
    "    # For tasks with external validation, you'd add that here.\n",
    "    reflections = reflection_chain.batch(\n",
    "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
    "        config,\n",
    "    )\n",
    "    # Grow tree\n",
    "    child_nodes = [\n",
    "        Node(cand, parent=best_candidate, reflection=reflection)\n",
    "        for cand, reflection in zip(output_messages, reflections)\n",
    "    ]\n",
    "    print(\"child nodes\")\n",
    "    print(child_nodes)\n",
    "    best_candidate.children.extend(child_nodes)\n",
    "    print(\"best_candidate\")\n",
    "    print(best_candidate)\n",
    "    state[\"previous_steps\"].append(state[\"current_step\"])\n",
    "    state[\"previous_results\"].append(best_candidate.messages[-1])\n",
    "    # We have already extended the tree directly, so we just return the state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f34882e-2720-4880-95cc-191d4e55a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph\n",
    "# With those two nodes defined, we are ready to define the graph. After each agent step, we have the option of finishing.\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "def should_loop(state: TreeState):\n",
    "    \"\"\"Determine whether to continue the tree search.\"\"\"\n",
    "    root = state[\"root\"]\n",
    "    if root.is_solved:\n",
    "        return END\n",
    "    if root.height > 5:\n",
    "        return END\n",
    "    return \"expand\"\n",
    "\n",
    "\n",
    "planner_builder = StateGraph(PlannerTreeState)\n",
    "planner_builder.add_node(\"start\", generate_initial_planner_response)\n",
    "planner_builder.add_node(\"expand\", plannerexpand)\n",
    "planner_builder.set_entry_point(\"start\")\n",
    "\n",
    "\n",
    "planner_builder.add_conditional_edges(\n",
    "    \"start\",\n",
    "    # Either expand/rollout or finish\n",
    "    should_loop,\n",
    ")\n",
    "planner_builder.add_conditional_edges(\n",
    "    \"expand\",\n",
    "    # Either continue to rollout or finish\n",
    "    should_loop,\n",
    ")\n",
    "\n",
    "planner_graph = planner_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5265edb-3b4d-43aa-bffb-61b5f5885718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- MIGHT TRUNCATE MESSAGES ------- \n",
      "message content=\"\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a step-by-step plan for the task. The plan should be descriptive and well-explained. \\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nCommunicate with the user in Markdown.\\n\\nConclude your plan with '<END_OF_PLAN>'.\\n\\nVerify the solution and provide evidence where possible.\\n\\n\"\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESeq2.'\n",
      "HumanMessage\n",
      "/\\/\\/\\/\\/\\ num_tokens_in_messages 270\n",
      "/\\/\\/\\/\\/\\ Hard coded context window size of: 120000\n",
      "message content=\"\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a step-by-step plan for the task. The plan should be descriptive and well-explained. \\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nCommunicate with the user in Markdown.\\n\\nConclude your plan with '<END_OF_PLAN>'.\\n\\nVerify the solution and provide evidence where possible.\\n\\n\"\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESeq2.'\n",
      "HumanMessage\n",
      "message content=\"\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a step-by-step plan for the task. The plan should be descriptive and well-explained. \\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nCommunicate with the user in Markdown.\\n\\nConclude your plan with '<END_OF_PLAN>'.\\n\\nVerify the solution and provide evidence where possible.\\n\\n\"\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESeq2.'\n",
      "HumanMessage\n",
      "message_dicts [{'role': 'system', 'content': \"\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nFirst, write a step-by-step plan for the task. The plan should be descriptive and well-explained. \\n\\nThe main objective is to plan and execute the workflow efficiently. Break down the execution into small, informed steps rather than attempting everything in one go.\\n\\nYou have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nCommunicate with the user in Markdown.\\n\\nConclude your plan with '<END_OF_PLAN>'.\\n\\nVerify the solution and provide evidence where possible.\\n\\n\"}, {'role': 'user', 'content': 'Implement an RNA-Sequence Analysis Workflow using DESeq2.'}]\n",
      " ------- MIGHT TRUNCATE MESSAGES ------- \n",
      "message content='Reflect and grade the assistant response to the user question below.'\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESeq2.'\n",
      "HumanMessage\n",
      "message content=\"To implement an RNA-Sequence Analysis Workflow using DESeq2, we will follow a structured approach. DESeq2 is a method for differential gene expression analysis based on the negative binomial distribution. It's widely used in bioinformatics for analyzing count data from RNA sequencing experiments. The workflow will involve several steps, from quality control of the raw data to the final differential expression analysis using DESeq2. Here's a step-by-step plan:\\n\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n\\n### Step 2: Preprocessing and Cleaning\\n- **2.1.** If necessary, use tools like Trimmomatic or Cutadapt to trim adapters and low-quality bases.\\n- **2.2.** Optionally, use FastQC again to check the quality of the data post-trimming.\\n\\n### Step 3: Alignment\\n- **3.1.** Align the cleaned reads to the reference genome using an aligner like STAR or HISAT2.\\n- **3.2.** Convert the output SAM files to BAM files for easier handling.\\n\\n### Step 4: Quantification\\n- **4.1.** Use featureCounts (from the Subread package) or a similar tool to count the number of reads mapping to each gene.\\n\\n### Step 5: Differential Expression Analysis with DESeq2\\n- **5.1.** Prepare the count matrix and the metadata (experimental design) for DESeq2 analysis.\\n- **5.2.** Use DESeq2 (within the R environment) to perform differential expression analysis.\\n- **5.3.** Interpret the results, focusing on significantly differentially expressed genes.\\n\\n### Step 6: Visualization and Further Analysis\\n- **6.1.** Generate plots (MA plot, volcano plot) to visualize the results.\\n- **6.2.** Perform additional analyses as needed, such as GO enrichment or pathway analysis.\\n\\n### Step 7: Reporting\\n- **7.1.** Use MultiQC or a similar tool to compile the results and statistics from the various steps into a comprehensive report.\\n\\n### Execution Plan:\\n- **Execution of Steps 1 & 2:** We will start by executing FastQC for quality control and, if necessary, proceed with data cleaning using a trimming tool.\\n- **Execution of Steps 3 & 4:** Next, we will align the reads to the reference genome and perform quantification.\\n- **Execution of Steps 5, 6, & 7:** Finally, we will carry out the differential expression analysis using DESeq2, visualize the results, and compile a comprehensive report.\\n\\nFor each of these steps, we will use the appropriate tools and scripts, ensuring that the workflow is executed efficiently and accurately. This plan outlines a comprehensive approach to RNA-Seq analysis, from raw data to differential expression analysis using DESeq2.\\n\\n<END_OF_PLAN>\"\n",
      "AIMessage\n",
      "/\\/\\/\\/\\/\\ num_tokens_in_messages 663\n",
      "/\\/\\/\\/\\/\\ Hard coded context window size of: 120000\n",
      "message content='Reflect and grade the assistant response to the user question below.'\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESeq2.'\n",
      "HumanMessage\n",
      "message content=\"To implement an RNA-Sequence Analysis Workflow using DESeq2, we will follow a structured approach. DESeq2 is a method for differential gene expression analysis based on the negative binomial distribution. It's widely used in bioinformatics for analyzing count data from RNA sequencing experiments. The workflow will involve several steps, from quality control of the raw data to the final differential expression analysis using DESeq2. Here's a step-by-step plan:\\n\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n\\n### Step 2: Preprocessing and Cleaning\\n- **2.1.** If necessary, use tools like Trimmomatic or Cutadapt to trim adapters and low-quality bases.\\n- **2.2.** Optionally, use FastQC again to check the quality of the data post-trimming.\\n\\n### Step 3: Alignment\\n- **3.1.** Align the cleaned reads to the reference genome using an aligner like STAR or HISAT2.\\n- **3.2.** Convert the output SAM files to BAM files for easier handling.\\n\\n### Step 4: Quantification\\n- **4.1.** Use featureCounts (from the Subread package) or a similar tool to count the number of reads mapping to each gene.\\n\\n### Step 5: Differential Expression Analysis with DESeq2\\n- **5.1.** Prepare the count matrix and the metadata (experimental design) for DESeq2 analysis.\\n- **5.2.** Use DESeq2 (within the R environment) to perform differential expression analysis.\\n- **5.3.** Interpret the results, focusing on significantly differentially expressed genes.\\n\\n### Step 6: Visualization and Further Analysis\\n- **6.1.** Generate plots (MA plot, volcano plot) to visualize the results.\\n- **6.2.** Perform additional analyses as needed, such as GO enrichment or pathway analysis.\\n\\n### Step 7: Reporting\\n- **7.1.** Use MultiQC or a similar tool to compile the results and statistics from the various steps into a comprehensive report.\\n\\n### Execution Plan:\\n- **Execution of Steps 1 & 2:** We will start by executing FastQC for quality control and, if necessary, proceed with data cleaning using a trimming tool.\\n- **Execution of Steps 3 & 4:** Next, we will align the reads to the reference genome and perform quantification.\\n- **Execution of Steps 5, 6, & 7:** Finally, we will carry out the differential expression analysis using DESeq2, visualize the results, and compile a comprehensive report.\\n\\nFor each of these steps, we will use the appropriate tools and scripts, ensuring that the workflow is executed efficiently and accurately. This plan outlines a comprehensive approach to RNA-Seq analysis, from raw data to differential expression analysis using DESeq2.\\n\\n<END_OF_PLAN>\"\n",
      "AIMessage\n",
      "message content='Reflect and grade the assistant response to the user question below.'\n",
      "message content='Implement an RNA-Sequence Analysis Workflow using DESeq2.'\n",
      "HumanMessage\n",
      "message content=\"To implement an RNA-Sequence Analysis Workflow using DESeq2, we will follow a structured approach. DESeq2 is a method for differential gene expression analysis based on the negative binomial distribution. It's widely used in bioinformatics for analyzing count data from RNA sequencing experiments. The workflow will involve several steps, from quality control of the raw data to the final differential expression analysis using DESeq2. Here's a step-by-step plan:\\n\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n\\n### Step 2: Preprocessing and Cleaning\\n- **2.1.** If necessary, use tools like Trimmomatic or Cutadapt to trim adapters and low-quality bases.\\n- **2.2.** Optionally, use FastQC again to check the quality of the data post-trimming.\\n\\n### Step 3: Alignment\\n- **3.1.** Align the cleaned reads to the reference genome using an aligner like STAR or HISAT2.\\n- **3.2.** Convert the output SAM files to BAM files for easier handling.\\n\\n### Step 4: Quantification\\n- **4.1.** Use featureCounts (from the Subread package) or a similar tool to count the number of reads mapping to each gene.\\n\\n### Step 5: Differential Expression Analysis with DESeq2\\n- **5.1.** Prepare the count matrix and the metadata (experimental design) for DESeq2 analysis.\\n- **5.2.** Use DESeq2 (within the R environment) to perform differential expression analysis.\\n- **5.3.** Interpret the results, focusing on significantly differentially expressed genes.\\n\\n### Step 6: Visualization and Further Analysis\\n- **6.1.** Generate plots (MA plot, volcano plot) to visualize the results.\\n- **6.2.** Perform additional analyses as needed, such as GO enrichment or pathway analysis.\\n\\n### Step 7: Reporting\\n- **7.1.** Use MultiQC or a similar tool to compile the results and statistics from the various steps into a comprehensive report.\\n\\n### Execution Plan:\\n- **Execution of Steps 1 & 2:** We will start by executing FastQC for quality control and, if necessary, proceed with data cleaning using a trimming tool.\\n- **Execution of Steps 3 & 4:** Next, we will align the reads to the reference genome and perform quantification.\\n- **Execution of Steps 5, 6, & 7:** Finally, we will carry out the differential expression analysis using DESeq2, visualize the results, and compile a comprehensive report.\\n\\nFor each of these steps, we will use the appropriate tools and scripts, ensuring that the workflow is executed efficiently and accurately. This plan outlines a comprehensive approach to RNA-Seq analysis, from raw data to differential expression analysis using DESeq2.\\n\\n<END_OF_PLAN>\"\n",
      "AIMessage\n",
      "message_dicts [{'role': 'system', 'content': 'Reflect and grade the assistant response to the user question below.'}, {'role': 'user', 'content': 'Implement an RNA-Sequence Analysis Workflow using DESeq2.'}, {'role': 'assistant', 'content': \"To implement an RNA-Sequence Analysis Workflow using DESeq2, we will follow a structured approach. DESeq2 is a method for differential gene expression analysis based on the negative binomial distribution. It's widely used in bioinformatics for analyzing count data from RNA sequencing experiments. The workflow will involve several steps, from quality control of the raw data to the final differential expression analysis using DESeq2. Here's a step-by-step plan:\\n\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n\\n### Step 2: Preprocessing and Cleaning\\n- **2.1.** If necessary, use tools like Trimmomatic or Cutadapt to trim adapters and low-quality bases.\\n- **2.2.** Optionally, use FastQC again to check the quality of the data post-trimming.\\n\\n### Step 3: Alignment\\n- **3.1.** Align the cleaned reads to the reference genome using an aligner like STAR or HISAT2.\\n- **3.2.** Convert the output SAM files to BAM files for easier handling.\\n\\n### Step 4: Quantification\\n- **4.1.** Use featureCounts (from the Subread package) or a similar tool to count the number of reads mapping to each gene.\\n\\n### Step 5: Differential Expression Analysis with DESeq2\\n- **5.1.** Prepare the count matrix and the metadata (experimental design) for DESeq2 analysis.\\n- **5.2.** Use DESeq2 (within the R environment) to perform differential expression analysis.\\n- **5.3.** Interpret the results, focusing on significantly differentially expressed genes.\\n\\n### Step 6: Visualization and Further Analysis\\n- **6.1.** Generate plots (MA plot, volcano plot) to visualize the results.\\n- **6.2.** Perform additional analyses as needed, such as GO enrichment or pathway analysis.\\n\\n### Step 7: Reporting\\n- **7.1.** Use MultiQC or a similar tool to compile the results and statistics from the various steps into a comprehensive report.\\n\\n### Execution Plan:\\n- **Execution of Steps 1 & 2:** We will start by executing FastQC for quality control and, if necessary, proceed with data cleaning using a trimming tool.\\n- **Execution of Steps 3 & 4:** Next, we will align the reads to the reference genome and perform quantification.\\n- **Execution of Steps 5, 6, & 7:** Finally, we will carry out the differential expression analysis using DESeq2, visualize the results, and compile a comprehensive report.\\n\\nFor each of these steps, we will use the appropriate tools and scripts, ensuring that the workflow is executed efficiently and accurately. This plan outlines a comprehensive approach to RNA-Seq analysis, from raw data to differential expression analysis using DESeq2.\\n\\n<END_OF_PLAN>\"}]\n",
      "tool choices :  [Reflection(reflections=\"The response provides a comprehensive and structured plan for implementing an RNA-Sequence Analysis Workflow using DESeq2, covering all essential steps from quality control to differential expression analysis and reporting. It outlines the use of various tools and techniques at each stage, ensuring a clear understanding of the workflow. However, the response could have been enhanced by including example commands or code snippets for some of the critical steps, especially for running DESeq2, which is the core of the user's request. This would have made the guide more practical and actionable for someone looking to implement the workflow. Additionally, mentioning the need for basic R programming skills for using DESeq2 and suggesting resources for users unfamiliar with R or the specific tools could have provided more comprehensive support.\", score=8, found_solution=True)]\n",
      "start\n",
      "rolled out:  1\n",
      "---\n",
      "__end__\n",
      "rolled out:  1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# invoke\n",
    "\n",
    "question = \"Implement an RNA-Sequence Analysis Workflow using DESeq2.\"\n",
    "for step in planner_graph.stream({\"input\": question}):\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print(\"rolled out: \", step_state[\"root\"].height)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb8a8ef3-78df-49f0-8682-bb241e28f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement an RNA-Sequence Analysis Workflow using DESeq2, we will follow a structured approach. DESeq2 is a method for differential gene expression analysis based on the negative binomial distribution. It's widely used in bioinformatics for analyzing count data from RNA sequencing experiments. The workflow will involve several steps, from quality control of the raw data to the final differential expression analysis using DESeq2. Here's a step-by-step plan:\n",
      "\n",
      "### Step 1: Quality Control of Raw Data\n",
      "- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\n",
      "- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\n",
      "\n",
      "### Step 2: Preprocessing and Cleaning\n",
      "- **2.1.** If necessary, use tools like Trimmomatic or Cutadapt to trim adapters and low-quality bases.\n",
      "- **2.2.** Optionally, use FastQC again to check the quality of the data post-trimming.\n",
      "\n",
      "### Step 3: Alignment\n",
      "- **3.1.** Align the cleaned reads to the reference genome using an aligner like STAR or HISAT2.\n",
      "- **3.2.** Convert the output SAM files to BAM files for easier handling.\n",
      "\n",
      "### Step 4: Quantification\n",
      "- **4.1.** Use featureCounts (from the Subread package) or a similar tool to count the number of reads mapping to each gene.\n",
      "\n",
      "### Step 5: Differential Expression Analysis with DESeq2\n",
      "- **5.1.** Prepare the count matrix and the metadata (experimental design) for DESeq2 analysis.\n",
      "- **5.2.** Use DESeq2 (within the R environment) to perform differential expression analysis.\n",
      "- **5.3.** Interpret the results, focusing on significantly differentially expressed genes.\n",
      "\n",
      "### Step 6: Visualization and Further Analysis\n",
      "- **6.1.** Generate plots (MA plot, volcano plot) to visualize the results.\n",
      "- **6.2.** Perform additional analyses as needed, such as GO enrichment or pathway analysis.\n",
      "\n",
      "### Step 7: Reporting\n",
      "- **7.1.** Use MultiQC or a similar tool to compile the results and statistics from the various steps into a comprehensive report.\n",
      "\n",
      "### Execution Plan:\n",
      "- **Execution of Steps 1 & 2:** We will start by executing FastQC for quality control and, if necessary, proceed with data cleaning using a trimming tool.\n",
      "- **Execution of Steps 3 & 4:** Next, we will align the reads to the reference genome and perform quantification.\n",
      "- **Execution of Steps 5, 6, & 7:** Finally, we will carry out the differential expression analysis using DESeq2, visualize the results, and compile a comprehensive report.\n",
      "\n",
      "For each of these steps, we will use the appropriate tools and scripts, ensuring that the workflow is executed efficiently and accurately. This plan outlines a comprehensive approach to RNA-Seq analysis, from raw data to differential expression analysis using DESeq2.\n",
      "\n",
      "<END_OF_PLAN>\n"
     ]
    }
   ],
   "source": [
    "planner_solution_node = step[\"__end__\"][\"root\"].get_best_solution()\n",
    "planner_best_trajectory = planner_solution_node.get_trajectory(include_reflections=False)\n",
    "print(planner_best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9a44b31-2e5b-42c7-a92a-022ec37b4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_builder = StateGraph(ExecutorTreeState)\n",
    "executor_builder.add_node(\"start\", generate_initial_executor_response)\n",
    "executor_builder.add_node(\"expand\", executorexpand)\n",
    "executor_builder.set_entry_point(\"start\")\n",
    "\n",
    "\n",
    "executor_builder.add_conditional_edges(\n",
    "    \"start\",\n",
    "    # Either expand/rollout or finish\n",
    "    should_loop,\n",
    ")\n",
    "executor_builder.add_conditional_edges(\n",
    "    \"expand\",\n",
    "    # Either continue to rollout or finish\n",
    "    should_loop,\n",
    ")\n",
    "\n",
    "executor_graph = executor_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06aa3920-9385-4081-acf9-69bda9a8b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_plan_step = \"\"\"\n",
    "### Step 1: Quality Control of Raw Data\n",
    "- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\n",
    "- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f87d204-e0e0-45bc-ac23-f43e37277cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- MIGHT TRUNCATE MESSAGES ------- \n",
      "message content='\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nWrite code to achieve the given task. You have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nExecute your code and provide results. If an error arises, correct it and provide the updated code. \\n\\nIf a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'\n",
      "message content='\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'\n",
      "HumanMessage\n",
      "/\\/\\/\\/\\/\\ num_tokens_in_messages 532\n",
      "/\\/\\/\\/\\/\\ Hard coded context window size of: 120000\n",
      "message content='\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nWrite code to achieve the given task. You have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nExecute your code and provide results. If an error arises, correct it and provide the updated code. \\n\\nIf a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'\n",
      "message content='\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'\n",
      "HumanMessage\n",
      "message content='\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nWrite code to achieve the given task. You have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nExecute your code and provide results. If an error arises, correct it and provide the updated code. \\n\\nIf a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'\n",
      "message content='\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'\n",
      "HumanMessage\n",
      "message_dicts [{'role': 'system', 'content': '\\nYou are a world-class programmer and AI assistant capable of executing any goal related to software development, genAI, LLMs, and full-stack technologies.\\n\\nWrite code to achieve the given task. You have access to a variety of tools, including browser, github_tools for interacting with GitHub, and multiple vectorstore instances. Utilize the browser for internet searches and github_tools for all interactions with GitHub repositories. For code execution, rely on PythonRepl and shell tools available in the Docker environment.\\n\\nWhen you send a message containing code, it will be executed in a Docker container. You have been granted full permission to execute any code necessary to complete the task within this Docker environment using PythonRepl and shell tools as required.\\n\\nBefore any execution task, prepare the development environment, whether that be a notebook, .sh, .py, .ipynb, .R, or other file types. Incrementally develop, execute, and debug the code, committing changes to GitHub regularly.\\n\\nFor package installations, use pip, and strive to install all necessary packages in a single command. In case of failures, debug and install them correctly, adhering to the Pydantic structure to avoid validation errors.\\n\\nWhen referencing files, assume they exist in the GitHub repository. Use github_tools for all interactions with GitHub and operate within the current working directory.\\n\\nPrefer universally applicable packages that are likely to be pre-installed and compatible across different applications. For example, ffmpeg and pandoc are recommended for their broad support and functionality.\\n\\nInclude steps and EXACT CODE SNIPPETS if they are applicable to the task. Do not suggest code that requires user modifications, and ensure all code is complete and executable as is.\\n\\nFor code that needs to be saved to a file, indicate this with # filename: <filename> at the start of the code block. Only include one code block per response and avoid asking users to copy and paste results. Use the print function for outputs.\\n\\nExecute your code and provide results. If an error arises, correct it and provide the updated code. \\n\\nIf a solution isn\\'t reached after successful code execution, reassess your approach, gather more information, and propose an alternative method.\\n\\nVerify the solution and provide evidence where possible.\\n\\nEnd the interaction with \"TERMINATE\" once the task is completed.\\n\\n'}, {'role': 'user', 'content': '\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'}]\n",
      " ------- MIGHT TRUNCATE MESSAGES ------- \n",
      "message content='Reflect and grade the assistant response to the user question below.'\n",
      "message content='\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'\n",
      "HumanMessage\n",
      "message content='To perform the initial quality checks on the raw sequencing data using FastQC, we\\'ll simulate the process since we can\\'t directly run FastQC in this environment. However, I\\'ll guide you through the steps you would take and provide an example of how to interpret the results.\\n\\n#### Step 1.1: Using FastQC for Initial Quality Checks\\n\\n1. **Installation**: If FastQC is not already installed, you can download it from the FastQC website or install it using Conda or Homebrew for macOS/Linux users. For example, using Conda:\\n   ```sh\\n   conda install -c bioconda fastqc\\n   ```\\n\\n2. **Running FastQC**: To run FastQC on your raw sequencing data, you would use the following command in your terminal (assuming you have FastQC installed and your data is in FastQ format):\\n   ```sh\\n   fastqc your_data.fastq\\n   ```\\n   Replace `your_data.fastq` with the path to your actual data file. If you have multiple files, you can run FastQC on all of them by specifying each file name or using wildcards.\\n\\n3. **Output**: FastQC will generate an HTML report for each input file, providing a detailed analysis of the data quality. These reports include information on various quality metrics such as per base sequence quality, sequence duplication levels, and overrepresented sequences.\\n\\n#### Step 1.2: Interpreting FastQC Reports and Deciding on Preprocessing\\n\\nAfter running FastQC, you\\'ll need to review the generated reports to assess the quality of your sequencing data. Here are some key sections to focus on:\\n\\n- **Per Base Sequence Quality**: This section shows the quality scores across all bases. Ideally, the plot should show most bases with high scores (green). If you observe a significant drop in quality towards the ends of reads, you might consider trimming those ends.\\n\\n- **Sequence Duplication Levels**: High duplication levels can indicate PCR artifacts. Depending on your experiment, you may need to remove duplicates.\\n\\n- **Overrepresented Sequences**: This section lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might need to trim or filter out.\\n\\nBased on these and other sections of the FastQC report, you can decide if your data requires preprocessing such as trimming for quality or removing adapters. Tools like Trimmomatic or Cutadapt are commonly used for this purpose.\\n\\nSince we can\\'t run FastQC directly here, let\\'s simulate the decision-making process based on a hypothetical FastQC report:\\n\\n- Assume the \"Per Base Sequence Quality\" section shows a significant quality drop at the ends of reads.\\n- The \"Sequence Duplication Levels\" are within acceptable limits.\\n- \"Overrepresented Sequences\" indicates the presence of adapter sequences.\\n\\nBased on this, we would decide to preprocess the data by trimming the low-quality ends and removing adapter sequences.\\n\\nFor the next steps, we would typically proceed with the preprocessing using a tool like Trimmomatic or Cutadapt. However, since this step is hypothetical, we\\'ll pause here. If you have specific questions about the preprocessing or need further assistance, please let me know!'\n",
      "AIMessage\n",
      "/\\/\\/\\/\\/\\ num_tokens_in_messages 723\n",
      "/\\/\\/\\/\\/\\ Hard coded context window size of: 120000\n",
      "message content='Reflect and grade the assistant response to the user question below.'\n",
      "message content='\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'\n",
      "HumanMessage\n",
      "message content='To perform the initial quality checks on the raw sequencing data using FastQC, we\\'ll simulate the process since we can\\'t directly run FastQC in this environment. However, I\\'ll guide you through the steps you would take and provide an example of how to interpret the results.\\n\\n#### Step 1.1: Using FastQC for Initial Quality Checks\\n\\n1. **Installation**: If FastQC is not already installed, you can download it from the FastQC website or install it using Conda or Homebrew for macOS/Linux users. For example, using Conda:\\n   ```sh\\n   conda install -c bioconda fastqc\\n   ```\\n\\n2. **Running FastQC**: To run FastQC on your raw sequencing data, you would use the following command in your terminal (assuming you have FastQC installed and your data is in FastQ format):\\n   ```sh\\n   fastqc your_data.fastq\\n   ```\\n   Replace `your_data.fastq` with the path to your actual data file. If you have multiple files, you can run FastQC on all of them by specifying each file name or using wildcards.\\n\\n3. **Output**: FastQC will generate an HTML report for each input file, providing a detailed analysis of the data quality. These reports include information on various quality metrics such as per base sequence quality, sequence duplication levels, and overrepresented sequences.\\n\\n#### Step 1.2: Interpreting FastQC Reports and Deciding on Preprocessing\\n\\nAfter running FastQC, you\\'ll need to review the generated reports to assess the quality of your sequencing data. Here are some key sections to focus on:\\n\\n- **Per Base Sequence Quality**: This section shows the quality scores across all bases. Ideally, the plot should show most bases with high scores (green). If you observe a significant drop in quality towards the ends of reads, you might consider trimming those ends.\\n\\n- **Sequence Duplication Levels**: High duplication levels can indicate PCR artifacts. Depending on your experiment, you may need to remove duplicates.\\n\\n- **Overrepresented Sequences**: This section lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might need to trim or filter out.\\n\\nBased on these and other sections of the FastQC report, you can decide if your data requires preprocessing such as trimming for quality or removing adapters. Tools like Trimmomatic or Cutadapt are commonly used for this purpose.\\n\\nSince we can\\'t run FastQC directly here, let\\'s simulate the decision-making process based on a hypothetical FastQC report:\\n\\n- Assume the \"Per Base Sequence Quality\" section shows a significant quality drop at the ends of reads.\\n- The \"Sequence Duplication Levels\" are within acceptable limits.\\n- \"Overrepresented Sequences\" indicates the presence of adapter sequences.\\n\\nBased on this, we would decide to preprocess the data by trimming the low-quality ends and removing adapter sequences.\\n\\nFor the next steps, we would typically proceed with the preprocessing using a tool like Trimmomatic or Cutadapt. However, since this step is hypothetical, we\\'ll pause here. If you have specific questions about the preprocessing or need further assistance, please let me know!'\n",
      "AIMessage\n",
      "message content='Reflect and grade the assistant response to the user question below.'\n",
      "message content='\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'\n",
      "HumanMessage\n",
      "message content='To perform the initial quality checks on the raw sequencing data using FastQC, we\\'ll simulate the process since we can\\'t directly run FastQC in this environment. However, I\\'ll guide you through the steps you would take and provide an example of how to interpret the results.\\n\\n#### Step 1.1: Using FastQC for Initial Quality Checks\\n\\n1. **Installation**: If FastQC is not already installed, you can download it from the FastQC website or install it using Conda or Homebrew for macOS/Linux users. For example, using Conda:\\n   ```sh\\n   conda install -c bioconda fastqc\\n   ```\\n\\n2. **Running FastQC**: To run FastQC on your raw sequencing data, you would use the following command in your terminal (assuming you have FastQC installed and your data is in FastQ format):\\n   ```sh\\n   fastqc your_data.fastq\\n   ```\\n   Replace `your_data.fastq` with the path to your actual data file. If you have multiple files, you can run FastQC on all of them by specifying each file name or using wildcards.\\n\\n3. **Output**: FastQC will generate an HTML report for each input file, providing a detailed analysis of the data quality. These reports include information on various quality metrics such as per base sequence quality, sequence duplication levels, and overrepresented sequences.\\n\\n#### Step 1.2: Interpreting FastQC Reports and Deciding on Preprocessing\\n\\nAfter running FastQC, you\\'ll need to review the generated reports to assess the quality of your sequencing data. Here are some key sections to focus on:\\n\\n- **Per Base Sequence Quality**: This section shows the quality scores across all bases. Ideally, the plot should show most bases with high scores (green). If you observe a significant drop in quality towards the ends of reads, you might consider trimming those ends.\\n\\n- **Sequence Duplication Levels**: High duplication levels can indicate PCR artifacts. Depending on your experiment, you may need to remove duplicates.\\n\\n- **Overrepresented Sequences**: This section lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might need to trim or filter out.\\n\\nBased on these and other sections of the FastQC report, you can decide if your data requires preprocessing such as trimming for quality or removing adapters. Tools like Trimmomatic or Cutadapt are commonly used for this purpose.\\n\\nSince we can\\'t run FastQC directly here, let\\'s simulate the decision-making process based on a hypothetical FastQC report:\\n\\n- Assume the \"Per Base Sequence Quality\" section shows a significant quality drop at the ends of reads.\\n- The \"Sequence Duplication Levels\" are within acceptable limits.\\n- \"Overrepresented Sequences\" indicates the presence of adapter sequences.\\n\\nBased on this, we would decide to preprocess the data by trimming the low-quality ends and removing adapter sequences.\\n\\nFor the next steps, we would typically proceed with the preprocessing using a tool like Trimmomatic or Cutadapt. However, since this step is hypothetical, we\\'ll pause here. If you have specific questions about the preprocessing or need further assistance, please let me know!'\n",
      "AIMessage\n",
      "message_dicts [{'role': 'system', 'content': 'Reflect and grade the assistant response to the user question below.'}, {'role': 'user', 'content': '\\n### Step 1: Quality Control of Raw Data\\n- **1.1.** Use FastQC to perform initial quality checks on the raw sequencing data.\\n- **1.2.** Based on FastQC reports, decide if any preprocessing (like trimming) is needed.\\n'}, {'role': 'assistant', 'content': 'To perform the initial quality checks on the raw sequencing data using FastQC, we\\'ll simulate the process since we can\\'t directly run FastQC in this environment. However, I\\'ll guide you through the steps you would take and provide an example of how to interpret the results.\\n\\n#### Step 1.1: Using FastQC for Initial Quality Checks\\n\\n1. **Installation**: If FastQC is not already installed, you can download it from the FastQC website or install it using Conda or Homebrew for macOS/Linux users. For example, using Conda:\\n   ```sh\\n   conda install -c bioconda fastqc\\n   ```\\n\\n2. **Running FastQC**: To run FastQC on your raw sequencing data, you would use the following command in your terminal (assuming you have FastQC installed and your data is in FastQ format):\\n   ```sh\\n   fastqc your_data.fastq\\n   ```\\n   Replace `your_data.fastq` with the path to your actual data file. If you have multiple files, you can run FastQC on all of them by specifying each file name or using wildcards.\\n\\n3. **Output**: FastQC will generate an HTML report for each input file, providing a detailed analysis of the data quality. These reports include information on various quality metrics such as per base sequence quality, sequence duplication levels, and overrepresented sequences.\\n\\n#### Step 1.2: Interpreting FastQC Reports and Deciding on Preprocessing\\n\\nAfter running FastQC, you\\'ll need to review the generated reports to assess the quality of your sequencing data. Here are some key sections to focus on:\\n\\n- **Per Base Sequence Quality**: This section shows the quality scores across all bases. Ideally, the plot should show most bases with high scores (green). If you observe a significant drop in quality towards the ends of reads, you might consider trimming those ends.\\n\\n- **Sequence Duplication Levels**: High duplication levels can indicate PCR artifacts. Depending on your experiment, you may need to remove duplicates.\\n\\n- **Overrepresented Sequences**: This section lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might need to trim or filter out.\\n\\nBased on these and other sections of the FastQC report, you can decide if your data requires preprocessing such as trimming for quality or removing adapters. Tools like Trimmomatic or Cutadapt are commonly used for this purpose.\\n\\nSince we can\\'t run FastQC directly here, let\\'s simulate the decision-making process based on a hypothetical FastQC report:\\n\\n- Assume the \"Per Base Sequence Quality\" section shows a significant quality drop at the ends of reads.\\n- The \"Sequence Duplication Levels\" are within acceptable limits.\\n- \"Overrepresented Sequences\" indicates the presence of adapter sequences.\\n\\nBased on this, we would decide to preprocess the data by trimming the low-quality ends and removing adapter sequences.\\n\\nFor the next steps, we would typically proceed with the preprocessing using a tool like Trimmomatic or Cutadapt. However, since this step is hypothetical, we\\'ll pause here. If you have specific questions about the preprocessing or need further assistance, please let me know!'}]\n",
      "tool choices :  [Reflection(reflections=\"The response provided a clear and structured guide on how to perform initial quality checks on raw sequencing data using FastQC, including steps for installation, running the tool, and interpreting the results. It also offered a hypothetical scenario to help the user understand how to decide on preprocessing based on FastQC reports. The explanation was thorough and tailored to users who might not be familiar with bioinformatics tools, making it accessible. However, the response could have been improved by explicitly stating that it's a simulation of the process rather than direct instructions for executing in a real environment, to avoid any confusion for users attempting to follow the steps in this format. Additionally, providing direct links to resources or further reading on FastQC and preprocessing tools like Trimmomatic or Cutadapt could enhance the utility of the response.\", score=8, found_solution=True)]\n",
      "start\n",
      "rolled out:  1 <Node value=0.8, visits=1, solution=[AIMessage(content='To perform the initial quality checks on the raw sequencing data using FastQC, we\\'ll simulate the process since we can\\'t directly run FastQC in this environment. However, I\\'ll guide you through the steps you would take and provide an example of how to interpret the results.\\n\\n#### Step 1.1: Using FastQC for Initial Quality Checks\\n\\n1. **Installation**: If FastQC is not already installed, you can download it from the FastQC website or install it using Conda or Homebrew for macOS/Linux users. For example, using Conda:\\n   ```sh\\n   conda install -c bioconda fastqc\\n   ```\\n\\n2. **Running FastQC**: To run FastQC on your raw sequencing data, you would use the following command in your terminal (assuming you have FastQC installed and your data is in FastQ format):\\n   ```sh\\n   fastqc your_data.fastq\\n   ```\\n   Replace `your_data.fastq` with the path to your actual data file. If you have multiple files, you can run FastQC on all of them by specifying each file name or using wildcards.\\n\\n3. **Output**: FastQC will generate an HTML report for each input file, providing a detailed analysis of the data quality. These reports include information on various quality metrics such as per base sequence quality, sequence duplication levels, and overrepresented sequences.\\n\\n#### Step 1.2: Interpreting FastQC Reports and Deciding on Preprocessing\\n\\nAfter running FastQC, you\\'ll need to review the generated reports to assess the quality of your sequencing data. Here are some key sections to focus on:\\n\\n- **Per Base Sequence Quality**: This section shows the quality scores across all bases. Ideally, the plot should show most bases with high scores (green). If you observe a significant drop in quality towards the ends of reads, you might consider trimming those ends.\\n\\n- **Sequence Duplication Levels**: High duplication levels can indicate PCR artifacts. Depending on your experiment, you may need to remove duplicates.\\n\\n- **Overrepresented Sequences**: This section lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might need to trim or filter out.\\n\\nBased on these and other sections of the FastQC report, you can decide if your data requires preprocessing such as trimming for quality or removing adapters. Tools like Trimmomatic or Cutadapt are commonly used for this purpose.\\n\\nSince we can\\'t run FastQC directly here, let\\'s simulate the decision-making process based on a hypothetical FastQC report:\\n\\n- Assume the \"Per Base Sequence Quality\" section shows a significant quality drop at the ends of reads.\\n- The \"Sequence Duplication Levels\" are within acceptable limits.\\n- \"Overrepresented Sequences\" indicates the presence of adapter sequences.\\n\\nBased on this, we would decide to preprocess the data by trimming the low-quality ends and removing adapter sequences.\\n\\nFor the next steps, we would typically proceed with the preprocessing using a tool like Trimmomatic or Cutadapt. However, since this step is hypothetical, we\\'ll pause here. If you have specific questions about the preprocessing or need further assistance, please let me know!')] reflection=reflections=\"The response provided a clear and structured guide on how to perform initial quality checks on raw sequencing data using FastQC, including steps for installation, running the tool, and interpreting the results. It also offered a hypothetical scenario to help the user understand how to decide on preprocessing based on FastQC reports. The explanation was thorough and tailored to users who might not be familiar with bioinformatics tools, making it accessible. However, the response could have been improved by explicitly stating that it's a simulation of the process rather than direct instructions for executing in a real environment, to avoid any confusion for users attempting to follow the steps in this format. Additionally, providing direct links to resources or further reading on FastQC and preprocessing tools like Trimmomatic or Cutadapt could enhance the utility of the response.\" score=8 found_solution=True/>\n",
      "---\n",
      "__end__\n",
      "rolled out:  1 <Node value=0.8, visits=1, solution=[AIMessage(content='To perform the initial quality checks on the raw sequencing data using FastQC, we\\'ll simulate the process since we can\\'t directly run FastQC in this environment. However, I\\'ll guide you through the steps you would take and provide an example of how to interpret the results.\\n\\n#### Step 1.1: Using FastQC for Initial Quality Checks\\n\\n1. **Installation**: If FastQC is not already installed, you can download it from the FastQC website or install it using Conda or Homebrew for macOS/Linux users. For example, using Conda:\\n   ```sh\\n   conda install -c bioconda fastqc\\n   ```\\n\\n2. **Running FastQC**: To run FastQC on your raw sequencing data, you would use the following command in your terminal (assuming you have FastQC installed and your data is in FastQ format):\\n   ```sh\\n   fastqc your_data.fastq\\n   ```\\n   Replace `your_data.fastq` with the path to your actual data file. If you have multiple files, you can run FastQC on all of them by specifying each file name or using wildcards.\\n\\n3. **Output**: FastQC will generate an HTML report for each input file, providing a detailed analysis of the data quality. These reports include information on various quality metrics such as per base sequence quality, sequence duplication levels, and overrepresented sequences.\\n\\n#### Step 1.2: Interpreting FastQC Reports and Deciding on Preprocessing\\n\\nAfter running FastQC, you\\'ll need to review the generated reports to assess the quality of your sequencing data. Here are some key sections to focus on:\\n\\n- **Per Base Sequence Quality**: This section shows the quality scores across all bases. Ideally, the plot should show most bases with high scores (green). If you observe a significant drop in quality towards the ends of reads, you might consider trimming those ends.\\n\\n- **Sequence Duplication Levels**: High duplication levels can indicate PCR artifacts. Depending on your experiment, you may need to remove duplicates.\\n\\n- **Overrepresented Sequences**: This section lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might need to trim or filter out.\\n\\nBased on these and other sections of the FastQC report, you can decide if your data requires preprocessing such as trimming for quality or removing adapters. Tools like Trimmomatic or Cutadapt are commonly used for this purpose.\\n\\nSince we can\\'t run FastQC directly here, let\\'s simulate the decision-making process based on a hypothetical FastQC report:\\n\\n- Assume the \"Per Base Sequence Quality\" section shows a significant quality drop at the ends of reads.\\n- The \"Sequence Duplication Levels\" are within acceptable limits.\\n- \"Overrepresented Sequences\" indicates the presence of adapter sequences.\\n\\nBased on this, we would decide to preprocess the data by trimming the low-quality ends and removing adapter sequences.\\n\\nFor the next steps, we would typically proceed with the preprocessing using a tool like Trimmomatic or Cutadapt. However, since this step is hypothetical, we\\'ll pause here. If you have specific questions about the preprocessing or need further assistance, please let me know!')] reflection=reflections=\"The response provided a clear and structured guide on how to perform initial quality checks on raw sequencing data using FastQC, including steps for installation, running the tool, and interpreting the results. It also offered a hypothetical scenario to help the user understand how to decide on preprocessing based on FastQC reports. The explanation was thorough and tailored to users who might not be familiar with bioinformatics tools, making it accessible. However, the response could have been improved by explicitly stating that it's a simulation of the process rather than direct instructions for executing in a real environment, to avoid any confusion for users attempting to follow the steps in this format. Additionally, providing direct links to resources or further reading on FastQC and preprocessing tools like Trimmomatic or Cutadapt could enhance the utility of the response.\" score=8 found_solution=True/>\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# invoke executor\n",
    "for step in executor_graph.stream({\"input\": executor_plan_step}):\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print(\"rolled out: \", step_state[\"root\"].height, step_state[\"root\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aef6c482-cb41-4982-ac54-7b85f238e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To perform the initial quality checks on the raw sequencing data using FastQC, we'll simulate the process since we can't directly run FastQC in this environment. However, I'll guide you through the steps you would take and provide an example of how to interpret the results from FastQC reports to decide if any preprocessing, such as trimming, is needed.\n",
      "\n",
      "### Step 1.1: Using FastQC for Initial Quality Checks\n",
      "\n",
      "Normally, you would run FastQC on your raw sequencing data files (usually in FASTQ format) using the following command in a terminal:\n",
      "\n",
      "```bash\n",
      "fastqc your_data_file.fastq\n",
      "```\n",
      "\n",
      "This command generates a report in HTML format that you can view in any web browser. The report contains several sections, each providing insights into different aspects of your data quality.\n",
      "\n",
      "### Step 1.2: Interpreting FastQC Reports for Preprocessing Decisions\n",
      "\n",
      "Here are some key sections of the FastQC report to pay attention to for deciding on preprocessing:\n",
      "\n",
      "1. **Per base sequence quality**: This plot shows the quality scores across all bases at each position in the reads. If you see a significant drop in quality towards the ends of the reads, you might consider trimming those ends.\n",
      "\n",
      "2. **Per sequence quality scores**: This graph shows the distribution of the average quality score over all bases for each read. If a large number of reads have low average quality scores, you might need to filter out low-quality reads.\n",
      "\n",
      "3. **Per base sequence content**: This section shows the proportion of each base (A, T, C, G) at each position. A significant deviation from the expected proportions might indicate a bias or contamination that could affect further analysis.\n",
      "\n",
      "4. **Sequence Duplication Levels**: High levels of duplication can indicate PCR artifacts. Depending on your experiment, you might want to remove duplicates.\n",
      "\n",
      "5. **Overrepresented sequences**: This part lists sequences that appear more often than expected. These could be adapters or other contaminant sequences that you might want to trim.\n",
      "\n",
      "Based on these sections, if you decide that trimming or filtering is necessary, you can use tools like Trimmomatic for trimming based on quality and removing adapters, or seqtk for simple quality and length-based filtering.\n",
      "\n",
      "Remember, the decision to preprocess and the choice of parameters depend on the specifics of your data and the requirements of your downstream analyses.\n",
      "\n",
      "Since we can't run FastQC directly here, if you have specific questions about interpreting FastQC reports or need further guidance on preprocessing steps, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "executor_solution_node = step[\"__end__\"][\"root\"].get_best_solution()\n",
    "executor_best_trajectory = executor_solution_node.get_trajectory(include_reflections=False)\n",
    "print(executor_best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd42e3-6e3c-433d-b20b-79f4c622bdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
